{"title":"SVD Based Image Processing Applications","markdown":{"yaml":{"title":"SVD Based Image Processing Applications","format":{"ieee-html":"default"},"author":[{"id":"dfolio","name":"Siju K S","affiliations":[{"name":"Amrita Vishwa Vidyapeetham","department":"School of Artificial Intelligence","city":"Coimbatore","country":"India","postal-code":"641 112"},{"name":"CEN"}],"orcid":"0009-0004-1983-5574","email":"siju.swamy@saintgits.org","url":"https://github.com/sijuswamy/SVD_project","membership":"Member, ISTE","attributes":{"corresponding":true},"photo":"Swamy.jpg","bio":"Pursuing research in Artificial Intelligence under the Faculty of Amrita School of Artificial Intelligence at the Center of Excellence in Computational Engineering & Networking, Amrita Vishwa Vidyapeetham, Coimbatore.\n"},{"name":"Dr.Soman K.P","affiliations":[{"name":"Professor & Dean, Amrita School of Artificial Intelligence"}],"photo":"soman_sir.jpg","bio":"Dr. Soman K. P. currently serves as the Dean of the School of Artificial Intelligence (AI), Head and Professor at Amrita Centre for Computational Engineering and Networking (CEN), Coimbatore Campus. He has more than 27 years of research and teaching experience in Artificial Intelligence (AI) and Data Science-related subjects at Amrita Vishwa Vidyapeetham, Coimbatore. He has authored over 500+ publications in reputed journals such as IEEE Transactions, IEEE Access, Applied Energy, and several conference proceedings.\n","note":"Template created June 23, 2023; revised `r format(Sys.Date(),format='%B %d, %Y')`."}],"abstract":"This study investigates the application of Singular Value Decomposition (SVD) as an effective mathematical framework for various image processing tasks. SVD offers a unique decomposition approach, making it suitable for applications like image compression, denoising, and watermarking by enabling optimal rank approximations and noise separation. The robustness of SVD in handling large matrices allows it to capture key image characteristics, preserving essential features while reducing data requirements. By leveraging SVD’s ability to separate data into dominant and subdominant subspaces, this research demonstrates enhanced image compression, effective noise reduction, and secure watermark embedding. Experimental results validate SVD's utility in optimizing image storage, clarity, and fidelity, with potential implications for advancing adaptive image processing techniques.\n","keywords":["Singular Value Decomposition (SVD)","Image Processing","Image Compression","Image Denoising","Digital Watermarking","Noise Filtering","Matrix Factorization","Rank Approximation","Frobenius Norm","Energy Compaction","Digital Forensics","Signal Processing","Adaptive Image Processing","Orthogonal Subspaces"],"funding":{"statement":"No funding is recieved for completion of this project work"},"pageheader":{"left":"ASAI, October 2024","right":"Project Report"},"bibliography":"./references.bib","date":"2024-10-30","pdf":"https://github.com/sijuswamy/SVD_project/blob/main/A%20Study%20on%20SVD%20Based%20Image%20Processing%20Applications.pdf","citation":{"container-title":"GitHUB","page":"1-27","type":"article","issued":"2024-11-04","url":"https://github.com/sijuswamy/SVD_project","pdf-url":"https://github.com/sijuswamy/SVD_project/blob/main/A%20Study%20on%20SVD%20Based%20Image%20Processing%20Applications.pdf"}},"headingText":"Introduction","containsRefs":true,"markdown":"\n\n\nLinear Algebra based SVD is a powerful method to be used within the realm of digital image processing. SVD decomposes a matrix in to three constitutive matrices U, S, and V, thus making it possible to represent an image using a fewer number of values @moonen1992singular. This characteristic has practical usage such as for image compression by keeping few singular values in $S$ matrix and stores the characteristics feature of the image, hence reduce storage.\n\nResearches suggested that It can be done if appropriate number of singular values maintained and image can be compressed at higher ratio with good quality The number of singular values kept (and thus the size of the image to be compressed) is always not more (and often far less) than the number of pixels in the original image. Thus, SVD turns out to be a relatively strong technique for applications where a minimum number of storage space and bandwidth is required to be preserved during transmission of signals such as in satellite imagery, medical imaging and photo enhancement.\n\nSingular Value Decomposition (SVD) is a powerful mathematical technique with a diverse range of applications in image processing. While its capabilities are well-established, there remains untapped potential in fully harnessing its versatility. This paper delves into the rich properties of SVD and demonstrates how they can be leveraged across various image processing tasks, such as compression, watermarking, and quality assessment.\n\nThe study presents several key findings. First, the experiments validate known but underutilized characteristics of Singular Value Decomposition (SVD) in the context of image processing. This serves to aid ongoing efforts aimed at enhancing the application of these SVD characteristics. Second, the research identifies new trends and challenges faced in the application of SVD for image processing. Some of these trends are corroborated by experimental data, while others require additional verification. Finally, this work lays the groundwork for future investigations, highlighting promising avenues for further exploration and development.\n\nOverall, this work offers a comprehensive examination of the rich properties of Singular Value Decomposition (SVD) and its multifaceted applications in the field of image processing. By shedding light on both the established and emerging aspects of SVD, the study paves the way for more efficient and innovative applications of this powerful mathematical technique.\n\n\n## Related works\n\nAndrews and Patterson (1976) explored the significant role of Singular Value Decomposition (SVD) techniques in the realm of digital image processing, particularly for applications that demand high computational power and precise imaging capabilities. Their work highlighted the versatility of SVD methods, which are applicable not only to images but also to broader representations of point spread functions (PSF) and impulse responses. The authors framed these techniques as natural extensions of linear filtering theory, thereby situating SVD within established methodologies for image enhancement and restoration  @andrews1976singular.\n \n\n Moonen et al. (1992) expanded upon the established QR updating scheme by introducing a more versatile and generally applicable method for updating the Singular Value Decomposition (SVD). Their approach enhances the QR updating technique by integrating a Jacobi-type SVD procedure. This innovative combination allows for the effective restoration of an acceptable approximation of the SVD after only a few SVD steps following each QR update. The authors demonstrated that this method not only maintains a comparable computational cost to that of traditional QR updating but also significantly reduces the overall computational burden associated with SVD updates. @moonen1992singular.\n \n\nIn their paper, Kakarala and Ogunbona (2001) introduced a novel multiresolution form of Singular Value Decomposition (SVD) designed for enhanced signal analysis and approximation. Recognizing the inherent strengths of traditional SVD—specifically its optimal decorrelation and subrank approximation properties—the authors expanded upon these foundations by developing a multiresolution approach that maintains linear computational complexity  @kakarala2001signal.\n \n\nD Chandra (2002) introduced a novel watermarking technique- scaled additive approach- for digital images that employs Singular Value Decomposition (SVD) as a foundational method. The paper provides comprehensive simulation results that showcase the robustness of this SVD-based watermarking approach against various common image degradations, underscoring its effectiveness in preserving watermark integrity in challenging conditions  @chandra2002digital.\n \n\nSadek (2008) explored the increasing prominence of Singular Value Decomposition (SVD) as a robust and reliable technique for orthogonal matrix decomposition in the field of signal processing, particularly in the context of watermarking and data hiding. The author highlighted the fundamental properties of SVD, such as its conceptual clarity and stability, which contribute to its growing popularity in various applications\nIn the realm of watermarking, many researchers have focused on leveraging the singular values of host images to embed hidden information. However, Sadek introduced a critical examination of these SVD-based watermarking techniques by presenting a counterfeiting attack specifically targeting the embedded watermark information within the singular values. The study underscored the inherent vulnerabilities of this class of watermarking methods, revealing how singular values can be easily compromised through a broad spectrum of image processing operations and deliberate attacks  @sadek2008blind.\n \n\nSadek (2012) explores the potential of Singular Value Decomposition (SVD) as a transformative tool in the realm of image processing. The paper presents a comprehensive experimental survey highlighting SVD's efficacy across various imaging applications and proposed the perceptual forensic approach in image watermarking. Recognizing SVD as an attractive algebraic transform, Sadek emphasizes that its application in image processing is still in its early stages despite its well-documented advantageous properties  @sadek2012svd.\n \n\nIn their study, Kahu and Rahate (2013) investigated the application of Singular Value Decomposition (SVD) as a technique for image compression, emphasizing its effectiveness in expressing image data through a limited number of eigenvectors determined by the image's dimensionality. They highlighted the significance of psycho-visual redundancies inherent in images, which enable compression without compromising the quality of the visual output @kahu2013image.\n\n## Introduction to Singular Value Decomposition (SVD)\n\nIn linear algebra, Singular Value Decomposition (SVD) is a fundamental factorization technique for rectangular real or complex matrices. It provides a structure similar to the diagonalization of symmetric or Hermitian square matrices, utilizing eigenvectors as a basis. SVD is particularly advantageous due to its stability and effectiveness, allowing for decomposition into a set of linearly independent components, each contributing uniquely to the matrix’s structure.\n\nFor a digital image $X$ of size $M \\times N$ (where $M \\geq N$), the SVD of $X$ is represented as:\n\n$$\nX = U \\Sigma V^T\n$$\n\nwhere $U$ is an $M \\times M$ orthogonal matrix, $V$ is an $N \\times N$ orthogonal matrix, and $\\Sigma$ is an $M \\times N$ diagonal matrix. The matrices $U = [u_1, u_2, \\ldots, u_m]$ and $V = [v_1, v_2, \\ldots, v_n]$ contain the left and right singular vectors of $X$, respectively, and $\\Sigma$ holds the singular values $\\sigma_i$ of $X$ along its diagonal in descending order of magnitude, with all off-diagonal elements set to zero.\n\nIn this setup, $U$ and $V$ are unitary orthogonal matrices, meaning that each column vector has a unit norm and is orthogonal to others. The singular values $\\sigma_i$ in $\\Sigma$ indicate the energy contribution of each corresponding component, while each pair of singular vectors from $U$ and $V$ defines the spatial orientation or geometry of these components.\n\nThe left singular vectors (LSCs) of $X$ are the eigenvectors of the matrix $X X^T$, while the right singular vectors (RSCs) are eigenvectors of $X^T X$. Each singular value represents the 2-norm of its associated component, with the largest singular values capturing the most significant patterns or features in the data. This property allows SVD to effectively highlight essential image components while suppressing noise or less critical features, making it ideal for applications focused on key structural features in image processing  @andrews1976singular.\n\n## SVD- A New Tool for Image Processing\n\nSingular Value Decomposition (SVD) is a powerful and robust method for orthogonal matrix decomposition, widely valued for its stability and conceptual clarity. These attributes have led to its growing popularity in signal processing, particularly in the domain of image processing. As an algebraic transformation, SVD brings several advantageous properties to imaging, which this section examines. While some of these properties are well-utilized, others present opportunities for further exploration and application.\n\nSeveral key properties of SVD make it particularly useful in image processing. These include maximum energy packing, efficient solutions to least squares problems, calculation of matrix pseudo-inverses, and multivariate analysis @strang2022introduction. An essential feature of SVD is its relationship to matrix rank and its ability to approximate matrices at a given rank. Digital images, often represented as low-rank matrices, can be effectively described by a limited number of eigenimages. This approach allows image signals to be manipulated in two distinct subspaces @kamm1998svd.\n\nIn the sections that follow, key hypotheses related to these properties are proposed and validated. For completeness, the theoretical SVD theorems relevant to these applications are summarized, followed by a practical review of SVD properties with experimental demonstrations.\n\n### SVD subspaces and architecture\n\nThe SVD method effectively divides a matrix into two orthogonal subspaces: the dominant and subdominant subspaces. This division corresponds to a partitioning of the $M$-dimensional vector space, separating primary signal components from secondary ones [@andrews1976singular, @kamm1998svd]. Such a property is particularly advantageous in applications like noise filtering and digital watermarking, where isolating signal elements from noise or embedding data is crucial [@chandra2002digital, @sadek2012svd].\n\nIn the context of image processing, SVD architecture further highlights its utility. For an image decomposed via SVD, each singular value (SV) represents the luminance level of a specific image layer, while the associated singular vectors (SCs) provide the geometric structure of that layer. Generally, prominent image features align with eigenimages associated with larger singular values, while smaller singular values correspond to components associated with noise @kahu2013image.\n\n### PCA versus SVD\n\nPrincipal Component Analysis (PCA), also known as the Karhunen-Loève Transform (KLT) or the Hotelling Transform, is a technique for computing dominant vectors that represent a given dataset. PCA achieves an optimal basis for minimum mean squared reconstruction of data and is computationally based on the SVD of the data matrix or the eigenvalue decomposition of the data covariance matrix. SVD is closely related to the eigenvalue-eigenvector decomposition of a square matrix $X$ into $V\\Lambda V^T$, where $V$ is orthogonal, and $\\Lambda$ is diagonal. Notably, the matrices $U$ and $V$ in SVD correspond to eigenvectors of $XX^T$ and $X^TX$, respectively. If $X$ is symmetric, the singular values of $X$ are the absolute values of its eigenvalues [@strang2022introduction,@deisenroth2020mathematics].\n\n### SVD Multiresolution\n\nSVD is known for its maximum energy packing capability, making it particularly useful for applications requiring multiresolution analysis. This approach enables statistical characterization of images across multiple resolutions, with SVD decomposing a matrix into orthogonal components that allow optimal sub-rank approximations. The multiresolution properties of SVD provide a framework to measure several critical image characteristics at various resolutions, including isotropy, sparsity of principal components, self-similarity under scaling, and decomposition of mean squared error into meaningful components @kakarala2001signal.\n\n### SVD oriented energy\n\nIn SVD-based analysis of oriented energy, both the rank of the problem and the signal space orientation are identifiable. SVD allows decomposition into linearly independent components, each with its own energy contribution. Represented as a linear combination of principal components, SVD highlights dominant components that define the rank of the observed system, with a few key components effectively capturing the system's structure. The concept of oriented energy is beneficial for separating signals from different sources or selecting signal subspaces with maximal activity and integrity. Singular values in SVD represent the square root of energy in the corresponding principal direction, with the primary direction often aligned with the first singular vector $V_1$. Dominance accuracy can be measured by evaluating the difference, or normalized difference, between the first two singular values [@kakarala2001signal, @sadek2008blind].\n\n\nMany properties of SVD remain underutilized in image processing applications. Subsequent sections will experimentally explore these unexploited properties to demonstrate their potential for enhancing various image processing techniques. Additional research is essential to fully harness this versatile transformation in new and evolving applications.\n\n## Optimal Approximation and Noise Isolation Using SVD\n\nThe SVD’s unique ability to distinguish image content from noise is critical for efficient matrix approximation. In an SVD-decomposed matrix, the highest singular values capture the most essential components of the image, while lower singular values represent noise. By reconstructing the matrix with only the top $k$ singular values—forming an approximation $X_k = U_k \\Sigma_k V_k^T$—SVD yields an optimal representation that preserves primary image features while suppressing noise. This characteristic makes SVD ideal for noise filtering, compression, and forensic applications, where detectable noise patterns are useful in watermarking and signal integrity assessment.\n### Rank approximation using SVD}\n\nSingular Value Decomposition (SVD) facilitates low-rank approximation, enabling optimal sub-rank representations by emphasizing the largest singular values that encapsulate the majority of the energy within an image. SVD illustrates that a matrix can be expressed as a sum of rank-one matrices. Given a matrix $X \\in \\mathbb{R}^{m \\times n}$ with $p = \\min(m,n)$, the approximation can be represented as a truncated matrix $X_k$ with a specified rank $k$. The representation is formulated as follows:\n\n$$\nX \\approx X_k = \\sum_{i=1}^{k} s_i u_i v_i^T,\n$$\n\nwhere $s_i$ are the singular values, $u_i$ are the left singular vectors, and $v_i$ are the right singular vectors. Each term $s_i u_i v_i^T$ corresponds to a rank-one matrix, leading to the conclusion that $X$ is the sum of $k$ rank-one matrices. This approximation captures as much of the *energy* of $X$ as possible while maintaining a rank of at most $k$. Here, *energy* is quantified using the 2-norm or Frobenius norm.\n\nThe outer product $u_i v_i^T$ results in a matrix of rank 1, requiring $M + N$ storage compared to $M \\times N$ for the original matrix. For truncated SVD transformations with rank $k$, the required storage space is reduced to $(m+n+1)k$, demonstrating the efficiency of SVD in applications such as image compression and watermarking.\n\n## Example of SVD Application: Image Reconstruction\n\nIn this section, we demonstrate the application of Singular Value Decomposition (SVD) on a JPEG image obtained from the internet. The image is subjected to low-rank approximation using SVD to explore its effectiveness in image reconstruction and compression.\n\nFor this experiment, we set the rank $k = 40$. The original image, denoted as $X$, is decomposed into its singular values and singular vectors as follows:\n\n$$\nX = U S V^T,\n$$\n\nwhere $U$ is an orthogonal matrix containing the left singular vectors, $S$ is a diagonal matrix of singular values, and $V^T$ contains the right singular vectors. By retaining only the top $k$ singular values and their corresponding singular vectors, we can reconstruct a low-rank approximation of the image:\n\n$$\nX_k \\approx \\sum_{i=1}^{k} s_i u_i v_i^T.\n$$\n\nIn this instance, the low-rank approximation captures a significant portion of the image's energy, effectively preserving the essential visual features while reducing the noise and detail represented by the higher-order singular values.\n\nThe reconstructed image using $k = 40$ is displayed in [@fig-svdreconstruction]. This result illustrates the ability of SVD to maintain the overall structure and key characteristics of the original image while achieving a notable reduction in data size. The efficiency of this low-rank approximation highlights the potential of SVD for applications in image compression and restoration, allowing for storage savings without substantial loss of quality.\n\nThis example underscores the practical utility of SVD in image processing, offering a powerful tool for manipulating image data in various applications, including compression, denoising, and feature extraction.\n\n:::{#fig-svdreconstruction}\n![](./Notebooks/figures/SVD1.png)\n\nReconstructed image using SVD with low-rank approximation (k=40).\n:::\n\n## Secrets of left and right singular matrices\n\nThe matrices $U$ and $V^T$ provide crucial insights into the structural characteristics of the image, specifically the column space and row space representations.\n\nThe left singular matrix $U$ captures the column space of the image, which represents the various features and patterns present in the image across its vertical axis. In contrast, the right singular matrix $V^T$ captures the row space of the image, representing patterns across the horizontal axis. By visualizing the first two components of these matrices, we can reconstruct the primary patterns within the image (refer @fig-SVD_components).\n\nThe first two components of the column space from matrix $U$ highlight the dominant vertical patterns, while the first two components of the row space from matrix $V^T$ reveal the dominant horizontal patterns. This reconstruction allows for a clear interpretation of how the image is constructed from these fundamental features, showcasing the spatial relationships inherent in the image data.\n\nIt is important to note that the components of $V$ associated with the smallest singular values correspond to noise in the image. This noise resides in the null space of the image matrix $X$ and contributes minimally to the overall structure of the image. By identifying these components, we can effectively distinguish between the essential features of the image and the extraneous noise that may obscure its true representation.\n\n:::{#fig-SVD_components}\n\n![](./Notebooks/figures/SVDcomponents.png)\n\nVisualization of the components obtained from the SVD of the image.\n:::\n\nFigure [@fig-singular_value_distribution} displays the log-mod distribution of the singular values from the SVD of the image, providing insight into the energy contributions of each component.\n\n\n:::{#fig-singular_value_distribution}\n\n![](./Notebooks/figures/singular-value-distribution.png)\n\nDistribution of the singular values of the image.\n:::\n\nThe log-mod distribution of the singular values reveals crucial information about the image's structure and the underlying data's dimensionality. The singular values, arranged in descending order, represent the amount of energy each corresponding eigenimage contributes to the overall image representation.\n\nFrom [@fig-singular_value_distribution], a rapid decay in singular values indicates that a small number of components capture the majority of the image's energy, signifying a low-rank structure. This property is advantageous for compression, as it suggests that the image can be approximated using fewer outer products of rank-one matrices, thus minimizing information loss.\n\nThe slope of the log-mod distribution further elucidates the significance of each singular value; a steep drop-off signifies that most information is concentrated in the first few singular values, while the tail end, characterized by smaller singular values, is associated with noise and less informative features of the image. This insight allows for strategic selection of singular values in applications such as compression and denoising, where retaining the dominant components while discarding those associated with lower energy can enhance the overall quality of the reconstructed image.\n\n### Image quality metrics\n\nIn the context of image compression and reconstruction using Singular Value Decomposition (SVD), it is essential to evaluate the quality of the reconstructed image. Three popular metrics for assessing image quality are the Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). Each of these metrics provides a different perspective on the quality of the reconstructed image compared to the original.\n\n#### Mean Squared Error (MSE)\n\nThe Mean Squared Error is a measure of the average squared differences between the original and reconstructed images. It is defined mathematically as:\n\n$$\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (I(i) - \\hat{I}(i))^2\n$$\n\nwhere $I(i)$ is the pixel value of the original image, $\\hat{I}(i)$ is the pixel value of the reconstructed image, and $N$ is the total number of pixels in the image. Lower MSE values indicate better image quality.\n\n#### Peak Signal-to-Noise Ratio (PSNR)\nThe Peak Signal-to-Noise Ratio is a logarithmic measure that compares the maximum possible power of a signal to the power of corrupting noise that affects the fidelity of its representation. It is given by:\n\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n$$\nwhere $\\text{MAX}$ represents the maximum pixel value (e.g., 255 for 8-bit images). Higher PSNR values indicate better image quality, as they correspond to lower MSE values. Higher PSNR values generally indicate better quality of the reconstructed image.\n\n#### Structural Similarity Index Measure (SSIM)\n\nThe Structural Similarity Index Measure assesses the visual impact of three characteristics: luminance, contrast, and structure. The SSIM index is defined as:\n\n$$\n\\text{SSIM}(I, \\hat{I}) = \\frac{(2\\mu_I \\mu_{\\hat{I}} + C_1)(2\\sigma_{I\\hat{I}} + C_2)}{(\\mu_I^2 + \\mu_{\\hat{I}}^2 + C_1)(\\sigma_I^2 + \\sigma_{\\hat{I}}^2 + C_2)}\n$$\n\nwhere $\\mu_I$ and $\\mu_{\\hat{I}}$ are the average pixel values of the original and reconstructed images, $\\sigma_I^2$ and $\\sigma_{\\hat{I}}^2$ are the variances, and $\\sigma_{I\\hat{I}}$ is the covariance. The constants $C_1$ and $C_2$ are small values added for stability. SSIM values range from -1 to 1, with values closer to 1 indicating better similarity.\n\nThese metrics can effectively evaluate the quality of images reconstructed through SVD compression, providing insights into how well the compression process preserves the original image details.\n\n#### Comparison of image compression methods\n\nIn this section, we compare the performance of different image compression methods, specifically focusing on Singular Value Decomposition (SVD), Discrete Cosine Transform (DCT), Wavelet Transform (Haar), Fractal Compression, Run-Length Encoding (RLE), and Predictive Coding. Each method has its unique characteristics and is suitable for different types of image data. The evaluation metrics used for comparison include Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM).\nA brief description of compression methods is given below.\n\n  - *Singular Value Decomposition:* A linear algebra technique that decomposes a matrix into singular values and orthogonal matrices, providing efficient low-rank approximations suitable for image compression.\n    \n  - *DCT (Discrete Cosine Transform:* Widely used in JPEG compression, DCT transforms image data into a frequency domain, allowing for the quantization and truncation of less significant frequencies to reduce file size while maintaining visual quality.\n    \n  - *Wavelet (Haar Transform):* Utilizes wavelet functions to represent data at different scales and resolutions, allowing for both spatial and frequency localization, making it effective for compressing images with varying detail levels.\n    \n  - *Fractal Compression:* This method relies on self-similarity in images and encodes them by identifying and representing repetitive patterns, which can lead to high compression ratios, especially for natural images.\n    \n  - *RLE (Run-Length Encoding):* A lossless compression technique that replaces sequences of the same data value with a single value and a count, making it effective for images with large uniform areas.\n    \n  - *Predictive Coding:*  This approach predicts pixel values based on neighboring pixels and encodes the difference between the predicted and actual values, effectively reducing redundancy in the image data.\n\nThe performance metrics for each compression method are summarized in @tbl-image_compression_comparison.\n\n:::{#tbl-image_compression_comparison}\n| Method     | MSE     | PSNR (dB) | SSIM   |\n|------------|---------|-----------|--------|\n| SVD        | 36.1802 | 32.5461   | 0.8247 |\n| DCT        | 107.6621| 27.8102   | 0.8217 |\n| Wavelet    | 32.9375 | 32.9539   | 0.9582 |\n| Fractal    | 20.4741 | 35.0188   | 0.9320 |\n| RLE        | 0.0000  | inf       | 1.0000 |\n| Predictive | 107.2521| 27.8267   | 0.5477 |\n\n: Comparison of Image Compression Methods\n:::: \n\nThe comparison of various image compression methods, as presented in @tbl-image_compression_comparison, highlights the promising performance of Singular Value Decomposition (SVD) image compression. The SVD method achieved a Mean Squared Error (MSE) of 36.1802, a Peak Signal-to-Noise Ratio (PSNR) of 32.5461 dB, and a Structural Similarity Index Measure (SSIM) of 0.8247. In terms of image quality, a PSNR value above 30 dB is generally considered acceptable for high-quality image reconstruction, and the SVD method meets this criterion. Similarly, the SSIM score of 0.8247 indicates a relatively high level of structural similarity, as values closer to 1.0 are preferred for maintaining perceptual quality. These results suggest that SVD is a promising approach for image compression, effectively balancing compression efficiency with visual quality, particularly suitable for applications that require efficient storage and satisfactory image fidelity.\n\n### Orthogonal subspaces in SVD\n\nThe Singular Value Decomposition (SVD) of the original data matrix $X$ enables its decomposition into two orthogonal subspaces: the *dominant subspace*, represented by the components $US_kV^T$, which corresponds to the signal information, and the *subdominant subspace*, represented by $US_{n-k}V^T$, which captures the noise components. This dual representation provides a clear delineation of the image data into signal and noise, significantly enhancing our ability to analyze and process the data effectively. This formalism can be represented as in Figure @fig-dom-subdom.\n\n:::{#fig-dom-subdom}\n\n![](./Notebooks/figures/domsubdom.png)\n\nDominant-subdominant splitting of the image SVD.\n:::\n\nUsing the SVD, all the fundamental subspaces and their rank can be extracted. This residing relationship can be visualized as:\n\n\\begin{align*}\n  \\mathbf{X} &=\n  \\mathbf{U} \\, \\Sigma \\, \\mathbf{V}^{T} \\\\\n%\n &=\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right] \n  \\left[ \\begin{array}{cccc|cc}\n     \\sigma_{1} & 0 & \\dots &  &   & \\dots &  0 \\\\\n     0 & \\sigma_{2}  \\\\\n     \\vdots && \\ddots \\\\\n       & & & \\sigma_{\\rho} \\\\\\hline\n       & & & & 0 & \\\\\n     \\vdots &&&&&\\ddots \\\\\n     0 & & &   &   &  & 0 \\\\\n  \\end{array} \\right]\n  \\left[ \\begin{array}{c}\n     \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{T} \\\\ \n     \\color{red}{\\mathbf{V}_{\\mathcal{N}}}^{T}\n  \\end{array} \\right]  \\\\\n  & =\n   \\left[ \\begin{array}{cccccccc}\n    \\color{blue}{u_{1}} & \\dots & \\color{blue}{u_{\\rho}} & \\color{red}{u_{\\rho+1}} & \\dots & \\color{red}{u_{m}}\n  \\end{array} \\right]\n  \\left[ \\begin{array}{cc}\n     \\mathbf{S}_{\\rho\\times \\rho} & \\mathbf{0} \\\\\n     \\mathbf{0} & \\mathbf{0} \n  \\end{array} \\right]\n   \\left[ \\begin{array}{c}\n    \\color{blue}{v_{1}^{T}} \\\\ \n    \\vdots \\\\\n    \\color{blue}{v_{\\rho}^{T}} \\\\\n    \\color{red}{v_{\\rho+1}^{T}} \\\\\n    \\vdots \\\\ \n    \\color{red}{v_{n}^{T}}\n  \\end{array} \\right]\n\\end{align*}\n\nThe column vectors form spans for the subspaces are given by\n\n\\begin{align*} \n% R A\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{X} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{u_{1}}, \\dots , \\color{blue}{u_{\\rho}}\n\\right\\} \\\\\n% R A*\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{X}^{T} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{v_{1}}, \\dots , \\color{blue}{v_{\\rho}}\n\\right\\} \\\\\n% N A*\n\\color{red}{\\mathcal{N} \\left( \\mathbf{X}^{T} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{u_{\\rho+1}}, \\dots , \\color{red}{u_{m}}\n\\right\\} \\\\\n% N A\n\\color{red}{\\mathcal{N} \\left( \\mathbf{X} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{v_{\\rho+1}}, \\dots , \\color{red}{v_{n}}\n\\right\\} \\\\\n%\n\\end{align*}\n\nThe conclusion is that the full SVD provides an orthonormal span for not only the two null spaces, but also both range spaces. All these theories can easily be extended to image processing.\nThe right singular vectors associated with the vanishing singular values of $X$ define the null space of the matrix, while the left singular vectors corresponding to the non-zero singular values span the range of $X$. Consequently, the rank of $X$ equals the count of non-zero singular values, which is directly related to the number of non-zero diagonal elements in the singular value matrix $S$. This orthogonal partitioning of the $M$-dimensional vector space mapped by $X$ is essential in applications such as image processing, where distinguishing between the signal and noise components can significantly enhance techniques such as watermarking.\n\n@fig-svd-comparison illustrates the image data's dominant subspace, truncated to $k = 50$ SVD components, alongside its subdominant noise subspace. \n\n:::{#fig-svd-comparison}\n\n![](./Notebooks/figures/svd_comparison.png)\n\nComparison of Original Image, Reconstructed Image after SVD Compression (with $k=40$), and Extracted Noise. These subplots illustrate the effectiveness of SVD in reconstructing the original image while isolating noise components.\n\n::: \n\n\nThis property of SVD effectively facilitates the identification of the rank of $X$, the orthonormal basis for its range and null spaces, and enables optimal low-rank approximations in various norms, thus paving the way for significant advancements in image processing applications, including watermarking, where the relationship between the SVD domain and noisy or watermarked images can be leveraged effectively.\n\n\nTo investigate the influence of the truncation factor $k$ on image quality, experiments were conducted to evaluate the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) values of reconstructed images for various \n$k$ values. The results, summarized in @tbl-truncation_vs_quality, indicate a clear trend: as the truncation factor increases, both PSNR and SSIM improve significantly. This improvement suggests that retaining more singular values enhances the quality of the reconstructed images, thereby preserving essential details and structures. Notably, the PSNR values reach a peak of 39.15 dB and the SSIM values approach 0.93 when  $k$ is set to 1000, indicating high fidelity to the original image.\n\n\n:::{#tbl-truncation_vs_quality}\n| $k$      | PSNR (dB)  | SSIM      |\n|----------|------------|-----------|\n| 1        | 14.445199  | 0.765134  |\n| 5        | 20.347972  | 0.779434  |\n| 10       | 22.906752  | 0.784555  |\n| 20       | 25.443104  | 0.789932  |\n| 50       | 28.667464  | 0.799783  |\n| 100      | 31.237551  | 0.814706  |\n| 200      | 33.401548  | 0.837690  |\n| 400      | 35.248494  | 0.870490  |\n| 600      | 36.602859  | 0.895303  |\n| 800      | 37.881342  | 0.915886  |\n| 1000     | 39.145403  | 0.933217  |\n\n:Relationship between Truncation Factor \\$k \\$ and Image Quality Metrics.\n:::: \n\n\n@fig-psnr_ssim_variation_plot illustrates the relationship between the truncation factor $k$ and the image quality metrics PSNR and SSIM. The horizontal axis represents the truncation factor $k$, while the two curves depict the corresponding PSNR and SSIM values for various $k$ settings.\n\n:::{#fig-psnr_ssim_variation_plot}\n![](./Notebooks/figures/psnr_ssim_variation_plot.png)\n\nVariation of PSNR and SSIM with respect to the truncation factor $k $.\n:::\n\nBy analyzing the plot, one can easily determine the appropriate truncation parameter $k$ needed to achieve a desired PSNR or SSIM value, thereby ensuring optimal fidelity and perceptual quality in the reconstructed image. This graphical representation serves as a practical tool for selecting the truncation factor, facilitating a balance between compression efficiency and image quality. For instance, if a target PSNR value of 35 dB is desired, one can project this value onto the PSNR curve and trace down to the horizontal axis to identify the corresponding $k$ value, which allows for informed decision-making in image compression settings.\n\n## New Role- SVD as a Denoiser \n\nSingular Value Decomposition (SVD) is a powerful mathematical technique that has various applications in image processing, including noise filtering and digital watermarking. \n\nIn the context of noise filtering, SVD can efficiently separate the noise components from the original image signal. The SVD approximates the image matrix by decomposing it into an optimal estimate of the signal and the noise components. This property makes SVD a useful tool for removing noise from images while preserving the quality and recognition of the original content.\n\nIn this study, we assessed the correlation between consecutive reconstructed images as a function of the truncation parameter $k$ in Singular Value Decomposition (SVD).\n\n:::{#fig-corr-slice}\n\n![](./Notebooks/figures/corr-slice.png)\n\nCorrelation between original and reconstructed images from image SVD.\n:::\n\nAs shown in  @fig-corr-slice, the sharp increase in correlation between consecutive reconstructed images as $k$ rises to 200 illustrates SVD’s strong ability to retain key image details even with relatively low truncation levels. This trend suggests that the primary singular values capture essential structural information of the original image, leading to high-fidelity reconstructions while efficiently filtering out less critical components. Given this preservation capacity, we proceed to assess SVD’s denoising capability by calculating the PSNR and SSIM values for both the noisy and denoised images.\n\n@fig-svd_denoising_results illustrates experimental results of the SVD-based denoising process on a high resolution image (20.0 MB, $4480\\times 6133$, at 24 bit depth).\n\n:::{#fig-svd_denoising_results}\n![](./Notebooks/figures/svd_denoising_results.png)\n\nComparison of Original, Noisy, and Denoised Images using SVD.\n\n:::\n\nBy considering the first 50 eigenimages as the image data subspace and the remainder as the noise subspace, and then removing the noise subspace, @fig-svd_denoising_results (c) shows the image after noise removal. \n\nNoise has a disproportionate impact on singular values (SVs) and singular vectors (SCs), with smaller SVs and their corresponding SCs being more severely affected compared to larger SVs and SCs. Experiments validate this phenomenon, as shown in @fig-svd_matrices_comparison, which depicts a 2-dimensional representation of the left and right SCs. This highlights the contrast between the slower changing waveforms of the former SCs and the faster changing waveforms of the latter SCs. \n\n:::{#fig-svd_matrices_comparison}\n![](./Notebooks/figures/svd_matrices_comparison.png)\n\nComparison of the image with the reconstructed traces in the left singular matrix ($U$) and the right singular matrix (V$^T$) of noisy image.\n:::\n\nWhile SVD-based denoising methods have demonstrated promising results, consistency in performance across different images is often challenging, particularly within datasets like BSD400. In such cases, fixing a truncation parameter $k$ does not always yield optimal denoising performance. This limitation arises because the variance of image information captured in the singular values varies across different images. Consequently, an adaptive approach is preferable over a fixed truncation level for retaining significant image details while effectively suppressing noise.\n\nTo address this, we propose dynamically thresholding the singular values rather than fixing $k$ for truncation. By removing singular values below a specific threshold, we focus on preserving image components that substantially contribute to the signal, thereby enhancing denoising effectiveness. Experimentally, we observe that setting the truncation threshold for singular values to $0.618 \\times \\text{mean}(\\Sigma)$, where $\\Sigma$ denotes the diagonal matrix of singular values, achieves optimal denoising. This threshold corresponds to approximately 61.8\\% of the mean singular value magnitude, which is notably effective in retaining essential image features while filtering out high-frequency noise components.\n\nOur empirical results further validate this approach, revealing that the dynamic thresholding method consistently produces higher Peak Signal-to-Noise Ratio (PSNR) values across a variety of images in the BSD400 dataset when compared to fixed-$k$ truncation. This improvement underscores the robustness of the adaptive threshold in aligning the denoising process with each image's inherent structural properties, thereby achieving superior fidelity to the original image.\n\nThe effectiveness of the adaptive thresholding approach in Singular Value Decomposition (SVD) for image denoising is exemplified in @fig-svd_denoising_resultsBSD. This figure displays an image from the BSD400 dataset, showcasing the original, noisy input and denoised output along with the PSNR and SSIM measures.\n\n:::{#fig-svd_denoising_resultsBSD}\n![](./Notebooks/figures/svd_denoising_resultsBSD.png)\n\nComparison of Original, Noisy, and Denoised images using SVD on BSD400 sample image.\n:::\n\n\n### Comparison and Advantages of SVD-Based Denoising in Medical Imaging\n\nIn medical imaging, one major hurdle is the lack of a clean reference image, which complicates the task of denoising. Creating datasets with perfect reference images is often impossible. This challenge is made even harder by the noise that arises from natural physiological movements, which can introduce dynamic noise into MRI, CT, and ultrasound scans, even if the patient is mostly still.\n\nMany traditional denoising methods depend on machine learning algorithms that are optimized with the help of reference images or alternative *doubly noisy* images that serve as substitutes for the ideal ground truth. For example, recent research, including a study by Floquet et al. (2024), has shown that using noisy reference images can effectively help adjust the parameters for denoising techniques. \n\nIn these scenarios, optimization methods such as the Scipy optimizer and stochastic gradient optimization are applied to refine the algorithms, aiming to reduce the Mean Square Error (MSE). This fine-tuning process has resulted in impressive outcomes, achieving a Peak Signal-to-Noise Ratio (PSNR) of 33.8, indicating a significant improvement in image quality (reference: <https://sijuswamy.github.io/Denoising-Manuscript/>).\n\nOn the other hand, an SVD-based approach has the potential to avoid the requirement for having any ground truth reference which could be more concept around it altogether. Using the SVD it is then possible to filter noise depending on the singular values relating to structural image information. The denoising based on SVD yielded a PSNR of 32.27 on a similarly noisy image in a comparative experiment—a value with 5\\% from PSNRs achieved by parameter-optimized methods of denoising without a need of a reference image. However, its independence from ground truth is an advantage for medical applications where unsupervised methods may reduce cost and complexity of operation. \n\nThese results could be further improved with a hybrid method that combines a first stage of initial denoising and even using a noisy image as a prior together with a method with optimized parameters then using SVD to help capture dominant features in images. Even without a reference image, SVD is able to act as an adaptive, standalone denoising solution and opens sustainable possibilities in the medical innovations context where the reference is commonly unknown and the denoising process is crucial for the meaningful diagnosis.\n\n## Image Forensics with SVD\n\nIn the contemporary digital era, digital forensics has become crucial for combating counterfeiting and manipulation of digital evidence aimed at illicit profit or legal evasion. Forensic research encompasses various domains, including steganography, watermarking, authentication, and labeling. Numerous solutions have been developed to fulfill consumer demands, such as authentication systems, DVD copy control, and hardware/software watermarking.\n\nSingular Value Decomposition (SVD) serves as a potent method in this realm, concentrating significant signal energy into a minimal number of coefficients while adapting to local statistical variations in images. As an image-adaptive transform, SVD requires careful representation to ensure accurate data retrieval.\n\n### Image watermarking with scaled additive approach\n\nSVD-based watermarking techniques exploit the stability of singular values (SVs), which represent the image’s luminance. Minor alterations in these values do not drastically compromise the visual quality of the host image. Methods typically utilize either the largest or smallest SVs for watermark embedding, employing additive techniques or quantization. For instance, D. Chandra's methodology involves the additive incorporation of scaled watermark singular values into the singular values of the host image $X$ @chandra2002digital:\n\n$$\nSV_{\\text{modified}} = SV_{\\text{original}} + \\alpha \\cdot \\text{Watermark}\n$$\n\nHere, $\\alpha$ denotes a scaling factor, allowing for effective watermark integration while maintaining the fidelity of the original image.\nThe scaled additive algorithm for image watermarking is given in the following algorithm.\n\n:::: {.algorithm #algo-SA}\n**Algorithm**: Scaled Additive Approach for Image Watermarking\n\n**Inputs**: \n- Cover image $A$\n- Watermark $W$\n- Scaling factor $\\alpha$\n\n**Outputs**: \n- Watermarked image $A_w$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Watermark Embedding**:\n    - Compute SVD of the cover image $A$: $[U_1, S_1, V_1] \\gets \\text{svd}(A)$\n    - Modify the singular values by adding the scaled watermark: $\\text{temp} \\gets S_1 + (\\alpha \\cdot W)$\n    - Compute SVD of the modified singular matrix: $[U_w, S_w, V_w] \\gets \\text{svd}(\\text{temp})$\n    - Reconstruct the watermarked image: $A_w \\gets U_1 \\cdot S_w \\cdot V_1^T$\n\n2. **Watermark Extraction**:\n    - Compute SVD of the watermarked image $A_w$: $[U_{w1}, S_{w1}, V_{w1}] \\gets \\text{svd}(A_w)$\n    - Reconstruct the matrix $D$ using the new singular values: $D \\gets U_w \\cdot S_{w1} \\cdot V_w^T$\n    - Extract the watermark: $W_e \\gets \\frac{D - S_1}{\\alpha}$\n\n3. **Verification**:\n    - If $W == W_e$:\n        - The image is **not attacked**.\n    - Else:\n        - The image has been **attacked**.\n:::: \n\n\n@fig-inageforensic represent the typical workflow of image forensic.\n\n:::{#fig-inageforensic}\n![](./Notebooks/figures/inageforensic.png)\n\nImage forensic workflow.\n:::\n\n@fig-image-forensicSVD demonstrate the watermarking of images using SVD. Since the extracted watermark is exactly what we embedded in the covering image, no attack is detected @Sharma2024.\n\n:::{#fig-image-forensicSVD}\n\n![](./Notebooks/figures/image-forensicSVD.png)\n\n\nDemonstration of watermarking a confidential image using SVD.\n::: \n\nIn this first evaluation the PSNR value of watermarked image is 29.08 and the extraction is successful.\n\nTo evaluate the robustness and effectiveness of the Singular Value Decomposition (SVD)-based watermarking approach on a broader spectrum of images, using the BSD400 dataset offers a comprehensive test bed. The BSD400 dataset contains a wide variety of images with intricate textures, fine details, and different visual complexities, making it ideal for testing how well the SVD-based watermarking technique can embed and extract watermarks under varied conditions.\n\nBy selecting images with delicate content, such as running letters and intricate textures, the goal is to assess how well the watermark remains visually unobtrusive in complex scenes while being resilient to potential attacks (such as noise, compression, or cropping). This method will also allow for calculating objective quality metrics like PSNR  across different image categories, providing a robust understanding of watermark quality and imperceptibility.\n\n### Image watermarking with adaptive scaled additive approach\n\nUsing the *test\\_077.png* image from the BSD400 dataset, we employ an adaptive approach to watermarking that integrates D. Chandra’s scaled addition technique with a balanced formula @chandra2002digital:\n\n\\begin{equation}\n    \\text{SV}_{\\text{mod}} = (1 - \\alpha) \\cdot \\text{SV}_{\\text{img}} + \\alpha \\cdot \\text{Watermark}\n\\end{equation}\n\nAlgorithm for the adaptive scaled additive (ASA) approach is shown below.\n\n:::: {.algorithm #algo-ADAPTIVE}\n**Algorithm**: Scaled Additive Adaptive Approach for Image Watermarking\n\n**Inputs**: \n- Cover image $A$\n- Watermark $W$\n- Scaling factor $\\alpha$\n\n**Outputs**: \n- Watermarked image $A_w$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Watermark Embedding**:\n    - Compute SVD of the cover image $A$: $[U_1, S_1, V_1] \\gets \\text{svd}(A)$\n    - Modify the singular values by adding the scaled watermark adaptively: $\\text{temp} \\gets (1 - \\alpha) \\cdot S_1 + (\\alpha \\cdot W)$\n    - Compute SVD of the modified singular matrix: $[U_w, S_w, V_w] \\gets \\text{svd}(\\text{temp})$\n    - Reconstruct the watermarked image: $A_w \\gets U_1 \\cdot S_w \\cdot V_1^T$\n\n2. **Watermark Extraction**:\n    - Compute SVD of the watermarked image $A_w$: $[U_{w1}, S_{w1}, V_{w1}] \\gets \\text{svd}(A_w)$\n    - Reconstruct the matrix $D$ using the new singular values: $D \\gets U_w \\cdot S_{w1} \\cdot V_w^T$\n    - Extract the watermark: $W_e \\gets \\frac{D - S_1}{\\alpha}$\n\n3. **Verification**:\n    - If $W == W_e$:\n        - The image is **not attacked**.\n    - Else:\n        - The image has been **attacked**.\n:::: \n\nThis new approach modifies the singular values by proportionally blending the image's original details with the watermark content based on the parameter $\\alpha$. The adaptive blend allows for fine-tuning the watermark’s influence, thus optimizing both its visibility and robustness.\n\nThis adaptive watermarking formula . The formula  provides *high readability* and *forensic resilience* and enables the watermark to stay subtle within the image, while enhancing durability against forensic attacks. This allows for improved image readability and detail retention, particularly in face images like *test\\_077.png*.\n\nAs a next step, experiment with various values of $\\alpha$ to fine-tune the watermark’s visibility and robustness. Additionally, evaluate the  PSNR (Peak Signal-to-Noise Ratio) values to assess the balance achieved by the adaptive method.\n\n### Perceptual Forensic Approach for Image Watermarking\n\nThe perceptual forensic approach for image watermarking, introduced by Sadek, represents a significant advance in singular value decomposition (SVD)-based watermarking techniques by targeting robustness and imperceptibility in forensic applications. This approach, termed Global SVD (GSVD), employs a private (non-blind) methodology, making it suitable for sensitive forensic tasks where watermark retrieval without the original image is critical. In this technique, the watermark data is optimally embedded within the host image’s less significant subspace, often referred to as the *noise subspace*. This embedding choice leverages the low-impact regions of the image’s singular value structure, thus maintaining the original image quality while preserving the watermark’s resilience.\n\nA key innovation in Sadek's method is the scaled addition of the watermark data subspace into the host image's singular values. Traditional SVD-based watermarking techniques typically rely on a direct scaled addition of watermark values to the singular values of the cover image. However, this conventional approach often neglects the varying magnitude across the singular value spectrum, leading to uneven watermark integration that may affect image quality. Sadek's approach addresses this limitation by *flattening* the range of singular values before watermark embedding, which smooths out the differences in value magnitude and allows for a more perceptually consistent embedding. This adjustment not only enhances the watermark’s imperceptibility but also strengthens its resilience against potential distortions or attacks, which are common in forensic scenarios.\n\nThe GSVD-based perceptual forensic approach is inherently adaptable, allowing the embedded watermark to withstand different types of image manipulations depending on the robustness requirements. By embedding the watermark within the less visually significant regions of the singular value matrix, the GSVD technique achieves a balance between maintaining high perceptual quality and ensuring the watermark’s durability.\n\nThe algorithm for the perceptual forensic method for watermarking is given below.\n\n:::: {.algorithm #algo-PFA}\n**Algorithm**: Perceptual Forensic Watermarking using SVD\n\n**Inputs**:\n- Cover image $X$\n- Watermark $W$\n- Scaling factor $\\alpha$\n- Threshold parameter $k$\n\n**Outputs**:\n- Watermarked image $Y$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Input**: Read cover image $X$ and watermark $W$.\n\n2. **Compute SVD**: Perform the Singular Value Decomposition (SVD) on both $X$ and $W$:\n    - $X = U_h S_h V_h^T$\n    - $W = U_w S_w V_w^T$\n\n3. **Define Scaled Addition for Modified Singular Values**:\n    - For $i = M - k$ to $M$, with $q = 1$ to $k$:\n        - $S_m(i) = S_h(i) + \\alpha \\cdot \\ln(S_w(q))$\n    - For all other $i$, set $S_m(i) = S_h(i)$.\n\n4. **Form the Watermarked Image** $Y$:\n    - $Y = U_h S_m V_h^T$\n\n5. **Reconstruct Singular Values for Watermark Extraction**:\n    - For $i = M - k$ to $M$:\n        - $S'_w(i) = \\exp\\left(\\frac{S_m(i) - S_h(i)}{\\alpha}\\right)$\n\n6. **Extract the Watermark**:\n    - $W_e = U_w S'_w V_w^T$\n\n7. **Reconstruction Check**: Verify watermark accuracy by comparing $W_e$ with $W$.\n\n:::: \n\nThis method is particularly useful in forensic watermarking applications that demand both high fidelity and robustness, such as in medical imaging and high-resolution photographic forensics, where maintaining image integrity is paramount. This technique’s ability to fine-tune watermark robustness based on singular value dynamics, while preserving the host image quality, marks it as a promising advancement in forensic watermarking applications.\n\n@fig-perceptualplusscaledaddition illustrate the results of the adaptive watermarking technique using D. Chandra’s approach and the perceptual forensic approach, applied to a sample image in the BSD400 image dataset at $\\alpha=0.01$. The first row shows the original and watermark-modified images, while the second row demonstrates the images after the application of direct perceptual forensic watermarking and the images with a Gaussian noise for forensic testing @sadek2012svd.\n\n:::{#fig-perceptualplusscaledaddition}\n![](./Notebooks/figures/perceptualplusscaledaddition.png)\n\nResults of Watermarking with scaled addition and perceptual forensic approaches using SVD.\n:::\n\nFrom @fig-perceptualplusscaledaddition (d), the perceptive forensic approach is a winner in maintaining the image details in watermarking and this fact is substantiated with @tbl-PSNRall . Also it is noted that noising after watermarking the image through SVD produces almost same PSNR across the experiments. A detailed comparison of  image detailing after watermarking on uncompressed and compressed version of the BSD400 image *test\\_077.png* is shown in Table @tbl-PSNRcomparison.\n\n:::{#tbl-PSNRall}\n| Image type                       | $\\alpha=0.01$ (SA) | $\\alpha=0.01$ (ASA) | $\\alpha=0.1$ (SA) | $\\alpha=0.1$ (ASA) | $\\alpha=0.2$ (SA) | $\\alpha=0.2$ (ASA) | $\\alpha=0.3$ (SA) | $\\alpha=0.3$ (ASA) |\n|----------------------------------|--------------------|---------------------|-------------------|--------------------|-------------------|--------------------|-------------------|--------------------|\n| Watermarked                      | 61.84             | 46.41              | 38.83            | 26.56             | 30.82            | 20.68             | 25.16            | 17.35             |\n| Noised after watermarked         | 20.70             | 20.66              | 20.66            | 19.74             | 20.48            | 17.86             | 19.91            | 16.06             |\n| Watermarked & Compressed         | 49.32             | 44.56              | 38.60            | 26.54             | 31.07            | 20.70             | 26.03            | 17.49             |\n\nPeak Signal to Noise Ratio of various watermarked versions of *test\\_077* image from BSD400 dataset under scaled additive (SA) and adaptive scaled additive (ASA) approaches.\n:::\n\nFrom @tbl-PSNRcomparison, it is clear that both scaled additive and adaptive scaled additive approaches gives maximum image detaining in the watermarked state is at lower values of $\\alpha$. Maintaining readability and security is the key aspect in image forensic. So $\\alpha=0.01$ is a safe choice. At the same level of scaling the perceptual forensic approach is used in the BSD400 image. Comparison of PSNR values of scaled additive, adaptive scaled additive and the perceptual forensic approaches at $\\alpha=0.01$ is shown in @tbl-PSNRall.\n\n:::{#tbl-PSNRcomparison}\n| Image type                       | Scaled Additive                 | Adaptive Scaled Additive                 |  Perceptual Forensic |\n|----------------------------------|---------------------------------|------------------------------------------|-------------------------------------|\n| Watermarked                      | 61.84                           | 46.41                                    | 75.17                               |\n| Noised after watermarked         | 20.70                           | 20.66                                    | 20.68                               |\n| Watermarked & Compressed         | 49.32                           | 44.56                                    | 38.87                               |\n\n\nPeak Signal to Noise Ratio of various watermarked versions of *test\\_077* image from BSD400 dataset under scaled additive (SA), adaptive scaled additive (ASA) and perceptual forensic approaches.\n\n:::\n\nWatermarking through Singular Value Decomposition (SVD) is emerging as a promising method in the field of medical imaging to protect data integrity and authenticity. In our study, an available CT image was used to embed a watermark using both a scaled addition method and an adaptive perceptual forensic approach.\n\nWhen unaltered, the watermark was effectively extracted, showing that SVD-based watermarking can preserve image integrity under normal conditions. However, when noise was introduced after embedding, the extracted watermark showed substantial degradation, highlighting the technique’s sensitivity to potential tampering.\n\n:::{#fig-BrainCTPerceptualWM}\n\n![](./Notebooks/figures/BrainCTPerceptualWM.png)\n\nComparison of Brain CT images: (a) Original Brain CT Image, (b) Watermarked with scaled additive approach, (c) Watermarked with perceptual forensic approach.\n\n:::\n\nThe effect of watermarking on the Brain CT image using different image forensic approaches is shown in @fig-BrainCTPerceptualWM. The scaled addition method achieved a Peak Signal-to-Noise Ratio (PSNR) of 33.93, balancing visibility and quality. Meanwhile, the perceptual forensic approach, designed to better manage watermark strength relative to image details, attained a PSNR of 102.87, maintaining high image fidelity. These results indicate that SVD-based watermarking techniques can be effective for medical imaging applications, where preserving diagnostic quality while protecting image authenticity is critical. This adaptive method offers a balanced approach to ensure data protection without compromising readability and detail in medical images.\n\n## Conclusion\n\nThis study investigated SVD-based image processing applications, specifically focusing on image compression, image denoising, and image forensic analysis. Through experimental analysis on high-resolution images, the BSD400 dataset, and medical images, this work examined the effectiveness of two watermarking approaches: scaled additive embedding and perceptual forensic embedding. In the scaled additive approach, the watermark was scaled and embedded within the singular values of the image before full SVD decomposition. To improve the adaptability across images with varying detail levels, an adaptive scaling mechanism was introduced, achieving high-quality image blending with minor scaling factors $(\\alpha < 0.02)$. \n\nIn the perceptual forensic approach, watermark embedding targeted distinct ranges of singular values, optimizing the visibility and robustness of the watermark under forensic scrutiny. This method employed a locally adaptive SVD, enhancing watermark resilience while preserving essential image details, making it effective for applications requiring forensic analysis. Additionally, image denoising was implemented as an automatic fine-tuning step to reduce noise introduced during watermarking, further solidifying the watermark's readability and stability.\n\nThis work is a partial replication and extension of Sadek’s review on SVD-based image processing applications, which highlights the state-of-the-art methods and challenges in SVD applications for image processing @sadek2012svd. By incorporating aspects of automated fine-tuning for denoising algorithms in the watermarking process, this study contributes a refined understanding of how SVD can be leveraged to balance image quality and watermark resilience. Overall, the findings affirm that SVD-based techniques fulfill the study’s objectives across compression, denoising, and forensic applications, providing a flexible and robust approach to image processing that is effective across various image types and contexts. Future work may explore additional fine-tuning and new methodologies to enhance forensic robustness and adaptive capabilities in real-world applications.\n\n## References\n\n::: {#refs}\n:::","srcMarkdownNoYaml":"\n\n## Introduction\n\nLinear Algebra based SVD is a powerful method to be used within the realm of digital image processing. SVD decomposes a matrix in to three constitutive matrices U, S, and V, thus making it possible to represent an image using a fewer number of values @moonen1992singular. This characteristic has practical usage such as for image compression by keeping few singular values in $S$ matrix and stores the characteristics feature of the image, hence reduce storage.\n\nResearches suggested that It can be done if appropriate number of singular values maintained and image can be compressed at higher ratio with good quality The number of singular values kept (and thus the size of the image to be compressed) is always not more (and often far less) than the number of pixels in the original image. Thus, SVD turns out to be a relatively strong technique for applications where a minimum number of storage space and bandwidth is required to be preserved during transmission of signals such as in satellite imagery, medical imaging and photo enhancement.\n\nSingular Value Decomposition (SVD) is a powerful mathematical technique with a diverse range of applications in image processing. While its capabilities are well-established, there remains untapped potential in fully harnessing its versatility. This paper delves into the rich properties of SVD and demonstrates how they can be leveraged across various image processing tasks, such as compression, watermarking, and quality assessment.\n\nThe study presents several key findings. First, the experiments validate known but underutilized characteristics of Singular Value Decomposition (SVD) in the context of image processing. This serves to aid ongoing efforts aimed at enhancing the application of these SVD characteristics. Second, the research identifies new trends and challenges faced in the application of SVD for image processing. Some of these trends are corroborated by experimental data, while others require additional verification. Finally, this work lays the groundwork for future investigations, highlighting promising avenues for further exploration and development.\n\nOverall, this work offers a comprehensive examination of the rich properties of Singular Value Decomposition (SVD) and its multifaceted applications in the field of image processing. By shedding light on both the established and emerging aspects of SVD, the study paves the way for more efficient and innovative applications of this powerful mathematical technique.\n\n\n## Related works\n\nAndrews and Patterson (1976) explored the significant role of Singular Value Decomposition (SVD) techniques in the realm of digital image processing, particularly for applications that demand high computational power and precise imaging capabilities. Their work highlighted the versatility of SVD methods, which are applicable not only to images but also to broader representations of point spread functions (PSF) and impulse responses. The authors framed these techniques as natural extensions of linear filtering theory, thereby situating SVD within established methodologies for image enhancement and restoration  @andrews1976singular.\n \n\n Moonen et al. (1992) expanded upon the established QR updating scheme by introducing a more versatile and generally applicable method for updating the Singular Value Decomposition (SVD). Their approach enhances the QR updating technique by integrating a Jacobi-type SVD procedure. This innovative combination allows for the effective restoration of an acceptable approximation of the SVD after only a few SVD steps following each QR update. The authors demonstrated that this method not only maintains a comparable computational cost to that of traditional QR updating but also significantly reduces the overall computational burden associated with SVD updates. @moonen1992singular.\n \n\nIn their paper, Kakarala and Ogunbona (2001) introduced a novel multiresolution form of Singular Value Decomposition (SVD) designed for enhanced signal analysis and approximation. Recognizing the inherent strengths of traditional SVD—specifically its optimal decorrelation and subrank approximation properties—the authors expanded upon these foundations by developing a multiresolution approach that maintains linear computational complexity  @kakarala2001signal.\n \n\nD Chandra (2002) introduced a novel watermarking technique- scaled additive approach- for digital images that employs Singular Value Decomposition (SVD) as a foundational method. The paper provides comprehensive simulation results that showcase the robustness of this SVD-based watermarking approach against various common image degradations, underscoring its effectiveness in preserving watermark integrity in challenging conditions  @chandra2002digital.\n \n\nSadek (2008) explored the increasing prominence of Singular Value Decomposition (SVD) as a robust and reliable technique for orthogonal matrix decomposition in the field of signal processing, particularly in the context of watermarking and data hiding. The author highlighted the fundamental properties of SVD, such as its conceptual clarity and stability, which contribute to its growing popularity in various applications\nIn the realm of watermarking, many researchers have focused on leveraging the singular values of host images to embed hidden information. However, Sadek introduced a critical examination of these SVD-based watermarking techniques by presenting a counterfeiting attack specifically targeting the embedded watermark information within the singular values. The study underscored the inherent vulnerabilities of this class of watermarking methods, revealing how singular values can be easily compromised through a broad spectrum of image processing operations and deliberate attacks  @sadek2008blind.\n \n\nSadek (2012) explores the potential of Singular Value Decomposition (SVD) as a transformative tool in the realm of image processing. The paper presents a comprehensive experimental survey highlighting SVD's efficacy across various imaging applications and proposed the perceptual forensic approach in image watermarking. Recognizing SVD as an attractive algebraic transform, Sadek emphasizes that its application in image processing is still in its early stages despite its well-documented advantageous properties  @sadek2012svd.\n \n\nIn their study, Kahu and Rahate (2013) investigated the application of Singular Value Decomposition (SVD) as a technique for image compression, emphasizing its effectiveness in expressing image data through a limited number of eigenvectors determined by the image's dimensionality. They highlighted the significance of psycho-visual redundancies inherent in images, which enable compression without compromising the quality of the visual output @kahu2013image.\n\n## Introduction to Singular Value Decomposition (SVD)\n\nIn linear algebra, Singular Value Decomposition (SVD) is a fundamental factorization technique for rectangular real or complex matrices. It provides a structure similar to the diagonalization of symmetric or Hermitian square matrices, utilizing eigenvectors as a basis. SVD is particularly advantageous due to its stability and effectiveness, allowing for decomposition into a set of linearly independent components, each contributing uniquely to the matrix’s structure.\n\nFor a digital image $X$ of size $M \\times N$ (where $M \\geq N$), the SVD of $X$ is represented as:\n\n$$\nX = U \\Sigma V^T\n$$\n\nwhere $U$ is an $M \\times M$ orthogonal matrix, $V$ is an $N \\times N$ orthogonal matrix, and $\\Sigma$ is an $M \\times N$ diagonal matrix. The matrices $U = [u_1, u_2, \\ldots, u_m]$ and $V = [v_1, v_2, \\ldots, v_n]$ contain the left and right singular vectors of $X$, respectively, and $\\Sigma$ holds the singular values $\\sigma_i$ of $X$ along its diagonal in descending order of magnitude, with all off-diagonal elements set to zero.\n\nIn this setup, $U$ and $V$ are unitary orthogonal matrices, meaning that each column vector has a unit norm and is orthogonal to others. The singular values $\\sigma_i$ in $\\Sigma$ indicate the energy contribution of each corresponding component, while each pair of singular vectors from $U$ and $V$ defines the spatial orientation or geometry of these components.\n\nThe left singular vectors (LSCs) of $X$ are the eigenvectors of the matrix $X X^T$, while the right singular vectors (RSCs) are eigenvectors of $X^T X$. Each singular value represents the 2-norm of its associated component, with the largest singular values capturing the most significant patterns or features in the data. This property allows SVD to effectively highlight essential image components while suppressing noise or less critical features, making it ideal for applications focused on key structural features in image processing  @andrews1976singular.\n\n## SVD- A New Tool for Image Processing\n\nSingular Value Decomposition (SVD) is a powerful and robust method for orthogonal matrix decomposition, widely valued for its stability and conceptual clarity. These attributes have led to its growing popularity in signal processing, particularly in the domain of image processing. As an algebraic transformation, SVD brings several advantageous properties to imaging, which this section examines. While some of these properties are well-utilized, others present opportunities for further exploration and application.\n\nSeveral key properties of SVD make it particularly useful in image processing. These include maximum energy packing, efficient solutions to least squares problems, calculation of matrix pseudo-inverses, and multivariate analysis @strang2022introduction. An essential feature of SVD is its relationship to matrix rank and its ability to approximate matrices at a given rank. Digital images, often represented as low-rank matrices, can be effectively described by a limited number of eigenimages. This approach allows image signals to be manipulated in two distinct subspaces @kamm1998svd.\n\nIn the sections that follow, key hypotheses related to these properties are proposed and validated. For completeness, the theoretical SVD theorems relevant to these applications are summarized, followed by a practical review of SVD properties with experimental demonstrations.\n\n### SVD subspaces and architecture\n\nThe SVD method effectively divides a matrix into two orthogonal subspaces: the dominant and subdominant subspaces. This division corresponds to a partitioning of the $M$-dimensional vector space, separating primary signal components from secondary ones [@andrews1976singular, @kamm1998svd]. Such a property is particularly advantageous in applications like noise filtering and digital watermarking, where isolating signal elements from noise or embedding data is crucial [@chandra2002digital, @sadek2012svd].\n\nIn the context of image processing, SVD architecture further highlights its utility. For an image decomposed via SVD, each singular value (SV) represents the luminance level of a specific image layer, while the associated singular vectors (SCs) provide the geometric structure of that layer. Generally, prominent image features align with eigenimages associated with larger singular values, while smaller singular values correspond to components associated with noise @kahu2013image.\n\n### PCA versus SVD\n\nPrincipal Component Analysis (PCA), also known as the Karhunen-Loève Transform (KLT) or the Hotelling Transform, is a technique for computing dominant vectors that represent a given dataset. PCA achieves an optimal basis for minimum mean squared reconstruction of data and is computationally based on the SVD of the data matrix or the eigenvalue decomposition of the data covariance matrix. SVD is closely related to the eigenvalue-eigenvector decomposition of a square matrix $X$ into $V\\Lambda V^T$, where $V$ is orthogonal, and $\\Lambda$ is diagonal. Notably, the matrices $U$ and $V$ in SVD correspond to eigenvectors of $XX^T$ and $X^TX$, respectively. If $X$ is symmetric, the singular values of $X$ are the absolute values of its eigenvalues [@strang2022introduction,@deisenroth2020mathematics].\n\n### SVD Multiresolution\n\nSVD is known for its maximum energy packing capability, making it particularly useful for applications requiring multiresolution analysis. This approach enables statistical characterization of images across multiple resolutions, with SVD decomposing a matrix into orthogonal components that allow optimal sub-rank approximations. The multiresolution properties of SVD provide a framework to measure several critical image characteristics at various resolutions, including isotropy, sparsity of principal components, self-similarity under scaling, and decomposition of mean squared error into meaningful components @kakarala2001signal.\n\n### SVD oriented energy\n\nIn SVD-based analysis of oriented energy, both the rank of the problem and the signal space orientation are identifiable. SVD allows decomposition into linearly independent components, each with its own energy contribution. Represented as a linear combination of principal components, SVD highlights dominant components that define the rank of the observed system, with a few key components effectively capturing the system's structure. The concept of oriented energy is beneficial for separating signals from different sources or selecting signal subspaces with maximal activity and integrity. Singular values in SVD represent the square root of energy in the corresponding principal direction, with the primary direction often aligned with the first singular vector $V_1$. Dominance accuracy can be measured by evaluating the difference, or normalized difference, between the first two singular values [@kakarala2001signal, @sadek2008blind].\n\n\nMany properties of SVD remain underutilized in image processing applications. Subsequent sections will experimentally explore these unexploited properties to demonstrate their potential for enhancing various image processing techniques. Additional research is essential to fully harness this versatile transformation in new and evolving applications.\n\n## Optimal Approximation and Noise Isolation Using SVD\n\nThe SVD’s unique ability to distinguish image content from noise is critical for efficient matrix approximation. In an SVD-decomposed matrix, the highest singular values capture the most essential components of the image, while lower singular values represent noise. By reconstructing the matrix with only the top $k$ singular values—forming an approximation $X_k = U_k \\Sigma_k V_k^T$—SVD yields an optimal representation that preserves primary image features while suppressing noise. This characteristic makes SVD ideal for noise filtering, compression, and forensic applications, where detectable noise patterns are useful in watermarking and signal integrity assessment.\n### Rank approximation using SVD}\n\nSingular Value Decomposition (SVD) facilitates low-rank approximation, enabling optimal sub-rank representations by emphasizing the largest singular values that encapsulate the majority of the energy within an image. SVD illustrates that a matrix can be expressed as a sum of rank-one matrices. Given a matrix $X \\in \\mathbb{R}^{m \\times n}$ with $p = \\min(m,n)$, the approximation can be represented as a truncated matrix $X_k$ with a specified rank $k$. The representation is formulated as follows:\n\n$$\nX \\approx X_k = \\sum_{i=1}^{k} s_i u_i v_i^T,\n$$\n\nwhere $s_i$ are the singular values, $u_i$ are the left singular vectors, and $v_i$ are the right singular vectors. Each term $s_i u_i v_i^T$ corresponds to a rank-one matrix, leading to the conclusion that $X$ is the sum of $k$ rank-one matrices. This approximation captures as much of the *energy* of $X$ as possible while maintaining a rank of at most $k$. Here, *energy* is quantified using the 2-norm or Frobenius norm.\n\nThe outer product $u_i v_i^T$ results in a matrix of rank 1, requiring $M + N$ storage compared to $M \\times N$ for the original matrix. For truncated SVD transformations with rank $k$, the required storage space is reduced to $(m+n+1)k$, demonstrating the efficiency of SVD in applications such as image compression and watermarking.\n\n## Example of SVD Application: Image Reconstruction\n\nIn this section, we demonstrate the application of Singular Value Decomposition (SVD) on a JPEG image obtained from the internet. The image is subjected to low-rank approximation using SVD to explore its effectiveness in image reconstruction and compression.\n\nFor this experiment, we set the rank $k = 40$. The original image, denoted as $X$, is decomposed into its singular values and singular vectors as follows:\n\n$$\nX = U S V^T,\n$$\n\nwhere $U$ is an orthogonal matrix containing the left singular vectors, $S$ is a diagonal matrix of singular values, and $V^T$ contains the right singular vectors. By retaining only the top $k$ singular values and their corresponding singular vectors, we can reconstruct a low-rank approximation of the image:\n\n$$\nX_k \\approx \\sum_{i=1}^{k} s_i u_i v_i^T.\n$$\n\nIn this instance, the low-rank approximation captures a significant portion of the image's energy, effectively preserving the essential visual features while reducing the noise and detail represented by the higher-order singular values.\n\nThe reconstructed image using $k = 40$ is displayed in [@fig-svdreconstruction]. This result illustrates the ability of SVD to maintain the overall structure and key characteristics of the original image while achieving a notable reduction in data size. The efficiency of this low-rank approximation highlights the potential of SVD for applications in image compression and restoration, allowing for storage savings without substantial loss of quality.\n\nThis example underscores the practical utility of SVD in image processing, offering a powerful tool for manipulating image data in various applications, including compression, denoising, and feature extraction.\n\n:::{#fig-svdreconstruction}\n![](./Notebooks/figures/SVD1.png)\n\nReconstructed image using SVD with low-rank approximation (k=40).\n:::\n\n## Secrets of left and right singular matrices\n\nThe matrices $U$ and $V^T$ provide crucial insights into the structural characteristics of the image, specifically the column space and row space representations.\n\nThe left singular matrix $U$ captures the column space of the image, which represents the various features and patterns present in the image across its vertical axis. In contrast, the right singular matrix $V^T$ captures the row space of the image, representing patterns across the horizontal axis. By visualizing the first two components of these matrices, we can reconstruct the primary patterns within the image (refer @fig-SVD_components).\n\nThe first two components of the column space from matrix $U$ highlight the dominant vertical patterns, while the first two components of the row space from matrix $V^T$ reveal the dominant horizontal patterns. This reconstruction allows for a clear interpretation of how the image is constructed from these fundamental features, showcasing the spatial relationships inherent in the image data.\n\nIt is important to note that the components of $V$ associated with the smallest singular values correspond to noise in the image. This noise resides in the null space of the image matrix $X$ and contributes minimally to the overall structure of the image. By identifying these components, we can effectively distinguish between the essential features of the image and the extraneous noise that may obscure its true representation.\n\n:::{#fig-SVD_components}\n\n![](./Notebooks/figures/SVDcomponents.png)\n\nVisualization of the components obtained from the SVD of the image.\n:::\n\nFigure [@fig-singular_value_distribution} displays the log-mod distribution of the singular values from the SVD of the image, providing insight into the energy contributions of each component.\n\n\n:::{#fig-singular_value_distribution}\n\n![](./Notebooks/figures/singular-value-distribution.png)\n\nDistribution of the singular values of the image.\n:::\n\nThe log-mod distribution of the singular values reveals crucial information about the image's structure and the underlying data's dimensionality. The singular values, arranged in descending order, represent the amount of energy each corresponding eigenimage contributes to the overall image representation.\n\nFrom [@fig-singular_value_distribution], a rapid decay in singular values indicates that a small number of components capture the majority of the image's energy, signifying a low-rank structure. This property is advantageous for compression, as it suggests that the image can be approximated using fewer outer products of rank-one matrices, thus minimizing information loss.\n\nThe slope of the log-mod distribution further elucidates the significance of each singular value; a steep drop-off signifies that most information is concentrated in the first few singular values, while the tail end, characterized by smaller singular values, is associated with noise and less informative features of the image. This insight allows for strategic selection of singular values in applications such as compression and denoising, where retaining the dominant components while discarding those associated with lower energy can enhance the overall quality of the reconstructed image.\n\n### Image quality metrics\n\nIn the context of image compression and reconstruction using Singular Value Decomposition (SVD), it is essential to evaluate the quality of the reconstructed image. Three popular metrics for assessing image quality are the Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). Each of these metrics provides a different perspective on the quality of the reconstructed image compared to the original.\n\n#### Mean Squared Error (MSE)\n\nThe Mean Squared Error is a measure of the average squared differences between the original and reconstructed images. It is defined mathematically as:\n\n$$\n\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (I(i) - \\hat{I}(i))^2\n$$\n\nwhere $I(i)$ is the pixel value of the original image, $\\hat{I}(i)$ is the pixel value of the reconstructed image, and $N$ is the total number of pixels in the image. Lower MSE values indicate better image quality.\n\n#### Peak Signal-to-Noise Ratio (PSNR)\nThe Peak Signal-to-Noise Ratio is a logarithmic measure that compares the maximum possible power of a signal to the power of corrupting noise that affects the fidelity of its representation. It is given by:\n\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n$$\nwhere $\\text{MAX}$ represents the maximum pixel value (e.g., 255 for 8-bit images). Higher PSNR values indicate better image quality, as they correspond to lower MSE values. Higher PSNR values generally indicate better quality of the reconstructed image.\n\n#### Structural Similarity Index Measure (SSIM)\n\nThe Structural Similarity Index Measure assesses the visual impact of three characteristics: luminance, contrast, and structure. The SSIM index is defined as:\n\n$$\n\\text{SSIM}(I, \\hat{I}) = \\frac{(2\\mu_I \\mu_{\\hat{I}} + C_1)(2\\sigma_{I\\hat{I}} + C_2)}{(\\mu_I^2 + \\mu_{\\hat{I}}^2 + C_1)(\\sigma_I^2 + \\sigma_{\\hat{I}}^2 + C_2)}\n$$\n\nwhere $\\mu_I$ and $\\mu_{\\hat{I}}$ are the average pixel values of the original and reconstructed images, $\\sigma_I^2$ and $\\sigma_{\\hat{I}}^2$ are the variances, and $\\sigma_{I\\hat{I}}$ is the covariance. The constants $C_1$ and $C_2$ are small values added for stability. SSIM values range from -1 to 1, with values closer to 1 indicating better similarity.\n\nThese metrics can effectively evaluate the quality of images reconstructed through SVD compression, providing insights into how well the compression process preserves the original image details.\n\n#### Comparison of image compression methods\n\nIn this section, we compare the performance of different image compression methods, specifically focusing on Singular Value Decomposition (SVD), Discrete Cosine Transform (DCT), Wavelet Transform (Haar), Fractal Compression, Run-Length Encoding (RLE), and Predictive Coding. Each method has its unique characteristics and is suitable for different types of image data. The evaluation metrics used for comparison include Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM).\nA brief description of compression methods is given below.\n\n  - *Singular Value Decomposition:* A linear algebra technique that decomposes a matrix into singular values and orthogonal matrices, providing efficient low-rank approximations suitable for image compression.\n    \n  - *DCT (Discrete Cosine Transform:* Widely used in JPEG compression, DCT transforms image data into a frequency domain, allowing for the quantization and truncation of less significant frequencies to reduce file size while maintaining visual quality.\n    \n  - *Wavelet (Haar Transform):* Utilizes wavelet functions to represent data at different scales and resolutions, allowing for both spatial and frequency localization, making it effective for compressing images with varying detail levels.\n    \n  - *Fractal Compression:* This method relies on self-similarity in images and encodes them by identifying and representing repetitive patterns, which can lead to high compression ratios, especially for natural images.\n    \n  - *RLE (Run-Length Encoding):* A lossless compression technique that replaces sequences of the same data value with a single value and a count, making it effective for images with large uniform areas.\n    \n  - *Predictive Coding:*  This approach predicts pixel values based on neighboring pixels and encodes the difference between the predicted and actual values, effectively reducing redundancy in the image data.\n\nThe performance metrics for each compression method are summarized in @tbl-image_compression_comparison.\n\n:::{#tbl-image_compression_comparison}\n| Method     | MSE     | PSNR (dB) | SSIM   |\n|------------|---------|-----------|--------|\n| SVD        | 36.1802 | 32.5461   | 0.8247 |\n| DCT        | 107.6621| 27.8102   | 0.8217 |\n| Wavelet    | 32.9375 | 32.9539   | 0.9582 |\n| Fractal    | 20.4741 | 35.0188   | 0.9320 |\n| RLE        | 0.0000  | inf       | 1.0000 |\n| Predictive | 107.2521| 27.8267   | 0.5477 |\n\n: Comparison of Image Compression Methods\n:::: \n\nThe comparison of various image compression methods, as presented in @tbl-image_compression_comparison, highlights the promising performance of Singular Value Decomposition (SVD) image compression. The SVD method achieved a Mean Squared Error (MSE) of 36.1802, a Peak Signal-to-Noise Ratio (PSNR) of 32.5461 dB, and a Structural Similarity Index Measure (SSIM) of 0.8247. In terms of image quality, a PSNR value above 30 dB is generally considered acceptable for high-quality image reconstruction, and the SVD method meets this criterion. Similarly, the SSIM score of 0.8247 indicates a relatively high level of structural similarity, as values closer to 1.0 are preferred for maintaining perceptual quality. These results suggest that SVD is a promising approach for image compression, effectively balancing compression efficiency with visual quality, particularly suitable for applications that require efficient storage and satisfactory image fidelity.\n\n### Orthogonal subspaces in SVD\n\nThe Singular Value Decomposition (SVD) of the original data matrix $X$ enables its decomposition into two orthogonal subspaces: the *dominant subspace*, represented by the components $US_kV^T$, which corresponds to the signal information, and the *subdominant subspace*, represented by $US_{n-k}V^T$, which captures the noise components. This dual representation provides a clear delineation of the image data into signal and noise, significantly enhancing our ability to analyze and process the data effectively. This formalism can be represented as in Figure @fig-dom-subdom.\n\n:::{#fig-dom-subdom}\n\n![](./Notebooks/figures/domsubdom.png)\n\nDominant-subdominant splitting of the image SVD.\n:::\n\nUsing the SVD, all the fundamental subspaces and their rank can be extracted. This residing relationship can be visualized as:\n\n\\begin{align*}\n  \\mathbf{X} &=\n  \\mathbf{U} \\, \\Sigma \\, \\mathbf{V}^{T} \\\\\n%\n &=\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right] \n  \\left[ \\begin{array}{cccc|cc}\n     \\sigma_{1} & 0 & \\dots &  &   & \\dots &  0 \\\\\n     0 & \\sigma_{2}  \\\\\n     \\vdots && \\ddots \\\\\n       & & & \\sigma_{\\rho} \\\\\\hline\n       & & & & 0 & \\\\\n     \\vdots &&&&&\\ddots \\\\\n     0 & & &   &   &  & 0 \\\\\n  \\end{array} \\right]\n  \\left[ \\begin{array}{c}\n     \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{T} \\\\ \n     \\color{red}{\\mathbf{V}_{\\mathcal{N}}}^{T}\n  \\end{array} \\right]  \\\\\n  & =\n   \\left[ \\begin{array}{cccccccc}\n    \\color{blue}{u_{1}} & \\dots & \\color{blue}{u_{\\rho}} & \\color{red}{u_{\\rho+1}} & \\dots & \\color{red}{u_{m}}\n  \\end{array} \\right]\n  \\left[ \\begin{array}{cc}\n     \\mathbf{S}_{\\rho\\times \\rho} & \\mathbf{0} \\\\\n     \\mathbf{0} & \\mathbf{0} \n  \\end{array} \\right]\n   \\left[ \\begin{array}{c}\n    \\color{blue}{v_{1}^{T}} \\\\ \n    \\vdots \\\\\n    \\color{blue}{v_{\\rho}^{T}} \\\\\n    \\color{red}{v_{\\rho+1}^{T}} \\\\\n    \\vdots \\\\ \n    \\color{red}{v_{n}^{T}}\n  \\end{array} \\right]\n\\end{align*}\n\nThe column vectors form spans for the subspaces are given by\n\n\\begin{align*} \n% R A\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{X} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{u_{1}}, \\dots , \\color{blue}{u_{\\rho}}\n\\right\\} \\\\\n% R A*\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{X}^{T} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{v_{1}}, \\dots , \\color{blue}{v_{\\rho}}\n\\right\\} \\\\\n% N A*\n\\color{red}{\\mathcal{N} \\left( \\mathbf{X}^{T} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{u_{\\rho+1}}, \\dots , \\color{red}{u_{m}}\n\\right\\} \\\\\n% N A\n\\color{red}{\\mathcal{N} \\left( \\mathbf{X} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{v_{\\rho+1}}, \\dots , \\color{red}{v_{n}}\n\\right\\} \\\\\n%\n\\end{align*}\n\nThe conclusion is that the full SVD provides an orthonormal span for not only the two null spaces, but also both range spaces. All these theories can easily be extended to image processing.\nThe right singular vectors associated with the vanishing singular values of $X$ define the null space of the matrix, while the left singular vectors corresponding to the non-zero singular values span the range of $X$. Consequently, the rank of $X$ equals the count of non-zero singular values, which is directly related to the number of non-zero diagonal elements in the singular value matrix $S$. This orthogonal partitioning of the $M$-dimensional vector space mapped by $X$ is essential in applications such as image processing, where distinguishing between the signal and noise components can significantly enhance techniques such as watermarking.\n\n@fig-svd-comparison illustrates the image data's dominant subspace, truncated to $k = 50$ SVD components, alongside its subdominant noise subspace. \n\n:::{#fig-svd-comparison}\n\n![](./Notebooks/figures/svd_comparison.png)\n\nComparison of Original Image, Reconstructed Image after SVD Compression (with $k=40$), and Extracted Noise. These subplots illustrate the effectiveness of SVD in reconstructing the original image while isolating noise components.\n\n::: \n\n\nThis property of SVD effectively facilitates the identification of the rank of $X$, the orthonormal basis for its range and null spaces, and enables optimal low-rank approximations in various norms, thus paving the way for significant advancements in image processing applications, including watermarking, where the relationship between the SVD domain and noisy or watermarked images can be leveraged effectively.\n\n\nTo investigate the influence of the truncation factor $k$ on image quality, experiments were conducted to evaluate the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) values of reconstructed images for various \n$k$ values. The results, summarized in @tbl-truncation_vs_quality, indicate a clear trend: as the truncation factor increases, both PSNR and SSIM improve significantly. This improvement suggests that retaining more singular values enhances the quality of the reconstructed images, thereby preserving essential details and structures. Notably, the PSNR values reach a peak of 39.15 dB and the SSIM values approach 0.93 when  $k$ is set to 1000, indicating high fidelity to the original image.\n\n\n:::{#tbl-truncation_vs_quality}\n| $k$      | PSNR (dB)  | SSIM      |\n|----------|------------|-----------|\n| 1        | 14.445199  | 0.765134  |\n| 5        | 20.347972  | 0.779434  |\n| 10       | 22.906752  | 0.784555  |\n| 20       | 25.443104  | 0.789932  |\n| 50       | 28.667464  | 0.799783  |\n| 100      | 31.237551  | 0.814706  |\n| 200      | 33.401548  | 0.837690  |\n| 400      | 35.248494  | 0.870490  |\n| 600      | 36.602859  | 0.895303  |\n| 800      | 37.881342  | 0.915886  |\n| 1000     | 39.145403  | 0.933217  |\n\n:Relationship between Truncation Factor \\$k \\$ and Image Quality Metrics.\n:::: \n\n\n@fig-psnr_ssim_variation_plot illustrates the relationship between the truncation factor $k$ and the image quality metrics PSNR and SSIM. The horizontal axis represents the truncation factor $k$, while the two curves depict the corresponding PSNR and SSIM values for various $k$ settings.\n\n:::{#fig-psnr_ssim_variation_plot}\n![](./Notebooks/figures/psnr_ssim_variation_plot.png)\n\nVariation of PSNR and SSIM with respect to the truncation factor $k $.\n:::\n\nBy analyzing the plot, one can easily determine the appropriate truncation parameter $k$ needed to achieve a desired PSNR or SSIM value, thereby ensuring optimal fidelity and perceptual quality in the reconstructed image. This graphical representation serves as a practical tool for selecting the truncation factor, facilitating a balance between compression efficiency and image quality. For instance, if a target PSNR value of 35 dB is desired, one can project this value onto the PSNR curve and trace down to the horizontal axis to identify the corresponding $k$ value, which allows for informed decision-making in image compression settings.\n\n## New Role- SVD as a Denoiser \n\nSingular Value Decomposition (SVD) is a powerful mathematical technique that has various applications in image processing, including noise filtering and digital watermarking. \n\nIn the context of noise filtering, SVD can efficiently separate the noise components from the original image signal. The SVD approximates the image matrix by decomposing it into an optimal estimate of the signal and the noise components. This property makes SVD a useful tool for removing noise from images while preserving the quality and recognition of the original content.\n\nIn this study, we assessed the correlation between consecutive reconstructed images as a function of the truncation parameter $k$ in Singular Value Decomposition (SVD).\n\n:::{#fig-corr-slice}\n\n![](./Notebooks/figures/corr-slice.png)\n\nCorrelation between original and reconstructed images from image SVD.\n:::\n\nAs shown in  @fig-corr-slice, the sharp increase in correlation between consecutive reconstructed images as $k$ rises to 200 illustrates SVD’s strong ability to retain key image details even with relatively low truncation levels. This trend suggests that the primary singular values capture essential structural information of the original image, leading to high-fidelity reconstructions while efficiently filtering out less critical components. Given this preservation capacity, we proceed to assess SVD’s denoising capability by calculating the PSNR and SSIM values for both the noisy and denoised images.\n\n@fig-svd_denoising_results illustrates experimental results of the SVD-based denoising process on a high resolution image (20.0 MB, $4480\\times 6133$, at 24 bit depth).\n\n:::{#fig-svd_denoising_results}\n![](./Notebooks/figures/svd_denoising_results.png)\n\nComparison of Original, Noisy, and Denoised Images using SVD.\n\n:::\n\nBy considering the first 50 eigenimages as the image data subspace and the remainder as the noise subspace, and then removing the noise subspace, @fig-svd_denoising_results (c) shows the image after noise removal. \n\nNoise has a disproportionate impact on singular values (SVs) and singular vectors (SCs), with smaller SVs and their corresponding SCs being more severely affected compared to larger SVs and SCs. Experiments validate this phenomenon, as shown in @fig-svd_matrices_comparison, which depicts a 2-dimensional representation of the left and right SCs. This highlights the contrast between the slower changing waveforms of the former SCs and the faster changing waveforms of the latter SCs. \n\n:::{#fig-svd_matrices_comparison}\n![](./Notebooks/figures/svd_matrices_comparison.png)\n\nComparison of the image with the reconstructed traces in the left singular matrix ($U$) and the right singular matrix (V$^T$) of noisy image.\n:::\n\nWhile SVD-based denoising methods have demonstrated promising results, consistency in performance across different images is often challenging, particularly within datasets like BSD400. In such cases, fixing a truncation parameter $k$ does not always yield optimal denoising performance. This limitation arises because the variance of image information captured in the singular values varies across different images. Consequently, an adaptive approach is preferable over a fixed truncation level for retaining significant image details while effectively suppressing noise.\n\nTo address this, we propose dynamically thresholding the singular values rather than fixing $k$ for truncation. By removing singular values below a specific threshold, we focus on preserving image components that substantially contribute to the signal, thereby enhancing denoising effectiveness. Experimentally, we observe that setting the truncation threshold for singular values to $0.618 \\times \\text{mean}(\\Sigma)$, where $\\Sigma$ denotes the diagonal matrix of singular values, achieves optimal denoising. This threshold corresponds to approximately 61.8\\% of the mean singular value magnitude, which is notably effective in retaining essential image features while filtering out high-frequency noise components.\n\nOur empirical results further validate this approach, revealing that the dynamic thresholding method consistently produces higher Peak Signal-to-Noise Ratio (PSNR) values across a variety of images in the BSD400 dataset when compared to fixed-$k$ truncation. This improvement underscores the robustness of the adaptive threshold in aligning the denoising process with each image's inherent structural properties, thereby achieving superior fidelity to the original image.\n\nThe effectiveness of the adaptive thresholding approach in Singular Value Decomposition (SVD) for image denoising is exemplified in @fig-svd_denoising_resultsBSD. This figure displays an image from the BSD400 dataset, showcasing the original, noisy input and denoised output along with the PSNR and SSIM measures.\n\n:::{#fig-svd_denoising_resultsBSD}\n![](./Notebooks/figures/svd_denoising_resultsBSD.png)\n\nComparison of Original, Noisy, and Denoised images using SVD on BSD400 sample image.\n:::\n\n\n### Comparison and Advantages of SVD-Based Denoising in Medical Imaging\n\nIn medical imaging, one major hurdle is the lack of a clean reference image, which complicates the task of denoising. Creating datasets with perfect reference images is often impossible. This challenge is made even harder by the noise that arises from natural physiological movements, which can introduce dynamic noise into MRI, CT, and ultrasound scans, even if the patient is mostly still.\n\nMany traditional denoising methods depend on machine learning algorithms that are optimized with the help of reference images or alternative *doubly noisy* images that serve as substitutes for the ideal ground truth. For example, recent research, including a study by Floquet et al. (2024), has shown that using noisy reference images can effectively help adjust the parameters for denoising techniques. \n\nIn these scenarios, optimization methods such as the Scipy optimizer and stochastic gradient optimization are applied to refine the algorithms, aiming to reduce the Mean Square Error (MSE). This fine-tuning process has resulted in impressive outcomes, achieving a Peak Signal-to-Noise Ratio (PSNR) of 33.8, indicating a significant improvement in image quality (reference: <https://sijuswamy.github.io/Denoising-Manuscript/>).\n\nOn the other hand, an SVD-based approach has the potential to avoid the requirement for having any ground truth reference which could be more concept around it altogether. Using the SVD it is then possible to filter noise depending on the singular values relating to structural image information. The denoising based on SVD yielded a PSNR of 32.27 on a similarly noisy image in a comparative experiment—a value with 5\\% from PSNRs achieved by parameter-optimized methods of denoising without a need of a reference image. However, its independence from ground truth is an advantage for medical applications where unsupervised methods may reduce cost and complexity of operation. \n\nThese results could be further improved with a hybrid method that combines a first stage of initial denoising and even using a noisy image as a prior together with a method with optimized parameters then using SVD to help capture dominant features in images. Even without a reference image, SVD is able to act as an adaptive, standalone denoising solution and opens sustainable possibilities in the medical innovations context where the reference is commonly unknown and the denoising process is crucial for the meaningful diagnosis.\n\n## Image Forensics with SVD\n\nIn the contemporary digital era, digital forensics has become crucial for combating counterfeiting and manipulation of digital evidence aimed at illicit profit or legal evasion. Forensic research encompasses various domains, including steganography, watermarking, authentication, and labeling. Numerous solutions have been developed to fulfill consumer demands, such as authentication systems, DVD copy control, and hardware/software watermarking.\n\nSingular Value Decomposition (SVD) serves as a potent method in this realm, concentrating significant signal energy into a minimal number of coefficients while adapting to local statistical variations in images. As an image-adaptive transform, SVD requires careful representation to ensure accurate data retrieval.\n\n### Image watermarking with scaled additive approach\n\nSVD-based watermarking techniques exploit the stability of singular values (SVs), which represent the image’s luminance. Minor alterations in these values do not drastically compromise the visual quality of the host image. Methods typically utilize either the largest or smallest SVs for watermark embedding, employing additive techniques or quantization. For instance, D. Chandra's methodology involves the additive incorporation of scaled watermark singular values into the singular values of the host image $X$ @chandra2002digital:\n\n$$\nSV_{\\text{modified}} = SV_{\\text{original}} + \\alpha \\cdot \\text{Watermark}\n$$\n\nHere, $\\alpha$ denotes a scaling factor, allowing for effective watermark integration while maintaining the fidelity of the original image.\nThe scaled additive algorithm for image watermarking is given in the following algorithm.\n\n:::: {.algorithm #algo-SA}\n**Algorithm**: Scaled Additive Approach for Image Watermarking\n\n**Inputs**: \n- Cover image $A$\n- Watermark $W$\n- Scaling factor $\\alpha$\n\n**Outputs**: \n- Watermarked image $A_w$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Watermark Embedding**:\n    - Compute SVD of the cover image $A$: $[U_1, S_1, V_1] \\gets \\text{svd}(A)$\n    - Modify the singular values by adding the scaled watermark: $\\text{temp} \\gets S_1 + (\\alpha \\cdot W)$\n    - Compute SVD of the modified singular matrix: $[U_w, S_w, V_w] \\gets \\text{svd}(\\text{temp})$\n    - Reconstruct the watermarked image: $A_w \\gets U_1 \\cdot S_w \\cdot V_1^T$\n\n2. **Watermark Extraction**:\n    - Compute SVD of the watermarked image $A_w$: $[U_{w1}, S_{w1}, V_{w1}] \\gets \\text{svd}(A_w)$\n    - Reconstruct the matrix $D$ using the new singular values: $D \\gets U_w \\cdot S_{w1} \\cdot V_w^T$\n    - Extract the watermark: $W_e \\gets \\frac{D - S_1}{\\alpha}$\n\n3. **Verification**:\n    - If $W == W_e$:\n        - The image is **not attacked**.\n    - Else:\n        - The image has been **attacked**.\n:::: \n\n\n@fig-inageforensic represent the typical workflow of image forensic.\n\n:::{#fig-inageforensic}\n![](./Notebooks/figures/inageforensic.png)\n\nImage forensic workflow.\n:::\n\n@fig-image-forensicSVD demonstrate the watermarking of images using SVD. Since the extracted watermark is exactly what we embedded in the covering image, no attack is detected @Sharma2024.\n\n:::{#fig-image-forensicSVD}\n\n![](./Notebooks/figures/image-forensicSVD.png)\n\n\nDemonstration of watermarking a confidential image using SVD.\n::: \n\nIn this first evaluation the PSNR value of watermarked image is 29.08 and the extraction is successful.\n\nTo evaluate the robustness and effectiveness of the Singular Value Decomposition (SVD)-based watermarking approach on a broader spectrum of images, using the BSD400 dataset offers a comprehensive test bed. The BSD400 dataset contains a wide variety of images with intricate textures, fine details, and different visual complexities, making it ideal for testing how well the SVD-based watermarking technique can embed and extract watermarks under varied conditions.\n\nBy selecting images with delicate content, such as running letters and intricate textures, the goal is to assess how well the watermark remains visually unobtrusive in complex scenes while being resilient to potential attacks (such as noise, compression, or cropping). This method will also allow for calculating objective quality metrics like PSNR  across different image categories, providing a robust understanding of watermark quality and imperceptibility.\n\n### Image watermarking with adaptive scaled additive approach\n\nUsing the *test\\_077.png* image from the BSD400 dataset, we employ an adaptive approach to watermarking that integrates D. Chandra’s scaled addition technique with a balanced formula @chandra2002digital:\n\n\\begin{equation}\n    \\text{SV}_{\\text{mod}} = (1 - \\alpha) \\cdot \\text{SV}_{\\text{img}} + \\alpha \\cdot \\text{Watermark}\n\\end{equation}\n\nAlgorithm for the adaptive scaled additive (ASA) approach is shown below.\n\n:::: {.algorithm #algo-ADAPTIVE}\n**Algorithm**: Scaled Additive Adaptive Approach for Image Watermarking\n\n**Inputs**: \n- Cover image $A$\n- Watermark $W$\n- Scaling factor $\\alpha$\n\n**Outputs**: \n- Watermarked image $A_w$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Watermark Embedding**:\n    - Compute SVD of the cover image $A$: $[U_1, S_1, V_1] \\gets \\text{svd}(A)$\n    - Modify the singular values by adding the scaled watermark adaptively: $\\text{temp} \\gets (1 - \\alpha) \\cdot S_1 + (\\alpha \\cdot W)$\n    - Compute SVD of the modified singular matrix: $[U_w, S_w, V_w] \\gets \\text{svd}(\\text{temp})$\n    - Reconstruct the watermarked image: $A_w \\gets U_1 \\cdot S_w \\cdot V_1^T$\n\n2. **Watermark Extraction**:\n    - Compute SVD of the watermarked image $A_w$: $[U_{w1}, S_{w1}, V_{w1}] \\gets \\text{svd}(A_w)$\n    - Reconstruct the matrix $D$ using the new singular values: $D \\gets U_w \\cdot S_{w1} \\cdot V_w^T$\n    - Extract the watermark: $W_e \\gets \\frac{D - S_1}{\\alpha}$\n\n3. **Verification**:\n    - If $W == W_e$:\n        - The image is **not attacked**.\n    - Else:\n        - The image has been **attacked**.\n:::: \n\nThis new approach modifies the singular values by proportionally blending the image's original details with the watermark content based on the parameter $\\alpha$. The adaptive blend allows for fine-tuning the watermark’s influence, thus optimizing both its visibility and robustness.\n\nThis adaptive watermarking formula . The formula  provides *high readability* and *forensic resilience* and enables the watermark to stay subtle within the image, while enhancing durability against forensic attacks. This allows for improved image readability and detail retention, particularly in face images like *test\\_077.png*.\n\nAs a next step, experiment with various values of $\\alpha$ to fine-tune the watermark’s visibility and robustness. Additionally, evaluate the  PSNR (Peak Signal-to-Noise Ratio) values to assess the balance achieved by the adaptive method.\n\n### Perceptual Forensic Approach for Image Watermarking\n\nThe perceptual forensic approach for image watermarking, introduced by Sadek, represents a significant advance in singular value decomposition (SVD)-based watermarking techniques by targeting robustness and imperceptibility in forensic applications. This approach, termed Global SVD (GSVD), employs a private (non-blind) methodology, making it suitable for sensitive forensic tasks where watermark retrieval without the original image is critical. In this technique, the watermark data is optimally embedded within the host image’s less significant subspace, often referred to as the *noise subspace*. This embedding choice leverages the low-impact regions of the image’s singular value structure, thus maintaining the original image quality while preserving the watermark’s resilience.\n\nA key innovation in Sadek's method is the scaled addition of the watermark data subspace into the host image's singular values. Traditional SVD-based watermarking techniques typically rely on a direct scaled addition of watermark values to the singular values of the cover image. However, this conventional approach often neglects the varying magnitude across the singular value spectrum, leading to uneven watermark integration that may affect image quality. Sadek's approach addresses this limitation by *flattening* the range of singular values before watermark embedding, which smooths out the differences in value magnitude and allows for a more perceptually consistent embedding. This adjustment not only enhances the watermark’s imperceptibility but also strengthens its resilience against potential distortions or attacks, which are common in forensic scenarios.\n\nThe GSVD-based perceptual forensic approach is inherently adaptable, allowing the embedded watermark to withstand different types of image manipulations depending on the robustness requirements. By embedding the watermark within the less visually significant regions of the singular value matrix, the GSVD technique achieves a balance between maintaining high perceptual quality and ensuring the watermark’s durability.\n\nThe algorithm for the perceptual forensic method for watermarking is given below.\n\n:::: {.algorithm #algo-PFA}\n**Algorithm**: Perceptual Forensic Watermarking using SVD\n\n**Inputs**:\n- Cover image $X$\n- Watermark $W$\n- Scaling factor $\\alpha$\n- Threshold parameter $k$\n\n**Outputs**:\n- Watermarked image $Y$\n- Extracted watermark $W_e$\n\n**Steps**:\n\n1. **Input**: Read cover image $X$ and watermark $W$.\n\n2. **Compute SVD**: Perform the Singular Value Decomposition (SVD) on both $X$ and $W$:\n    - $X = U_h S_h V_h^T$\n    - $W = U_w S_w V_w^T$\n\n3. **Define Scaled Addition for Modified Singular Values**:\n    - For $i = M - k$ to $M$, with $q = 1$ to $k$:\n        - $S_m(i) = S_h(i) + \\alpha \\cdot \\ln(S_w(q))$\n    - For all other $i$, set $S_m(i) = S_h(i)$.\n\n4. **Form the Watermarked Image** $Y$:\n    - $Y = U_h S_m V_h^T$\n\n5. **Reconstruct Singular Values for Watermark Extraction**:\n    - For $i = M - k$ to $M$:\n        - $S'_w(i) = \\exp\\left(\\frac{S_m(i) - S_h(i)}{\\alpha}\\right)$\n\n6. **Extract the Watermark**:\n    - $W_e = U_w S'_w V_w^T$\n\n7. **Reconstruction Check**: Verify watermark accuracy by comparing $W_e$ with $W$.\n\n:::: \n\nThis method is particularly useful in forensic watermarking applications that demand both high fidelity and robustness, such as in medical imaging and high-resolution photographic forensics, where maintaining image integrity is paramount. This technique’s ability to fine-tune watermark robustness based on singular value dynamics, while preserving the host image quality, marks it as a promising advancement in forensic watermarking applications.\n\n@fig-perceptualplusscaledaddition illustrate the results of the adaptive watermarking technique using D. Chandra’s approach and the perceptual forensic approach, applied to a sample image in the BSD400 image dataset at $\\alpha=0.01$. The first row shows the original and watermark-modified images, while the second row demonstrates the images after the application of direct perceptual forensic watermarking and the images with a Gaussian noise for forensic testing @sadek2012svd.\n\n:::{#fig-perceptualplusscaledaddition}\n![](./Notebooks/figures/perceptualplusscaledaddition.png)\n\nResults of Watermarking with scaled addition and perceptual forensic approaches using SVD.\n:::\n\nFrom @fig-perceptualplusscaledaddition (d), the perceptive forensic approach is a winner in maintaining the image details in watermarking and this fact is substantiated with @tbl-PSNRall . Also it is noted that noising after watermarking the image through SVD produces almost same PSNR across the experiments. A detailed comparison of  image detailing after watermarking on uncompressed and compressed version of the BSD400 image *test\\_077.png* is shown in Table @tbl-PSNRcomparison.\n\n:::{#tbl-PSNRall}\n| Image type                       | $\\alpha=0.01$ (SA) | $\\alpha=0.01$ (ASA) | $\\alpha=0.1$ (SA) | $\\alpha=0.1$ (ASA) | $\\alpha=0.2$ (SA) | $\\alpha=0.2$ (ASA) | $\\alpha=0.3$ (SA) | $\\alpha=0.3$ (ASA) |\n|----------------------------------|--------------------|---------------------|-------------------|--------------------|-------------------|--------------------|-------------------|--------------------|\n| Watermarked                      | 61.84             | 46.41              | 38.83            | 26.56             | 30.82            | 20.68             | 25.16            | 17.35             |\n| Noised after watermarked         | 20.70             | 20.66              | 20.66            | 19.74             | 20.48            | 17.86             | 19.91            | 16.06             |\n| Watermarked & Compressed         | 49.32             | 44.56              | 38.60            | 26.54             | 31.07            | 20.70             | 26.03            | 17.49             |\n\nPeak Signal to Noise Ratio of various watermarked versions of *test\\_077* image from BSD400 dataset under scaled additive (SA) and adaptive scaled additive (ASA) approaches.\n:::\n\nFrom @tbl-PSNRcomparison, it is clear that both scaled additive and adaptive scaled additive approaches gives maximum image detaining in the watermarked state is at lower values of $\\alpha$. Maintaining readability and security is the key aspect in image forensic. So $\\alpha=0.01$ is a safe choice. At the same level of scaling the perceptual forensic approach is used in the BSD400 image. Comparison of PSNR values of scaled additive, adaptive scaled additive and the perceptual forensic approaches at $\\alpha=0.01$ is shown in @tbl-PSNRall.\n\n:::{#tbl-PSNRcomparison}\n| Image type                       | Scaled Additive                 | Adaptive Scaled Additive                 |  Perceptual Forensic |\n|----------------------------------|---------------------------------|------------------------------------------|-------------------------------------|\n| Watermarked                      | 61.84                           | 46.41                                    | 75.17                               |\n| Noised after watermarked         | 20.70                           | 20.66                                    | 20.68                               |\n| Watermarked & Compressed         | 49.32                           | 44.56                                    | 38.87                               |\n\n\nPeak Signal to Noise Ratio of various watermarked versions of *test\\_077* image from BSD400 dataset under scaled additive (SA), adaptive scaled additive (ASA) and perceptual forensic approaches.\n\n:::\n\nWatermarking through Singular Value Decomposition (SVD) is emerging as a promising method in the field of medical imaging to protect data integrity and authenticity. In our study, an available CT image was used to embed a watermark using both a scaled addition method and an adaptive perceptual forensic approach.\n\nWhen unaltered, the watermark was effectively extracted, showing that SVD-based watermarking can preserve image integrity under normal conditions. However, when noise was introduced after embedding, the extracted watermark showed substantial degradation, highlighting the technique’s sensitivity to potential tampering.\n\n:::{#fig-BrainCTPerceptualWM}\n\n![](./Notebooks/figures/BrainCTPerceptualWM.png)\n\nComparison of Brain CT images: (a) Original Brain CT Image, (b) Watermarked with scaled additive approach, (c) Watermarked with perceptual forensic approach.\n\n:::\n\nThe effect of watermarking on the Brain CT image using different image forensic approaches is shown in @fig-BrainCTPerceptualWM. The scaled addition method achieved a Peak Signal-to-Noise Ratio (PSNR) of 33.93, balancing visibility and quality. Meanwhile, the perceptual forensic approach, designed to better manage watermark strength relative to image details, attained a PSNR of 102.87, maintaining high image fidelity. These results indicate that SVD-based watermarking techniques can be effective for medical imaging applications, where preserving diagnostic quality while protecting image authenticity is critical. This adaptive method offers a balanced approach to ensure data protection without compromising readability and detail in medical images.\n\n## Conclusion\n\nThis study investigated SVD-based image processing applications, specifically focusing on image compression, image denoising, and image forensic analysis. Through experimental analysis on high-resolution images, the BSD400 dataset, and medical images, this work examined the effectiveness of two watermarking approaches: scaled additive embedding and perceptual forensic embedding. In the scaled additive approach, the watermark was scaled and embedded within the singular values of the image before full SVD decomposition. To improve the adaptability across images with varying detail levels, an adaptive scaling mechanism was introduced, achieving high-quality image blending with minor scaling factors $(\\alpha < 0.02)$. \n\nIn the perceptual forensic approach, watermark embedding targeted distinct ranges of singular values, optimizing the visibility and robustness of the watermark under forensic scrutiny. This method employed a locally adaptive SVD, enhancing watermark resilience while preserving essential image details, making it effective for applications requiring forensic analysis. Additionally, image denoising was implemented as an automatic fine-tuning step to reduce noise introduced during watermarking, further solidifying the watermark's readability and stability.\n\nThis work is a partial replication and extension of Sadek’s review on SVD-based image processing applications, which highlights the state-of-the-art methods and challenges in SVD applications for image processing @sadek2012svd. By incorporating aspects of automated fine-tuning for denoising algorithms in the watermarking process, this study contributes a refined understanding of how SVD can be leveraged to balance image quality and watermark resilience. Overall, the findings affirm that SVD-based techniques fulfill the study’s objectives across compression, denoising, and forensic applications, providing a flexible and robust approach to image processing that is effective across various image types and contexts. Future work may explore additional fine-tuning and new methodologies to enhance forensic robustness and adaptive capabilities in real-world applications.\n\n## References\n\n::: {#refs}\n:::"},"formats":{"ieee-html":{"identifier":{"display-name":"HTML","target-format":"ieee-html","base-format":"html","extension-name":"ieee"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"shortcodes":["D:\\SVD_project\\_extensions\\dfolio\\ieee\\_extensions\\quarto-ext\\fancy-text\\fancy-text.lua"]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"filters":["D:\\SVD_project\\_extensions\\dfolio\\ieee\\_extensions\\quarto-ext\\latex-environment\\latex-environment.lua","D:\\SVD_project\\_extensions\\dfolio\\ieee\\ieee.lua"],"cite-method":"citeproc","toc":true,"template":"_extensions/dfolio/ieee/partials/ieee-template.html","include-in-header":["_extensions/dfolio/ieee/partials/mathjax.html"],"html-math-method":{"method":"mathjax"},"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","environments":["IEEEbiography","IEEEbiographynophoto"],"commands":["IEEEPARstart","appendix"],"crossref":{"chapters":false,"eq-prefix":null,"eq-labels":"(roman)","fig-title":"Fig.","fig-prefix":"Fig."},"csl":"_extensions/dfolio/ieee/ieee-with-url.csl","link-citations":true,"toc-location":"left","toc-title":"Document Sections","theme":["_extensions/dfolio/ieee/styles.scss"],"template-partials":["_extensions/dfolio/ieee/partials/title-block.html","_extensions/dfolio/ieee/partials/title-metadata.html","_extensions/dfolio/ieee/partials/author.html","_extensions/dfolio/ieee/partials/after-body.html","_extensions/dfolio/ieee/partials/affiliation.tex"],"date-format":"D MMMM YYYY","google-scholar":true,"refs":"::: {#refs}\n:::\n","revealjs-plugins":[],"formats":{"ieee-pdf":"default","ieee-html":"default"},"title":"SVD Based Image Processing Applications","author":[{"id":"dfolio","name":"Siju K S","affiliations":[{"name":"Amrita Vishwa Vidyapeetham","department":"School of Artificial Intelligence","city":"Coimbatore","country":"India","postal-code":"641 112"},{"name":"CEN"}],"orcid":"0009-0004-1983-5574","email":"siju.swamy@saintgits.org","url":"https://github.com/sijuswamy/SVD_project","membership":"Member, ISTE","attributes":{"corresponding":true},"photo":"Swamy.jpg","bio":"Pursuing research in Artificial Intelligence under the Faculty of Amrita School of Artificial Intelligence at the Center of Excellence in Computational Engineering & Networking, Amrita Vishwa Vidyapeetham, Coimbatore.\n"},{"name":"Dr.Soman K.P","affiliations":[{"name":"Professor & Dean, Amrita School of Artificial Intelligence"}],"photo":"soman_sir.jpg","bio":"Dr. Soman K. P. currently serves as the Dean of the School of Artificial Intelligence (AI), Head and Professor at Amrita Centre for Computational Engineering and Networking (CEN), Coimbatore Campus. He has more than 27 years of research and teaching experience in Artificial Intelligence (AI) and Data Science-related subjects at Amrita Vishwa Vidyapeetham, Coimbatore. He has authored over 500+ publications in reputed journals such as IEEE Transactions, IEEE Access, Applied Energy, and several conference proceedings.\n","note":"Template created June 23, 2023; revised `r format(Sys.Date(),format='%B %d, %Y')`."}],"abstract":"This study investigates the application of Singular Value Decomposition (SVD) as an effective mathematical framework for various image processing tasks. SVD offers a unique decomposition approach, making it suitable for applications like image compression, denoising, and watermarking by enabling optimal rank approximations and noise separation. The robustness of SVD in handling large matrices allows it to capture key image characteristics, preserving essential features while reducing data requirements. By leveraging SVD’s ability to separate data into dominant and subdominant subspaces, this research demonstrates enhanced image compression, effective noise reduction, and secure watermark embedding. Experimental results validate SVD's utility in optimizing image storage, clarity, and fidelity, with potential implications for advancing adaptive image processing techniques.\n","keywords":["Singular Value Decomposition (SVD)","Image Processing","Image Compression","Image Denoising","Digital Watermarking","Noise Filtering","Matrix Factorization","Rank Approximation","Frobenius Norm","Energy Compaction","Digital Forensics","Signal Processing","Adaptive Image Processing","Orthogonal Subspaces"],"funding":{"statement":"No funding is recieved for completion of this project work"},"pageheader":{"left":"ASAI, October 2024","right":"Project Report"},"bibliography":["./references.bib"],"date":"2024-10-30","pdf":"https://github.com/sijuswamy/SVD_project/blob/main/A%20Study%20on%20SVD%20Based%20Image%20Processing%20Applications.pdf","citation":{"container-title":"GitHUB","page":"1-27","type":"article","issued":"2024-11-04","url":"https://github.com/sijuswamy/SVD_project","pdf-url":"https://github.com/sijuswamy/SVD_project/blob/main/A%20Study%20on%20SVD%20Based%20Image%20Processing%20Applications.pdf"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}