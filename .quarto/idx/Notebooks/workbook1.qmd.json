{"title":"SVD workbook","markdown":{"yaml":{"title":"SVD workbook","format":"html","jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\nImage processing has become integral to numerous fields, from medical imaging to digital forensics, where large volumes of visual data demand efficient storage, transmission, and quality retention techniques. Among the many mathematical transformations applied to images, Singular Value Decomposition (SVD) has emerged as a particularly valuable tool. SVD is a matrix factorization technique that represents a given matrix as a product of three matrices: $U$, $\\Sigma$, and $V^T$. This decomposition is significant in image processing because it maximizes the energy contained in the largest singular values, enabling the creation of compact, high-quality approximations of the original data. Unlike other transformations, SVD does not require a specific image size or type, making it highly adaptable and robust for various image processing tasks.\n\nThe primary strength of SVD lies in its capacity to separate image data into meaningful components. For instance, in an image represented by SVD, the larger singular values and their corresponding vectors encode most of the structural content, while smaller singular values can often represent noise. This property is beneficial for applications requiring data reduction, such as image compression and denoising, where maintaining the primary structure while reducing extraneous information is essential. Additionally, SVD’s stable mathematical foundation and adaptability have made it increasingly popular in other specialized applications, including watermarking for digital forensics and security.\n\nIn image compression, SVD enables reduced data storage by approximating the image using fewer singular values, providing a balance between quality and compression ratio. This application is critical in fields where storage and bandwidth are constrained. Similarly, in denoising, SVD can isolate noise by exploiting the decomposition’s ability to differentiate between dominant and subdominant subspaces, allowing effective noise suppression without significantly affecting the image’s core structure. Furthermore, SVD is also used in watermarking, where slight modifications to specific singular values embed unique patterns within images, enhancing security and ensuring authenticity.\n\nDespite these advantages, SVD in image processing remains an area with unexplored potential. This paper explores these established applications while addressing underutilized SVD properties to uncover new applications. By investigating SVD's adaptive properties in compressing and filtering images, as well as its potential for encoding data securely, this work contributes to a growing body of research on SVD-based image processing and presents promising directions for further study.\n\n# SVD Application in Image Processing\n\nSingular Value Decomposition (SVD) has several important applications in image processing. The SVD can be used to reduce the noise or compress matrix data by eliminating small singular values or higher ranks @Chen2018SingularVD. This allows for the size of stored images to be reduced @cao2006singular. Additionally, the SVD has properties that make it useful for various image processing tasks, such as enhancing image quality and filtering out noise. The main theorem of SVD is reviewed in the search results, and numerical experiments have been conducted to illustrate its applications in image processing.\n\n## Image Compression\n\nImage compression represents a vital technique to reduce the data needed to represent an image. This is crucial for achieving efficient storage and transmission across various applications, including digital photography, video streaming, and web graphics. Compression methods are primarily categorized into two distinct types: lossy and lossless.\n\nLossy compression diminishes file size by irreversibly eliminating certain image data, which can result in a degradation of image quality, as observed in JPEG formats. This method is frequently employed when the reduction of file size is of paramount importance, and any resultant loss in quality is considered acceptable.\n\nConversely, lossless compression techniques allow for the compression of images without any loss of data, facilitating the exact reconstruction of the original image, as exemplified by PNG formats. This approach is beneficial when preserving image quality is essential and minimizing file size is of lesser importance.\n\nThe decision to use either lossy or lossless compression hinges on the specific needs of the application, balancing the trade-offs between file size and image quality.\n\nSVD-based image compression functions by decomposing the image matrix into three components and subsequently approximating the original matrix with only the most significant singular values and vectors. This process results in a compact image representation while preserving the essential information.\n\nMathematically, given an image represented as a matrix $A$ with dimensions $m \\times n$, the Singular Value Decomposition (SVD) decomposes $A$ into three matrices: $U$, $\\Sigma$, and $V^T$. Here, $U$ is an $m \\times m$ orthogonal matrix containing the left singular vectors, $\\Sigma$ is an $m \\times n$ diagonal matrix containing singular values, and $V^T$ is the transpose of an $n \\times n$ orthogonal matrix containing the right singular vectors. To compress the image, we keep only the top $k$ singular values (where $k$ is significantly smaller than both $m$ and $n$). The compressed image can be reconstructed as \n\n$$\nA_k = U_k \\Sigma_k V_k^T,\n$$\n\nwhere $U_k$ contains the first $k$ columns of $U$, $\\Sigma_k$ is a $k \\times k$ diagonal matrix of the top $k$ singular values, and $V_k^T$ consists of the first $k$ rows of $V^T$.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Read and convert the image to grayscale\nimg = Image.open('amrita_campus.jpg')  # Specify your image file\ngray_img = img.convert('L')  # Convert to grayscale\nA = np.array(gray_img, dtype=np.float64)  # Convert to float64 for SVD computation\noriginal=A\n# Apply Singular Value Decomposition (SVD)\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Choose the number of singular values to keep for compression\nk = 50  # You can adjust this value to see different compression levels\n\n# Create a compressed version of the image using the first k singular values\nS_k = np.zeros_like(A)  # Initialize a zero matrix for S_k\nS_k[:k, :k] = np.diag(S[:k])  # Keep only the top k singular values\n\n# Reconstruct the compressed image\nA_k = np.dot(U[:, :k], np.dot(S_k[:k, :k], Vt[:k, :]))  # Reconstruct the image from the reduced SVD\n\n# Display the original and compressed images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(A, cmap='gray', vmin=0, vmax=255)  # Display original image\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(A_k, cmap='gray', vmin=0, vmax=255)  # Display compressed image\nplt.title(f'Compressed Image (k = {k})')\nplt.axis('off')\n\nplt.show()\n```\n\nTo assess the quality of the original and compressed images, various metrics can be employed. Commonly used measures are discussion in this section.\n\n### Image Quality Assessment Metrics\n\nTo evaluate the quality of compressed images relative to their original versions, several standardized metrics are commonly employed. These metrics provide quantitative comparisons across aspects such as pixel-level error, signal fidelity, structural similarity, and compression efficiency. The following are the key metrics used in image quality assessment:\n\n####  Mean Squared Error (MSE)\nThe Mean Squared Error quantifies the average squared difference between corresponding pixel values of the original and compressed images. Lower values indicate higher fidelity to the original. Mathematically, MSE is defined as:\n$$\n\\text{MSE} = \\frac{1}{m \\cdot n} \\sum_{i=1}^{m} \\sum_{j=1}^{n} (A(i,j) - A_k(i,j))^2\n$$\nwhere $A(i,j)$ and $A_k(i,j)$ denote the pixel values of the original and compressed images, respectively, and $m \\times n$ represents the image dimensions.\n\n#### Peak Signal-to-Noise Ratio (PSNR)\nPSNR is a widely used metric that compares the maximum possible signal value to the noise level introduced by compression. It is computed as:\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n$$\nwhere $\\text{MAX}$ represents the maximum pixel value (e.g., 255 for 8-bit images). Higher PSNR values indicate better image quality, as they correspond to lower MSE values.\n\n#### Structural Similarity Index (SSIM)\nThe Structural Similarity Index assesses perceptual similarity by analyzing luminance, contrast, and structural information between the original and compressed images. The SSIM index, ranging from -1 to 1, is calculated as:\n$$\n\\text{SSIM}(A, A_k) = \\frac{(2 \\mu_A \\mu_{A_k} + C_1)(2 \\sigma_{AA_k} + C_2)}{(\\mu_A^2 + \\mu_{A_k}^2 + C_1)(\\sigma_A^2 + \\sigma_{A_k}^2 + C_2)}\n$$\nwhere $\\mu$, $\\sigma$, and $\\sigma_{AA_k}$ denote means, variances, and covariances of $A$ and $A_k$, with constants $C_1$ and $C_2$ to prevent division by zero. Higher SSIM values suggest higher structural fidelity.\n\n#### Compression Ratio (CR)\nCompression Ratio quantifies the efficiency of compression, calculated as the ratio of the original image size to the compressed size:\n$$\n\\text{Compression Ratio} = \\frac{\\text{Size of Original Image}}{\\text{Size of Compressed Image}}\n$$\nA higher compression ratio indicates a greater reduction in file size, which is desirable in applications requiring efficient storage or transmission.\n\n#### Normalized Cross-Correlation (NCC)\nNormalized Cross-Correlation measures the similarity in pixel intensity patterns between the original and compressed images. NCC is calculated as:\n$$\n\\text{NCC} = \\frac{\\sum (A \\cdot A_k)}{\\sqrt{\\sum A^2 \\cdot \\sum A_k^2}}\n$$\nValues closer to 1 indicate a stronger correlation, signifying greater retention of the original image characteristics in the compressed version.\n\nThese metrics collectively provide a comprehensive assessment of image quality by addressing both objective and perceptual aspects of compression, making them suitable for a wide range of applications in image processing and computer vision.\n\n\n\n:::{#tbl-quality-metrics}\n| Metric                               | Value          |\n|:-------------------------------------|----------------|\n| Mean Squared Error (MSE)             | 110.2853       |\n| Peak Signal-to-Noise Ratio (PSNR)    | 27.7056 dB     |\n| Structural Similarity Index (SSIM)   | 0.8116         |\n| Compression Ratio (CR)               | 10.78          |\n| Normalized Cross-Correlation (NCC)   | 0.9976         |\n| Original Image Size                  | 9709.38 KB     |\n| Compressed Image Size                | 900.78 KB      |\n| Size Reduction                       | 8808.59 KB     |\n\n: Quality assessment metrics for original and compressed images, detailing standard measures of image compression and fidelity.\n:::\n\nThe quality assessment metrics indicate effective compression with minimal loss of fidelity in the image. A Mean Squared Error (MSE) of 110.29 suggests that the average pixel intensity differences between the original and compressed images are small. The Peak Signal-to-Noise Ratio (PSNR) of 27.71 dB, typically above the 30 dB threshold for high-quality compression, indicates moderate quality but acceptable for many applications.\n\nThe Structural Similarity Index (SSIM) of 0.8116, close to 1, suggests that the perceptual similarity between the images remains high. The Compression Ratio (CR) of 10.78 shows significant size reduction, and the Normalized Cross-Correlation (NCC) of 0.9976 demonstrates a high correlation between the original and compressed images, supporting strong structural consistency.\n\nThe compressed image achieves substantial size reduction (from 9709.38 KB to 900.78 KB) with reasonable preservation of visual quality, making it suitable for applications prioritizing storage efficiency without heavily compromising visual fidelity.\n\nThe table below presents a comparison of compression quality metrics for three different image compression methods: Singular Value Decomposition (SVD), Discrete Cosine Transform (DCT), and Wavelet Transform. The metrics included are Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Compression Ratio (CR), Normalized Cross-Correlation (NCC), Compressed Size, and Size Reduction. Each metric provides insight into the effectiveness of the compression techniques in terms of image quality and storage efficiency.\n\n:::{#tbl-quality-assessment}\n| Method   |      MSE      |     PSNR (dB)     |      SSIM      |    CR    |     NCC     | Compressed Size (KB) |\n|----------|---------------|-------------------|----------------|----------|-------------|----------------------|\n| SVD      | 282.9933     | 23.6130           | 0.7122         | 6.88     | 0.9938      | 176.33               |\n| DCT      | 1172.0801    | 17.4412           | 0.5914         | 2.28     | 0.9741      | 531.82               |\n| Wavelet  | 0.0197       | 65.1859           | 0.9999         | 0.12     | 1.0000      | 9715.31              |\n: Quality assessment metrics for original and compressed images in comparison with popular image compression algorithms.\n:::\n\nThe results demonstrate that Singular Value Decomposition (SVD) offers a superior balance between image quality and compression efficiency compared to Discrete Cosine Transform (DCT) and Wavelet Transform. With a significantly lower Mean Squared Error (MSE) and a Peak Signal-to-Noise Ratio (PSNR) of 23.6130 dB, SVD preserves the original image quality more effectively than DCT (17.4412 dB) and offers practical structural similarity (SSIM) of 0.7122. In contrast, while the Wavelet method achieves excellent PSNR (65.1859 dB) and SSIM (0.9999), its large compressed size (9715.31 KB) renders it impractical for many applications.\n\nIn terms of compression efficiency, SVD yields a Compression Ratio (CR) of 6.88 with a manageable compressed size of 176.33 KB, resulting in a significant size reduction of 1037.34 KB. This contrasts sharply with DCT’s lower CR of 2.28 and Wavelet’s CR of 0.12, which implies an increase in size for the latter. Overall, SVD stands out as a robust image compression method, effectively maintaining quality while achieving substantial reductions in storage requirements, making it particularly advantageous for applications prioritizing both quality and efficiency.\n\n## SVD Architecture and Denoising\n\nThe Singular Value Decomposition (SVD) architecture provides a powerful framework for analyzing and compressing images. In the context of image decomposition, the singular values (SVs) represent the luminance levels of various layers within the image, while the corresponding singular vectors (SCs) define the geometric characteristics of these layers. \n\nWhen applied to a high-resolution image, SVD enables the extraction of significant image content through the left singular matrix, capturing the primary structures and features. Conversely, the right singular matrix isolates the noise components, which are typically linked to the smaller singular values found in the diagonal matrix, $\\Sigma$. \n\nThus, the largest singular values correspond to the most prominent image features, often referred to as eigenimages, while the noise components are associated with the smaller singular values. This decomposition allows for a clear distinction between meaningful image information and noise, facilitating effective compression and analysis. By leveraging SVD, one can efficiently manage and manipulate image data, ensuring that essential visual content is retained while minimizing the impact of noise.\n\n:::{#fig-2 layout-ncol=2}\n\n![Original Image](original_image.pdf){#fig-2a}\n\n![Extracted Noise](extracted_noise.pdf){#fig-2b}\n\n![Reconstructed Signal](reconstructed_signal_k_40.pdf){#fig-2c}\n\nAn example with sub-figure illustrating the effectiveness of SVD in separating significant image content from noise.\n\n:::\n\n\n## A starting example\n\nAn example demonstrating the image compression using SVD is given below.\n\n:::{.panel-tabset}\n\n## Code\n```{.matlab}\n\n% Read and convert the image to grayscale\nimg = imread('amrita_campus.jpg'); % Specify your image file\ngray_img = rgb2gray(img); % Convert to grayscale\nA = double(gray_img); % Convert to double for SVD computation\n\n% Apply Singular Value Decomposition (SVD)\n[U, S, V] = svd(A)\n\n% Choose the number of singular values to keep for compression\nk = 50; % You can adjust this value to see different compression levels\n\n% Create a compressed version of the image using the first k singular values\nS_k = zeros(size(A)); % Initialize a zero matrix for S_k\nS_k(1:k, 1:k) = S(1:k, 1:k); % Keep only the top k singular values\n\n% Reconstruct the compressed image\nA_k = U*S_k*V'; % Reconstruct the image from the reduced SVD\n\n% Display the original and compressed images\nfigure;\nsubplot(1, 2, 1);\nimshow(uint8(A)); % Display original image\ntitle('Original Image');\n\nsubplot(1, 2, 2);\nimshow(uint8(A_k)); % Display compressed image\ntitle(['Compressed Image (k = ', num2str(k), ')']);\n```\n\n## Output\n\n```{.python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Read and convert the image to grayscale\nimg = Image.open('amrita_campus.jpg')  # Specify your image file\ngray_img = img.convert('L')  # Convert to grayscale\nA = np.array(gray_img, dtype=np.float64)  # Convert to float64 for SVD computation\noriginal=A\n# Apply Singular Value Decomposition (SVD)\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Choose the number of singular values to keep for compression\nk = 50  # You can adjust this value to see different compression levels\n\n# Create a compressed version of the image using the first k singular values\nS_k = np.zeros_like(A)  # Initialize a zero matrix for S_k\nS_k[:k, :k] = np.diag(S[:k])  # Keep only the top k singular values\n\n# Reconstruct the compressed image\nA_k = np.dot(U[:, :k], np.dot(S_k[:k, :k], Vt[:k, :]))  # Reconstruct the image from the reduced SVD\n\n# Display the original and compressed images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(A, cmap='gray', vmin=0, vmax=255)  # Display original image\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(A_k, cmap='gray', vmin=0, vmax=255)  # Display compressed image\nplt.title(f'Compressed Image (k = {k})')\nplt.axis('off')\n\nplt.show()\n```\n:::\n\n## Assessing quality of compression\n\n\n:::{.panel-tabset}\n\n### Code\n\n```{.matlab}\n% Calculate Mean Squared Error (MSE)\nmse = mean((A(:) - A_k(:)).^2);\n\n% Calculate Peak Signal-to-Noise Ratio (PSNR)\nmax_pixel_value = 255; % Maximum pixel value for 8-bit images\npsnr = 10 * log10((max_pixel_value^2) / mse);\n\n% Calculate sizes\noriginal_size = numel(A) * 8; % Size of the original image in bytes (double data type)\ncompressed_size = (k * (size(A, 1) + size(A, 2))) * 8; % Size of compressed representation (U, S_k, V)\n\n% Display results\nfprintf('Mean Squared Error (MSE): %.4f\\n', mse);\nfprintf('Peak Signal-to-Noise Ratio (PSNR): %.4f dB\\n', psnr);\nfprintf('Original Image Size: %.2f KB\\n', original_size / 1024); % Convert to KB\nfprintf('Compressed Image Size: %.2f KB\\n', compressed_size / 1024); % Convert to KB\nfprintf('Size Reduction: %.2f KB\\n', (original_size - compressed_size) / 1024); % Convert to KB\n```\n\n### Output\n\n```{.python}\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\nimport math\n# Mean Squared Error (MSE)\nmse = np.mean((A - A_k) ** 2)\n\n# Peak Signal-to-Noise Ratio (PSNR)\nmax_pixel_value = 255.0  # For an 8-bit image\npsnr = 10 * np.log10((max_pixel_value ** 2) / mse)\n\n# Structural Similarity Index (SSIM)\nssim_index = ssim(A, A_k, data_range=max_pixel_value)\n\n# Compression Ratio (CR)\noriginal_size = A.nbytes\ncompressed_size = (U[:, :k].nbytes + S_k[:k, :k].nbytes + Vt[:k, :].nbytes)\ncompression_ratio = original_size / compressed_size\n\n# Normalized Cross-Correlation (NCC)\nncc = np.sum(A * A_k) / np.sqrt(np.sum(A ** 2) * np.sum(A_k ** 2))\n# Display results\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr:.4f} dB\")\nprint(f\"Structural Similarity Index (SSIM): {ssim_index:.4f}\")\nprint(f\"Compression Ratio (CR): {compression_ratio:.2f}\")\nprint(f\"Normalized Cross-Correlation (NCC): {ncc:.4f}\")\nprint(f'Original Image Size: {original_size / 1024:.2f} KB')  # Convert to KB\nprint(f'Compressed Image Size: {compressed_size / 1024:.2f} KB')  # Convert to KB\nprint(f'Size Reduction: {(original_size - compressed_size) / 1024:.2f} KB') \n```\n:::\n\n\n\n```{.python}\n#pip install PyWavelets\nimport cv2\nimport numpy as np\nimport pywt\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\n\n# Load and convert image to grayscale\nimg = cv2.imread('amrita_campus.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to display images side-by-side\ndef display_images(original, compressed, title):\n    plt.figure(figsize=(10,5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original, cmap='gray')\n    plt.title(\"Original Image\")\n    plt.subplot(1, 2, 2)\n    plt.imshow(compressed, cmap='gray')\n    plt.title(title)\n    plt.show()\n\n# 1. Discrete Cosine Transform (DCT) Compression\ndef dct_compression(img, k=50):\n    img = img.astype(np.float32)\n    dct_img = cv2.dct(img)  # Apply DCT\n    dct_img[np.abs(dct_img) < k] = 0  # Thresholding\n    compressed_img = cv2.idct(dct_img)  # Apply inverse DCT\n    display_images(img, compressed_img, \"DCT Compressed Image\")\n    return compressed_img\n\n# 2. Wavelet Transform Compression (JPEG 2000 equivalent)\ndef wavelet_compression(img, wavelet='haar', level=1, threshold=10):\n    coeffs = pywt.wavedec2(img, wavelet, level=level)\n    coeffs_thresholded = []\n    for c in coeffs:\n        if isinstance(c, tuple):  # For detail coefficients\n            coeffs_thresholded.append(tuple(pywt.threshold(arr, threshold, mode='soft') for arr in c))\n        else:  # For approximation coefficients\n            coeffs_thresholded.append(pywt.threshold(c, threshold, mode='soft'))\n    compressed_img = pywt.waverec2(coeffs_thresholded, wavelet)\n    display_images(img, compressed_img, \"Wavelet Compressed Image\")\n    return compressed_img\n\n# 3. Fractal Compression (simplified example with downsampling)\ndef fractal_compression(img, scale_factor=0.5):\n    small_img = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)\n    compressed_img = cv2.resize(small_img, (img.shape[1], img.shape[0]))  # Upscale back\n    display_images(img, compressed_img, \"Fractal Compressed Image (downsampled)\")\n    return compressed_img\n\n# 4. Run-Length Encoding (RLE) Compression\ndef rle_compression(img):\n    pixels = img.flatten()\n    rle = []\n    i = 0\n    while i < len(pixels):\n        count = 1\n        while i + 1 < len(pixels) and pixels[i] == pixels[i + 1]:\n            i += 1\n            count += 1\n        rle.append((pixels[i], count))\n        i += 1\n    # Decoding RLE for display (just a simple reconstruction)\n    decompressed = np.concatenate([np.full(count, val) for val, count in rle])\n    decompressed_img = decompressed.reshape(img.shape)\n    display_images(img, decompressed_img, \"RLE Compressed Image\")\n    return decompressed_img\n\n# 5. Predictive Coding Compression\ndef predictive_coding_compression(img):\n    img = img.astype(np.int16)  # To handle negative differences\n    prediction_error = img.copy()\n    for i in range(1, img.shape[0]):\n        for j in range(1, img.shape[1]):\n            prediction = (img[i-1, j] + img[i, j-1]) // 2\n            prediction_error[i, j] = img[i, j] - prediction\n    compressed_img = np.clip(prediction_error + img.mean(), 0, 255).astype(np.uint8)\n    display_images(img, compressed_img, \"Predictive Coded Image\")\n    return compressed_img\n\n# Run all compression methods\ndct_compressed = dct_compression(img)\nwavelet_compressed = wavelet_compression(img)\nfractal_compressed = fractal_compression(img)\nrle_compressed = rle_compression(img)\npredictive_coded = predictive_coding_compression(img)\n\n```\n\n```{.python}\nimport numpy as np\nimport cv2\nimport pywt\nimport matplotlib.pyplot as plt\n\ndef load_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n    return img\n\ndef calculate_metrics(original, compressed):\n    mse = np.mean((original - compressed) ** 2)\n    psnr = 10 * np.log10(255**2 / mse) if mse != 0 else float('inf')\n    \n    # Update SSIM to include data_range\n    from skimage.metrics import structural_similarity as ssim\n    ssim_value = ssim(original, compressed, data_range=original.max() - original.min())\n    \n    return mse, psnr, ssim_value\n\ndef svd_compression(img, k=50):\n    A = img.astype(np.float32)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    A_k = np.dot(U, np.dot(np.diag(S_k), Vt))\n\n    compressed = {\n        'U': U[:, :k],\n        'S': S_k[:k],\n        'Vt': Vt[:k, :]\n    }\n    \n    # Debug: Check sizes\n    compressed_size = compressed['U'].nbytes + compressed['S'].nbytes + compressed['Vt'].nbytes\n    print(f\"SVD Compressed Size: {compressed_size} bytes\")\n    return A_k\n\ndef dct_compression(img, threshold=10):\n    dct = cv2.dct(np.float32(img))\n    dct[dct < threshold] = 0  # Zero out small coefficients\n    idct = cv2.idct(dct)\n\n    # Debug: Check sizes\n    compressed_size = dct.nbytes + idct.nbytes\n    print(f\"DCT Compressed Size: {compressed_size} bytes\")\n    return idct\n\ndef wavelet_compression(img, threshold=0.1):\n    coeffs = pywt.wavedec2(img, 'haar', level=2)\n    coeffs_thresholded = [coeffs[0]] + [tuple(pywt.threshold(c, threshold, mode='soft') for c in detail) for detail in coeffs[1:]]\n    \n    # Reconstruct the image from the thresholded coefficients\n    img_reconstructed = pywt.waverec2(coeffs_thresholded, 'haar')\n    \n    # Calculate the compressed size correctly\n    compressed_size = sum(c.nbytes for c in coeffs_thresholded[1]) + coeffs_thresholded[0].nbytes + img_reconstructed.nbytes\n    print(f\"Wavelet Compressed Size: {compressed_size} bytes\")\n    \n    return img_reconstructed\n\ndef display_images(original, compressed, title1, title2):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(title1)\n    plt.imshow(original, cmap='gray')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title(title2)\n    plt.imshow(compressed, cmap='gray')\n    plt.axis('off')\n\n    plt.show()\n\ndef main():\n    file_path = r'D:\\SVD_project\\amrita_campus.jpg'  # Use a raw string for Windows paths\n    original_image = load_image(file_path)\n\n    # Get original image size in KB\n    original_size = original_image.nbytes / 1024  # Convert bytes to KB\n\n    # Perform compression\n    svd_compressed = svd_compression(original_image)\n    dct_compressed = dct_compression(original_image)\n    wavelet_compressed = wavelet_compression(original_image)\n\n    # Calculate metrics\n    svd_mse, svd_psnr, svd_ssim = calculate_metrics(original_image, svd_compressed)\n    dct_mse, dct_psnr, dct_ssim = calculate_metrics(original_image, dct_compressed)\n    wavelet_mse, wavelet_psnr, wavelet_ssim = calculate_metrics(original_image, wavelet_compressed)\n\n    # Calculate Compressed Sizes and additional metrics\n    svd_compressed_size = svd_compressed.nbytes / 1024  # Size in KB\n    dct_compressed_size = dct_compressed.nbytes / 1024  # Size in KB\n    wavelet_compressed_size = wavelet_compressed.nbytes / 1024  # Size in KB\n\n    svd_cr = original_size / svd_compressed_size\n    dct_cr = original_size / dct_compressed_size\n    wavelet_cr = original_size / wavelet_compressed_size\n\n    # NCC calculation\n    def normalized_cross_correlation(original, compressed):\n        return np.sum(original * compressed) / (np.linalg.norm(original) * np.linalg.norm(compressed))\n\n    svd_ncc = normalized_cross_correlation(original_image, svd_compressed)\n    dct_ncc = normalized_cross_correlation(original_image, dct_compressed)\n    wavelet_ncc = normalized_cross_correlation(original_image, wavelet_compressed)\n\n    # Size Reduction\n    svd_size_reduction = original_size - svd_compressed_size\n    dct_size_reduction = original_size - dct_compressed_size\n    wavelet_size_reduction = original_size - wavelet_compressed_size\n\n    # Print Comparison Table\n    print(f\"{'Method':<10} {'MSE':<20} {'PSNR (dB)':<15} {'SSIM':<15} {'CR':<10} {'NCC':<10} {'Compressed Size (KB)':<25} {'Size Reduction (KB)':<20}\")\n    print(f\"{'SVD':<10} {svd_mse:<20.4f} {svd_psnr:<15.4f} {svd_ssim:<15.4f} {svd_cr:<10.2f} {svd_ncc:<10.4f} {svd_compressed_size:<25.2f} {svd_size_reduction:<20.2f}\")\n    print(f\"{'DCT':<10} {dct_mse:<20.4f} {dct_psnr:<15.4f} {dct_ssim:<15.4f} {dct_cr:<10.2f} {dct_ncc:<10.4f} {dct_compressed_size:<25.2f} {dct_size_reduction:<20.2f}\")\n    print(f\"{'Wavelet':<10} {wavelet_mse:<20.4f} {wavelet_psnr:<15.4f} {wavelet_ssim:<15.4f} {wavelet_cr:<10.2f} {wavelet_ncc:<10.4f} {wavelet_compressed_size:<25.2f} {wavelet_size_reduction:<20.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n```{.python}\nimport numpy as np\nimport cv2\nimport pywt\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\n\ndef load_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n    return img\n\ndef calculate_metrics(original, compressed):\n    mse = np.mean((original - compressed) ** 2)\n    psnr = 10 * np.log10(255**2 / mse) if mse != 0 else float('inf')\n    ssim_value = ssim(original, compressed, data_range=original.max() - original.min())\n    return mse, psnr, ssim_value\n\ndef svd_compression(img, k=20):\n    A = img.astype(np.float32)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    A_k = np.dot(U[:, :k], np.dot(np.diag(S_k[:k]), Vt[:k, :]))\n\n    compressed = {\n        'U': U[:, :k],\n        'S': S_k[:k],\n        'Vt': Vt[:k, :]\n    }\n    \n    # Calculate the size of compressed data\n    compressed_size = compressed['U'].nbytes + compressed['S'].nbytes + compressed['Vt'].nbytes\n    print(f\"SVD Compressed Size: {compressed_size} bytes\")\n    return A_k, compressed_size\n\ndef dct_compression(img, threshold=10):\n    dct = cv2.dct(np.float32(img))\n    dct[dct < threshold] = 0  # Zero out small coefficients\n    \n    # Count non-zero coefficients\n    non_zero_coeffs = np.count_nonzero(dct)\n    compressed_size = dct.nbytes - (dct.size - non_zero_coeffs) * dct.dtype.itemsize  # Size excluding zeros\n\n    idct = cv2.idct(dct)  # Reconstruct the image (not needed for size calculation)\n\n    print(f\"DCT Compressed Size: {compressed_size} bytes\")\n    return idct, compressed_size\n\ndef wavelet_compression(image, wavelet='haar', threshold=0.2):\n    # Perform 2D wavelet decomposition\n    coeffs = pywt.wavedec2(image, wavelet)\n    \n    # Threshold the detail coefficients\n    coeffs_thresholded = list(coeffs)\n    for i in range(1, len(coeffs_thresholded)):\n        coeffs_thresholded[i] = tuple(pywt.threshold(c, threshold, mode='soft') for c in coeffs_thresholded[i])\n    \n    # Calculate compressed size\n    compressed_size = sum(np.prod(c.shape) * c.dtype.itemsize for detail in coeffs_thresholded[1:] for c in detail) + coeffs_thresholded[0].nbytes\n    \n    # Reconstruct the image\n    img_reconstructed = pywt.waverec2(coeffs_thresholded, wavelet)\n    \n    return img_reconstructed, compressed_size\n\ndef display_images(original, compressed, title1, title2):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(title1)\n    plt.imshow(original, cmap='gray')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title(title2)\n    plt.imshow(compressed, cmap='gray')\n    plt.axis('off')\n\n    plt.show()\n\ndef main():\n    file_path = r'D:\\SVD_project\\amrita_campus.jpg'  # Use a raw string for Windows paths\n    original_image = load_image(file_path)\n\n    # Get original image size in bytes\n    original_size = original_image.nbytes  # Size in bytes\n\n    # Perform compression\n    svd_compressed, svd_compressed_size = svd_compression(original_image)\n    dct_compressed, dct_compressed_size = dct_compression(original_image)\n    wavelet_compressed, wavelet_compressed_size = wavelet_compression(original_image)\n\n    # Calculate metrics\n    svd_mse, svd_psnr, svd_ssim = calculate_metrics(original_image, svd_compressed)\n    dct_mse, dct_psnr, dct_ssim = calculate_metrics(original_image, dct_compressed)\n    wavelet_mse, wavelet_psnr, wavelet_ssim = calculate_metrics(original_image, wavelet_compressed)\n    \n    # Calculate Compressed Sizes and additional metrics\n    svd_cr = original_size / svd_compressed_size\n    dct_cr = original_size / dct_compressed_size\n    wavelet_cr = original_size / wavelet_compressed_size\n\n    # NCC calculation\n    def normalized_cross_correlation(original, compressed):\n        return np.sum(original * compressed) / (np.linalg.norm(original) * np.linalg.norm(compressed))\n\n    svd_ncc = normalized_cross_correlation(original_image, svd_compressed)\n    dct_ncc = normalized_cross_correlation(original_image, dct_compressed)\n    wavelet_ncc = normalized_cross_correlation(original_image, wavelet_compressed)\n\n    # Size Reduction\n    svd_size_reduction = original_size - svd_compressed_size\n    dct_size_reduction = original_size - dct_compressed_size\n    wavelet_size_reduction = original_size - wavelet_compressed_size\n\n    # Print Comparison Table\n    print(f\"{'Method':<10} {'MSE':<20} {'PSNR (dB)':<15} {'SSIM':<15} {'CR':<10} {'NCC':<10} {'Compressed Size (KB)':<25} {'Size Reduction (KB)':<20}\")\n    print(f\"{'SVD':<10} {svd_mse:<20.4f} {svd_psnr:<15.4f} {svd_ssim:<15.4f} {svd_cr:<10.2f} {svd_ncc:<10.4f} {svd_compressed_size/1024:<25.2f} {svd_size_reduction/1024:<20.2f}\")\n    print(f\"{'DCT':<10} {dct_mse:<20.4f} {dct_psnr:<15.4f} {dct_ssim:<15.4f} {dct_cr:<10.2f} {dct_ncc:<10.4f} {dct_compressed_size/1024:<25.2f} {dct_size_reduction/1024:<20.2f}\")\n    print(f\"{'Wavelet':<10} {wavelet_mse:<20.4f} {wavelet_psnr:<15.4f} {wavelet_ssim:<15.4f} {wavelet_cr:<10.2f} {wavelet_ncc:<10.4f} {wavelet_compressed_size/1024:<25.2f} {wavelet_size_reduction/1024:<20.2f}\")\n    display_images(original_image, svd_compressed,\"original\",\"compressed\")\nif __name__ == \"__main__\":\n    main()\n    \n```\n\n```{.python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Set number of components to visualize\nnum_components = 2\n\n# Function to normalize and visualize singular vectors\ndef visualize_singular_vectors(vectors, title, n_components, shape):\n    fig, axs = plt.subplots(1, n_components, figsize=(15, 5))\n    fig.suptitle(title, fontsize=16)\n    for i in range(n_components):\n        vector = vectors[:, i] if title == 'Column Space (U)' else vectors[i, :]\n        # Normalize vector for better visibility\n        normalized_vector = (vector - np.min(vector)) / (np.max(vector) - np.min(vector))\n        axs[i].imshow(normalized_vector.reshape(shape), cmap='gray', aspect='auto')\n        axs[i].axis('off')\n        axs[i].set_title(f'Component {i+1}')\n    plt.show()\n\n# Visualize Column Space (U matrix columns)\nvisualize_singular_vectors(U, \"Column Space (U)\", num_components, (image.shape[0], 1))\n\n# Visualize Row Space (V^T matrix rows)\nvisualize_singular_vectors(Vt, \"Row Space (V^T)\", num_components, (1, image.shape[1]))\n\n# Plot Singular Values\nplt.figure(figsize=(8, 6))\nplt.plot(np.log(1+S), 'o-', label=\"Singular Values\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Singular Value\")\nplt.title(\"Singular Values Plot\")\nplt.legend()\nplt.grid()\nplt.show()\n\n```\n\n### Ploting the signal and noise part of an image \n\n\n```{.python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\n\n# Load the image and convert it to grayscale\nimage = io.imread('.TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\n\n# Perform SVD\nU, S, VT = np.linalg.svd(image, full_matrices=False)\n\n# Number of components to keep\nk = 40  # Adjust this for more or fewer components\n\n# Reconstruct the signal part\nS_k = np.zeros_like(S)  # Create a zero array for singular values\nS_k[:k] = S[:k]  # Keep the largest k singular values\n\n# Reconstruct the signal image\nreconstructed_signal = U @ np.diag(S_k) @ VT\n\n# Extract noise\nnoise = image - reconstructed_signal\n# Convert images to uint8 for saving\nimage_uint8 = (image * 255).astype(np.uint8)\nreconstructed_signal_uint8 = (reconstructed_signal * 255).astype(np.uint8)\nnoise_uint8 = (noise * 255).astype(np.uint8)\n\n# Save each image as a PDF\nio.imsave('original_image.pdf', image_uint8)\nio.imsave('reconstructed_signal_k_{}.pdf'.format(k), reconstructed_signal_uint8)\nio.imsave('extracted_noise.pdf', noise_uint8)\n# Plotting\nplt.figure(figsize=(15, 10))\n\nplt.subplot(1, 3, 1)\nplt.title('Original Image')\nplt.imshow(image, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.title('Reconstructed Signal (k={})'.format(k))\nplt.imshow(reconstructed_signal, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.title('Extracted Noise')\nplt.imshow(noise, cmap='gray')\nplt.axis('off')\n\nplt.tight_layout()\n# Save the entire figure as a PDF\nplt.savefig('comparison_plot.pdf', bbox_inches='tight')\nplt.show()\n```\n\n\n### Compression quality with different values of k\n\n```{.python}\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\n\n# Initialize a list to store results\nresults = []\n\n# Define a range of k values\nk_values = [1, 5, 10, 20, 50, 100, 200, 400, 600, 800, 1000]\n\nfor k in k_values:\n    # Perform SVD\n    U, S, VT = np.linalg.svd(image, full_matrices=False)\n\n    # Reconstruct the signal part with k components\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    reconstructed_signal = U @ np.diag(S_k) @ VT\n\n    # Compute PSNR and SSIM\n    current_psnr = psnr(image, reconstructed_signal)\n    \n    # Set data_range for SSIM\n    data_range = 1  # Use 255 if your image is in the range [0, 255]\n    current_ssim = ssim(image, reconstructed_signal, data_range=data_range)\n\n    # Append results\n    results.append({'k': k, 'PSNR': current_psnr, 'SSIM': current_ssim})\n# Create a DataFrame from the results\nresults_df = pd.DataFrame(results)\n\n# Display the DataFrame as a table\nprint(results_df)\n\n# Optionally, save the results to a CSV file\nresults_df.to_csv('psnr_ssim_variation.csv', index=False)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(results_df['k'], results_df['PSNR'], marker='o', label='PSNR')\nplt.plot(results_df['k'], results_df['SSIM'], marker='o', label='SSIM')\nplt.xscale('log')  # Log scale for better visualization\nplt.xlabel('Number of Components (k)')\nplt.ylabel('Value')\nplt.title('Variation of PSNR and SSIM with Different k Values')\nplt.legend()\nplt.grid()\nplt.savefig('psnr_ssim_variation_plot.pdf')\nplt.show()\n```\n\n### Creating images from the left singular and right singular matrices\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Normalize function for better visualization\ndef normalize_matrix(matrix):\n    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n\n# Save the original image\nplt.imshow(image, cmap='gray')\nplt.axis('off')\nplt.title('Original Image')\nplt.savefig('original_image.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n\n# Save the left singular matrix (U)\nplt.imshow(normalize_matrix(U), cmap='gray', aspect='auto')\nplt.axis('off')\nplt.title('Left Singular Matrix (U)')\nplt.savefig('left_singular_matrix_U.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n\n# Save the right singular matrix (V^T)\nplt.imshow(normalize_matrix(Vt), cmap='gray', aspect='auto')\nplt.axis('off')\nplt.title('Right Singular Matrix (V^T)')\nplt.savefig('right_singular_matrix_Vt.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n```\n\n### Ploting the Dominant and sub dominant components\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Number of components for partial reconstruction\nk = 50  # Choose k to retain enough detail without full reconstruction\n\n# Reconstructing left singular matrix U and right singular matrix V^T with k components\nU_reconstructed = U[:, :k] @ np.diag(S[:k])\nVt_reconstructed = np.diag(S[:k]) @ Vt[:k, :]\n\n# Set figure size based on the original image dimensions\nfigsize = (image.shape[1] / 100, image.shape[0] / 100)\n\n# Function to save reconstructed singular matrix images with the same dimensions as the original\ndef save_reconstructed_matrix(matrix, title, filename):\n    fig, ax = plt.subplots(figsize=figsize)\n    # Normalize for better visualization\n    #normalized_matrix = (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n    ax.imshow(matrix, cmap='gray', aspect='auto')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n\n# Save the original image in grayscale for reference\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\n\n# Save reconstructed left singular matrix U\nsave_reconstructed_matrix(U_reconstructed, \"Reconstructed Left Singular Matrix (U)\", \"left_singular_matrix_U_traces.pdf\")\n\n# Save reconstructed right singular matrix V^T\nsave_reconstructed_matrix(Vt_reconstructed, \"Reconstructed Right Singular Matrix (V^T)\", \"right_singular_matrix_Vt_traces.pdf\")\n```\n\n### Code for Denoising with SVD\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the original image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Check the original image statistics\nprint(\"Original Image - Min:\", np.min(image), \"Max:\", np.max(image))\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.1  # Set this value based on your requirements\n# Generate Gaussian noise\nnoise = np.random.normal(0, np.sqrt(noise_variance), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Check the noisy image statistics\nprint(\"Noisy Image - Min:\", np.min(noisy_image), \"Max:\", np.max(noisy_image))\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Filter Noise by Truncating Smaller Singular Values\n# Adjust `k` to retain more singular values, which should improve quality\nk = 50  # Adjust this value as needed\nS_denoised = np.zeros_like(S)\nS_denoised[:k] = S[:k]  # Keep the top `k` singular values\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\n\n# Check the denoised image statistics\nprint(\"Denoised Image - Min:\", np.min(denoised_image), \"Max:\", np.max(denoised_image))\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(\"Noisy Image\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(\"Denoised Image (SVD)\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Saving each image as a standalone PDF\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\nplt.imsave(\"noisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"denoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n## Denoising BSD400 Dataet using SVD\n\n```{python}\nimport numpy as np\nfrom skimage import io, color, metrics\nimport matplotlib.pyplot as plt\n\n# Load the original image from the BSD400 dataset and convert it to grayscale\nimage = io.imread('DenoiseImage.png')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Normalize the image to the range [0, 1]\nimage -= np.min(image)\nimage /= np.max(image)\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.001  # Use a smaller noise variance relative to the normalized image\n# Generate Gaussian noise scaled by the maximum value of the image\nnoise = np.random.normal(0, np.sqrt(noise_variance * np.max(image)), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Determine a threshold for singular values\nthreshold =0.618*np.mean(S)  # Example threshold: mean of singular values\nS_denoised = np.where(S > threshold, S, 0)  # Set small singular values to zero\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\ndata_range = 1.0  # If your images are in the range [0, 1]. Use 255 if they are in [0, 255].\n\n# Step 5: Calculate PSNR and SSIM to assess denoising performance\npsnr_noisy = metrics.peak_signal_noise_ratio(image, noisy_image, data_range=data_range)\nssim_noisy = metrics.structural_similarity(image, noisy_image, data_range=data_range)\n\npsnr_denoised = metrics.peak_signal_noise_ratio(image, denoised_image, data_range=data_range)\nssim_denoised = metrics.structural_similarity(image, denoised_image, data_range=data_range)\nprint(f\"PSNR (Noisy Image): {psnr_noisy:.2f}\")\nprint(f\"SSIM (Noisy Image): {ssim_noisy:.4f}\")\nprint(f\"PSNR (Denoised Image): {psnr_denoised:.2f}\")\nprint(f\"SSIM (Denoised Image): {ssim_denoised:.4f}\")\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}, SSIM: {ssim_noisy:.4f}\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(f\"Denoised Image (SVD)\\nPSNR: {psnr_denoised:.2f}, SSIM: {ssim_denoised:.4f}\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Saving each image as a standalone PDF\nplt.imsave(\"BSDoriginal_image.pdf\", image, cmap='gray')\nplt.imsave(\"BSDnoisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"BSDdenoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n\n### Assesing quality of SVD denoiser\n\n```{python}\nimport numpy as np\nfrom skimage import io, color, metrics\nimport matplotlib.pyplot as plt\n\n# Load the original image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Check the original image statistics\nprint(\"Original Image - Min:\", np.min(image), \"Max:\", np.max(image))\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.1  # Set this value based on your requirements\n# Generate Gaussian noise\nnoise = np.random.normal(0, np.sqrt(noise_variance), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Check the noisy image statistics\nprint(\"Noisy Image - Min:\", np.min(noisy_image), \"Max:\", np.max(noisy_image))\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Filter Noise by Truncating Smaller Singular Values\n# Adjust `k` to retain more singular values, which should improve quality\nk = 50  # Adjust this value as needed\nS_denoised = np.zeros_like(S)\nS_denoised[:k] = S[:k]  # Keep the top `k` singular values\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\n\n# Check the denoised image statistics\nprint(\"Denoised Image - Min:\", np.min(denoised_image), \"Max:\", np.max(denoised_image))\ndata_range = 1.0  # If your images are in the range [0, 1]. Use 255 if they are in [0, 255].\n\n# Step 5: Calculate PSNR and SSIM to assess denoising performance\npsnr_noisy = metrics.peak_signal_noise_ratio(image, noisy_image, data_range=data_range)\nssim_noisy = metrics.structural_similarity(image, noisy_image, data_range=data_range)\n\npsnr_denoised = metrics.peak_signal_noise_ratio(image, denoised_image, data_range=data_range)\nssim_denoised = metrics.structural_similarity(image, denoised_image, data_range=data_range)\nprint(f\"PSNR (Noisy Image): {psnr_noisy:.2f}\")\nprint(f\"SSIM (Noisy Image): {ssim_noisy:.4f}\")\nprint(f\"PSNR (Denoised Image): {psnr_denoised:.2f}\")\nprint(f\"SSIM (Denoised Image): {ssim_denoised:.4f}\")\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}, SSIM: {ssim_noisy:.4f}\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(f\"Denoised Image (SVD)\\nPSNR: {psnr_denoised:.2f}, SSIM: {ssim_denoised:.4f}\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Optionally save each image as a standalone PDF\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\nplt.imsave(\"noisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"denoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n### Ploting the correlation between regenerated images over k\n\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the original grayscale image\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD on the original image\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# List of k-values to experiment with\nk_values = [10, 20, 50, 100, 200, 400, 600]\n\n# Flatten the original image to compare correlations\noriginal_flattened = image.flatten()\n\n# Array to store correlations with the original image for each k\ncorrelations = []\n\n# Reconstruct images using different numbers of singular values and compute correlations\nfor k in k_values:\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]  # Retain only the top k singular values\n    reconstructed_image = U @ np.diag(S_k) @ Vt\n    reconstructed_flattened = reconstructed_image.flatten()\n    \n    # Calculate correlation with the original image\n    corr = np.corrcoef(original_flattened, reconstructed_flattened)[0, 1]\n    correlations.append(corr)\n\n# Plotting k-values against their corresponding correlations\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, correlations, marker='o', linestyle='-')\nplt.xlabel(\"Number of Singular Values (k)\")\nplt.ylabel(\"Correlation with Original Image\")\nplt.title(\"Correlation between Reconstructed Images and Original Image\")\nplt.grid()\nplt.show()\n```\n\n### Image Forensic with SVD\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\n\ndef preprocess_image(image):\n    if image.ndim == 3:\n        if image.shape[2] == 4:  # Check for RGBA\n            image = image[:, :, :3]  # Take RGB only\n        gray_image = color.rgb2gray(image)\n    else:\n        gray_image = image  # Already grayscale\n    return gray_image.astype(float)\n\ndef embed_watermark(image, watermark, alpha=0.1):\n    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n    watermark_resized = np.resize(watermark, S.shape)\n    S_watermarked = S + alpha * watermark_resized\n    watermarked_image = np.dot(U, np.dot(np.diag(S_watermarked), Vt))\n    return np.clip(watermarked_image, 0, 1)\n\ndef extract_watermark(original_image, watermarked_image, alpha=0.1):\n    U1, S1, Vt1 = np.linalg.svd(original_image, full_matrices=False)\n    U2, S2, Vt2 = np.linalg.svd(watermarked_image, full_matrices=False)\n    extracted_watermark = (S2 - S1) / alpha\n    return extracted_watermark\n\n# Load images\nhost_image = io.imread('TESTIMAGE.png')\nhost_image = preprocess_image(host_image)\n\nwatermark_image = io.imread('amritha_TL.png')\nwatermark_image = preprocess_image(watermark_image)\n\n# Ensure images are in [0, 1] range\nhost_image = np.clip(host_image, 0, 1)\nwatermark_image = np.clip(watermark_image, 0, 1)\n\n# Embed watermark\nwatermarked_image = embed_watermark(host_image, watermark_image, alpha=0.1)\n\n# Extract watermark\nextracted_watermark = extract_watermark(host_image, watermarked_image, alpha=0.1)\n\n# Reshape the extracted watermark to the size of the original watermark image\nextracted_watermark = np.clip(extracted_watermark, 0, 1)  # Ensure valid pixel range\nextracted_watermark = np.resize(extracted_watermark, watermark_image.shape)  # Resize to match original watermark\n\n# Plotting results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\naxs[0].imshow(host_image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\naxs[1].imshow(watermarked_image, cmap='gray')\naxs[1].set_title(\"Watermarked Image\")\naxs[1].axis('off')\n\naxs[2].imshow(extracted_watermark, cmap='gray')\naxs[2].set_title(\"Extracted Watermark\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n```\n\n```{python}\nimport numpy as np\nimport cv2\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\ndef read_image(filename):\n    # Read the image using OpenCV\n    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)  # Read with unchanged channels\n    if img is None:\n        raise ValueError(\"Image not found or unable to read.\")\n    \n    # Check if the image has an alpha channel\n    if img.shape[2] == 4:  # If there are 4 channels (RGBA)\n        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)  # Convert to BGR without alpha\n    \n    # Convert to grayscale if it's a 3-channel image\n    if len(img.shape) == 3 and img.shape[2] == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    return img\n\n# Load host image\nhost_image = read_image('TESTIMAGE.png')  # Replace with your actual path\nhost_image = cv2.resize(host_image, (256, 256))  # Resize to 256x256\nhost_image = host_image.astype(np.float64)  # Convert to double\n\n# Perform SVD\nUimg, Simg, Vimg = svd(host_image)\n\n# Read watermark\nwatermark_image = read_image('copyright1.png')  # Replace with your actual path\nwatermark_image = cv2.resize(watermark_image, (256, 256))  # Resize to 256x256\nalfa = 0.1  # Alpha value for watermark embedding\nwatermark_image = watermark_image.astype(np.float64)  # Convert to double\n\n# Ensure watermark is in the same shape as Simg\nif watermark_image.shape[0] > len(Simg):\n    watermark_image = watermark_image[:len(Simg), :]\n\n# Modify the singular values by adding the scaled watermark\nfor i in range(len(Simg)):\n    Simg[i] += alfa * watermark_image[i, 0]  # Update singular values\n\n# Reconstruct the watermarked image\nSimg_matrix = np.diag(Simg)  # Create a diagonal matrix from singular values\nwatermarked_image = np.dot(Uimg, np.dot(Simg_matrix, Vimg))\n\n# Normalize the watermarked image to the range [0, 255]\nwatermarked_image = np.clip(watermarked_image, 0, 255).astype(np.uint8)\n\n# Display the original host image\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(host_image, cmap='gray')\nplt.title('The Original Image')\nplt.axis('off')\n\n# Display the watermarked image\nplt.subplot(1, 2, 2)\nplt.imshow(watermarked_image, cmap='gray')\nplt.title('Watermarked Image')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n```\n\n## Extraction part\n\n```{python}\nimport numpy as np\nimport cv2\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\ndef read_image(filename):\n    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n    if img is None:\n        raise ValueError(f\"Image not found or unable to read: {filename}\")\n    \n    if len(img.shape) == 3 and img.shape[2] == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    return img\n\ndef embed_watermark(host_image, watermark_image, alfa=0.1):\n    Uimg, Simg, Vimg = svd(host_image)\n    \n    if watermark_image.shape[0] > len(Simg):\n        raise ValueError(\"Watermark size exceeds singular values length.\")\n\n    for i in range(len(Simg)):\n        Simg[i] += alfa * watermark_image[i, 0]\n\n    Simg_matrix = np.diag(Simg)\n    watermarked_image = np.dot(Uimg, np.dot(Simg_matrix, Vimg))\n    \n    return watermarked_image, Simg, Uimg, Vimg\n\ndef extract_watermark(watermarked_image, original_singular_values, alfa=0.1, watermark_shape=None):\n    U_Wimg, S_Wimg, V_Wimg = svd(watermarked_image)\n\n    # Calculate the extracted watermark\n    extracted_watermark = (S_Wimg - original_singular_values) / alfa\n\n    if watermark_shape is not None:\n        extracted_watermark = extracted_watermark.reshape(watermark_shape)\n\n    # Normalize to uint8 range\n    extracted_watermark = np.clip(extracted_watermark, 0, 255).astype(np.uint8)\n\n    return extracted_watermark\n\ndef calculate_psnr(original_image, watermarked_image):\n    mse = np.mean((original_image.astype(np.float64) - watermarked_image.astype(np.float64)) ** 2)\n    if mse == 0:\n        return 100\n    max_pixel_value = 255.0\n    psnr = 10 * np.log10((max_pixel_value ** 2) / mse)\n    return psnr\n\n# Load host image\nhost_image = read_image('TESTIMAGE.png')  # Replace with your actual path\nhost_image = cv2.resize(host_image, (256, 256)).astype(np.float64)\n\n# Load watermark image\nwatermark_image = read_image('copyright1.png')  # Replace with your actual path\nwatermark_image = cv2.resize(watermark_image, (256, 1)).astype(np.float64)\n\n# Embed watermark\nalfa = 0.1\nwatermarked_image, Simg, Uimg, Vimg = embed_watermark(host_image, watermark_image, alfa)\n\n# Extract watermark\nextracted_watermark = extract_watermark(watermarked_image, Simg, alfa, watermark_shape=(256, 1))\n\n# Calculate PSNR\npsnr_value = calculate_psnr(host_image, watermarked_image)\n\n# Display the images\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.imshow(host_image, cmap='gray')\nplt.title('The Original Image')\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(watermarked_image, cmap='gray')\nplt.title('Watermarked Image')\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(extracted_watermark, cmap='gray')\nplt.title('Extracted Watermark')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"PSNR between the original and watermarked image: {psnr_value:.2f} dB\")\n```","srcMarkdownNoYaml":"\n\n\n# Introduction\n\nImage processing has become integral to numerous fields, from medical imaging to digital forensics, where large volumes of visual data demand efficient storage, transmission, and quality retention techniques. Among the many mathematical transformations applied to images, Singular Value Decomposition (SVD) has emerged as a particularly valuable tool. SVD is a matrix factorization technique that represents a given matrix as a product of three matrices: $U$, $\\Sigma$, and $V^T$. This decomposition is significant in image processing because it maximizes the energy contained in the largest singular values, enabling the creation of compact, high-quality approximations of the original data. Unlike other transformations, SVD does not require a specific image size or type, making it highly adaptable and robust for various image processing tasks.\n\nThe primary strength of SVD lies in its capacity to separate image data into meaningful components. For instance, in an image represented by SVD, the larger singular values and their corresponding vectors encode most of the structural content, while smaller singular values can often represent noise. This property is beneficial for applications requiring data reduction, such as image compression and denoising, where maintaining the primary structure while reducing extraneous information is essential. Additionally, SVD’s stable mathematical foundation and adaptability have made it increasingly popular in other specialized applications, including watermarking for digital forensics and security.\n\nIn image compression, SVD enables reduced data storage by approximating the image using fewer singular values, providing a balance between quality and compression ratio. This application is critical in fields where storage and bandwidth are constrained. Similarly, in denoising, SVD can isolate noise by exploiting the decomposition’s ability to differentiate between dominant and subdominant subspaces, allowing effective noise suppression without significantly affecting the image’s core structure. Furthermore, SVD is also used in watermarking, where slight modifications to specific singular values embed unique patterns within images, enhancing security and ensuring authenticity.\n\nDespite these advantages, SVD in image processing remains an area with unexplored potential. This paper explores these established applications while addressing underutilized SVD properties to uncover new applications. By investigating SVD's adaptive properties in compressing and filtering images, as well as its potential for encoding data securely, this work contributes to a growing body of research on SVD-based image processing and presents promising directions for further study.\n\n# SVD Application in Image Processing\n\nSingular Value Decomposition (SVD) has several important applications in image processing. The SVD can be used to reduce the noise or compress matrix data by eliminating small singular values or higher ranks @Chen2018SingularVD. This allows for the size of stored images to be reduced @cao2006singular. Additionally, the SVD has properties that make it useful for various image processing tasks, such as enhancing image quality and filtering out noise. The main theorem of SVD is reviewed in the search results, and numerical experiments have been conducted to illustrate its applications in image processing.\n\n## Image Compression\n\nImage compression represents a vital technique to reduce the data needed to represent an image. This is crucial for achieving efficient storage and transmission across various applications, including digital photography, video streaming, and web graphics. Compression methods are primarily categorized into two distinct types: lossy and lossless.\n\nLossy compression diminishes file size by irreversibly eliminating certain image data, which can result in a degradation of image quality, as observed in JPEG formats. This method is frequently employed when the reduction of file size is of paramount importance, and any resultant loss in quality is considered acceptable.\n\nConversely, lossless compression techniques allow for the compression of images without any loss of data, facilitating the exact reconstruction of the original image, as exemplified by PNG formats. This approach is beneficial when preserving image quality is essential and minimizing file size is of lesser importance.\n\nThe decision to use either lossy or lossless compression hinges on the specific needs of the application, balancing the trade-offs between file size and image quality.\n\nSVD-based image compression functions by decomposing the image matrix into three components and subsequently approximating the original matrix with only the most significant singular values and vectors. This process results in a compact image representation while preserving the essential information.\n\nMathematically, given an image represented as a matrix $A$ with dimensions $m \\times n$, the Singular Value Decomposition (SVD) decomposes $A$ into three matrices: $U$, $\\Sigma$, and $V^T$. Here, $U$ is an $m \\times m$ orthogonal matrix containing the left singular vectors, $\\Sigma$ is an $m \\times n$ diagonal matrix containing singular values, and $V^T$ is the transpose of an $n \\times n$ orthogonal matrix containing the right singular vectors. To compress the image, we keep only the top $k$ singular values (where $k$ is significantly smaller than both $m$ and $n$). The compressed image can be reconstructed as \n\n$$\nA_k = U_k \\Sigma_k V_k^T,\n$$\n\nwhere $U_k$ contains the first $k$ columns of $U$, $\\Sigma_k$ is a $k \\times k$ diagonal matrix of the top $k$ singular values, and $V_k^T$ consists of the first $k$ rows of $V^T$.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Read and convert the image to grayscale\nimg = Image.open('amrita_campus.jpg')  # Specify your image file\ngray_img = img.convert('L')  # Convert to grayscale\nA = np.array(gray_img, dtype=np.float64)  # Convert to float64 for SVD computation\noriginal=A\n# Apply Singular Value Decomposition (SVD)\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Choose the number of singular values to keep for compression\nk = 50  # You can adjust this value to see different compression levels\n\n# Create a compressed version of the image using the first k singular values\nS_k = np.zeros_like(A)  # Initialize a zero matrix for S_k\nS_k[:k, :k] = np.diag(S[:k])  # Keep only the top k singular values\n\n# Reconstruct the compressed image\nA_k = np.dot(U[:, :k], np.dot(S_k[:k, :k], Vt[:k, :]))  # Reconstruct the image from the reduced SVD\n\n# Display the original and compressed images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(A, cmap='gray', vmin=0, vmax=255)  # Display original image\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(A_k, cmap='gray', vmin=0, vmax=255)  # Display compressed image\nplt.title(f'Compressed Image (k = {k})')\nplt.axis('off')\n\nplt.show()\n```\n\nTo assess the quality of the original and compressed images, various metrics can be employed. Commonly used measures are discussion in this section.\n\n### Image Quality Assessment Metrics\n\nTo evaluate the quality of compressed images relative to their original versions, several standardized metrics are commonly employed. These metrics provide quantitative comparisons across aspects such as pixel-level error, signal fidelity, structural similarity, and compression efficiency. The following are the key metrics used in image quality assessment:\n\n####  Mean Squared Error (MSE)\nThe Mean Squared Error quantifies the average squared difference between corresponding pixel values of the original and compressed images. Lower values indicate higher fidelity to the original. Mathematically, MSE is defined as:\n$$\n\\text{MSE} = \\frac{1}{m \\cdot n} \\sum_{i=1}^{m} \\sum_{j=1}^{n} (A(i,j) - A_k(i,j))^2\n$$\nwhere $A(i,j)$ and $A_k(i,j)$ denote the pixel values of the original and compressed images, respectively, and $m \\times n$ represents the image dimensions.\n\n#### Peak Signal-to-Noise Ratio (PSNR)\nPSNR is a widely used metric that compares the maximum possible signal value to the noise level introduced by compression. It is computed as:\n$$\n\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n$$\nwhere $\\text{MAX}$ represents the maximum pixel value (e.g., 255 for 8-bit images). Higher PSNR values indicate better image quality, as they correspond to lower MSE values.\n\n#### Structural Similarity Index (SSIM)\nThe Structural Similarity Index assesses perceptual similarity by analyzing luminance, contrast, and structural information between the original and compressed images. The SSIM index, ranging from -1 to 1, is calculated as:\n$$\n\\text{SSIM}(A, A_k) = \\frac{(2 \\mu_A \\mu_{A_k} + C_1)(2 \\sigma_{AA_k} + C_2)}{(\\mu_A^2 + \\mu_{A_k}^2 + C_1)(\\sigma_A^2 + \\sigma_{A_k}^2 + C_2)}\n$$\nwhere $\\mu$, $\\sigma$, and $\\sigma_{AA_k}$ denote means, variances, and covariances of $A$ and $A_k$, with constants $C_1$ and $C_2$ to prevent division by zero. Higher SSIM values suggest higher structural fidelity.\n\n#### Compression Ratio (CR)\nCompression Ratio quantifies the efficiency of compression, calculated as the ratio of the original image size to the compressed size:\n$$\n\\text{Compression Ratio} = \\frac{\\text{Size of Original Image}}{\\text{Size of Compressed Image}}\n$$\nA higher compression ratio indicates a greater reduction in file size, which is desirable in applications requiring efficient storage or transmission.\n\n#### Normalized Cross-Correlation (NCC)\nNormalized Cross-Correlation measures the similarity in pixel intensity patterns between the original and compressed images. NCC is calculated as:\n$$\n\\text{NCC} = \\frac{\\sum (A \\cdot A_k)}{\\sqrt{\\sum A^2 \\cdot \\sum A_k^2}}\n$$\nValues closer to 1 indicate a stronger correlation, signifying greater retention of the original image characteristics in the compressed version.\n\nThese metrics collectively provide a comprehensive assessment of image quality by addressing both objective and perceptual aspects of compression, making them suitable for a wide range of applications in image processing and computer vision.\n\n\n\n:::{#tbl-quality-metrics}\n| Metric                               | Value          |\n|:-------------------------------------|----------------|\n| Mean Squared Error (MSE)             | 110.2853       |\n| Peak Signal-to-Noise Ratio (PSNR)    | 27.7056 dB     |\n| Structural Similarity Index (SSIM)   | 0.8116         |\n| Compression Ratio (CR)               | 10.78          |\n| Normalized Cross-Correlation (NCC)   | 0.9976         |\n| Original Image Size                  | 9709.38 KB     |\n| Compressed Image Size                | 900.78 KB      |\n| Size Reduction                       | 8808.59 KB     |\n\n: Quality assessment metrics for original and compressed images, detailing standard measures of image compression and fidelity.\n:::\n\nThe quality assessment metrics indicate effective compression with minimal loss of fidelity in the image. A Mean Squared Error (MSE) of 110.29 suggests that the average pixel intensity differences between the original and compressed images are small. The Peak Signal-to-Noise Ratio (PSNR) of 27.71 dB, typically above the 30 dB threshold for high-quality compression, indicates moderate quality but acceptable for many applications.\n\nThe Structural Similarity Index (SSIM) of 0.8116, close to 1, suggests that the perceptual similarity between the images remains high. The Compression Ratio (CR) of 10.78 shows significant size reduction, and the Normalized Cross-Correlation (NCC) of 0.9976 demonstrates a high correlation between the original and compressed images, supporting strong structural consistency.\n\nThe compressed image achieves substantial size reduction (from 9709.38 KB to 900.78 KB) with reasonable preservation of visual quality, making it suitable for applications prioritizing storage efficiency without heavily compromising visual fidelity.\n\nThe table below presents a comparison of compression quality metrics for three different image compression methods: Singular Value Decomposition (SVD), Discrete Cosine Transform (DCT), and Wavelet Transform. The metrics included are Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Compression Ratio (CR), Normalized Cross-Correlation (NCC), Compressed Size, and Size Reduction. Each metric provides insight into the effectiveness of the compression techniques in terms of image quality and storage efficiency.\n\n:::{#tbl-quality-assessment}\n| Method   |      MSE      |     PSNR (dB)     |      SSIM      |    CR    |     NCC     | Compressed Size (KB) |\n|----------|---------------|-------------------|----------------|----------|-------------|----------------------|\n| SVD      | 282.9933     | 23.6130           | 0.7122         | 6.88     | 0.9938      | 176.33               |\n| DCT      | 1172.0801    | 17.4412           | 0.5914         | 2.28     | 0.9741      | 531.82               |\n| Wavelet  | 0.0197       | 65.1859           | 0.9999         | 0.12     | 1.0000      | 9715.31              |\n: Quality assessment metrics for original and compressed images in comparison with popular image compression algorithms.\n:::\n\nThe results demonstrate that Singular Value Decomposition (SVD) offers a superior balance between image quality and compression efficiency compared to Discrete Cosine Transform (DCT) and Wavelet Transform. With a significantly lower Mean Squared Error (MSE) and a Peak Signal-to-Noise Ratio (PSNR) of 23.6130 dB, SVD preserves the original image quality more effectively than DCT (17.4412 dB) and offers practical structural similarity (SSIM) of 0.7122. In contrast, while the Wavelet method achieves excellent PSNR (65.1859 dB) and SSIM (0.9999), its large compressed size (9715.31 KB) renders it impractical for many applications.\n\nIn terms of compression efficiency, SVD yields a Compression Ratio (CR) of 6.88 with a manageable compressed size of 176.33 KB, resulting in a significant size reduction of 1037.34 KB. This contrasts sharply with DCT’s lower CR of 2.28 and Wavelet’s CR of 0.12, which implies an increase in size for the latter. Overall, SVD stands out as a robust image compression method, effectively maintaining quality while achieving substantial reductions in storage requirements, making it particularly advantageous for applications prioritizing both quality and efficiency.\n\n## SVD Architecture and Denoising\n\nThe Singular Value Decomposition (SVD) architecture provides a powerful framework for analyzing and compressing images. In the context of image decomposition, the singular values (SVs) represent the luminance levels of various layers within the image, while the corresponding singular vectors (SCs) define the geometric characteristics of these layers. \n\nWhen applied to a high-resolution image, SVD enables the extraction of significant image content through the left singular matrix, capturing the primary structures and features. Conversely, the right singular matrix isolates the noise components, which are typically linked to the smaller singular values found in the diagonal matrix, $\\Sigma$. \n\nThus, the largest singular values correspond to the most prominent image features, often referred to as eigenimages, while the noise components are associated with the smaller singular values. This decomposition allows for a clear distinction between meaningful image information and noise, facilitating effective compression and analysis. By leveraging SVD, one can efficiently manage and manipulate image data, ensuring that essential visual content is retained while minimizing the impact of noise.\n\n:::{#fig-2 layout-ncol=2}\n\n![Original Image](original_image.pdf){#fig-2a}\n\n![Extracted Noise](extracted_noise.pdf){#fig-2b}\n\n![Reconstructed Signal](reconstructed_signal_k_40.pdf){#fig-2c}\n\nAn example with sub-figure illustrating the effectiveness of SVD in separating significant image content from noise.\n\n:::\n\n\n## A starting example\n\nAn example demonstrating the image compression using SVD is given below.\n\n:::{.panel-tabset}\n\n## Code\n```{.matlab}\n\n% Read and convert the image to grayscale\nimg = imread('amrita_campus.jpg'); % Specify your image file\ngray_img = rgb2gray(img); % Convert to grayscale\nA = double(gray_img); % Convert to double for SVD computation\n\n% Apply Singular Value Decomposition (SVD)\n[U, S, V] = svd(A)\n\n% Choose the number of singular values to keep for compression\nk = 50; % You can adjust this value to see different compression levels\n\n% Create a compressed version of the image using the first k singular values\nS_k = zeros(size(A)); % Initialize a zero matrix for S_k\nS_k(1:k, 1:k) = S(1:k, 1:k); % Keep only the top k singular values\n\n% Reconstruct the compressed image\nA_k = U*S_k*V'; % Reconstruct the image from the reduced SVD\n\n% Display the original and compressed images\nfigure;\nsubplot(1, 2, 1);\nimshow(uint8(A)); % Display original image\ntitle('Original Image');\n\nsubplot(1, 2, 2);\nimshow(uint8(A_k)); % Display compressed image\ntitle(['Compressed Image (k = ', num2str(k), ')']);\n```\n\n## Output\n\n```{.python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Read and convert the image to grayscale\nimg = Image.open('amrita_campus.jpg')  # Specify your image file\ngray_img = img.convert('L')  # Convert to grayscale\nA = np.array(gray_img, dtype=np.float64)  # Convert to float64 for SVD computation\noriginal=A\n# Apply Singular Value Decomposition (SVD)\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Choose the number of singular values to keep for compression\nk = 50  # You can adjust this value to see different compression levels\n\n# Create a compressed version of the image using the first k singular values\nS_k = np.zeros_like(A)  # Initialize a zero matrix for S_k\nS_k[:k, :k] = np.diag(S[:k])  # Keep only the top k singular values\n\n# Reconstruct the compressed image\nA_k = np.dot(U[:, :k], np.dot(S_k[:k, :k], Vt[:k, :]))  # Reconstruct the image from the reduced SVD\n\n# Display the original and compressed images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(A, cmap='gray', vmin=0, vmax=255)  # Display original image\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(A_k, cmap='gray', vmin=0, vmax=255)  # Display compressed image\nplt.title(f'Compressed Image (k = {k})')\nplt.axis('off')\n\nplt.show()\n```\n:::\n\n## Assessing quality of compression\n\n\n:::{.panel-tabset}\n\n### Code\n\n```{.matlab}\n% Calculate Mean Squared Error (MSE)\nmse = mean((A(:) - A_k(:)).^2);\n\n% Calculate Peak Signal-to-Noise Ratio (PSNR)\nmax_pixel_value = 255; % Maximum pixel value for 8-bit images\npsnr = 10 * log10((max_pixel_value^2) / mse);\n\n% Calculate sizes\noriginal_size = numel(A) * 8; % Size of the original image in bytes (double data type)\ncompressed_size = (k * (size(A, 1) + size(A, 2))) * 8; % Size of compressed representation (U, S_k, V)\n\n% Display results\nfprintf('Mean Squared Error (MSE): %.4f\\n', mse);\nfprintf('Peak Signal-to-Noise Ratio (PSNR): %.4f dB\\n', psnr);\nfprintf('Original Image Size: %.2f KB\\n', original_size / 1024); % Convert to KB\nfprintf('Compressed Image Size: %.2f KB\\n', compressed_size / 1024); % Convert to KB\nfprintf('Size Reduction: %.2f KB\\n', (original_size - compressed_size) / 1024); % Convert to KB\n```\n\n### Output\n\n```{.python}\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\nimport math\n# Mean Squared Error (MSE)\nmse = np.mean((A - A_k) ** 2)\n\n# Peak Signal-to-Noise Ratio (PSNR)\nmax_pixel_value = 255.0  # For an 8-bit image\npsnr = 10 * np.log10((max_pixel_value ** 2) / mse)\n\n# Structural Similarity Index (SSIM)\nssim_index = ssim(A, A_k, data_range=max_pixel_value)\n\n# Compression Ratio (CR)\noriginal_size = A.nbytes\ncompressed_size = (U[:, :k].nbytes + S_k[:k, :k].nbytes + Vt[:k, :].nbytes)\ncompression_ratio = original_size / compressed_size\n\n# Normalized Cross-Correlation (NCC)\nncc = np.sum(A * A_k) / np.sqrt(np.sum(A ** 2) * np.sum(A_k ** 2))\n# Display results\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr:.4f} dB\")\nprint(f\"Structural Similarity Index (SSIM): {ssim_index:.4f}\")\nprint(f\"Compression Ratio (CR): {compression_ratio:.2f}\")\nprint(f\"Normalized Cross-Correlation (NCC): {ncc:.4f}\")\nprint(f'Original Image Size: {original_size / 1024:.2f} KB')  # Convert to KB\nprint(f'Compressed Image Size: {compressed_size / 1024:.2f} KB')  # Convert to KB\nprint(f'Size Reduction: {(original_size - compressed_size) / 1024:.2f} KB') \n```\n:::\n\n\n\n```{.python}\n#pip install PyWavelets\nimport cv2\nimport numpy as np\nimport pywt\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\n\n# Load and convert image to grayscale\nimg = cv2.imread('amrita_campus.jpg', cv2.IMREAD_GRAYSCALE)\n\n# Function to display images side-by-side\ndef display_images(original, compressed, title):\n    plt.figure(figsize=(10,5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original, cmap='gray')\n    plt.title(\"Original Image\")\n    plt.subplot(1, 2, 2)\n    plt.imshow(compressed, cmap='gray')\n    plt.title(title)\n    plt.show()\n\n# 1. Discrete Cosine Transform (DCT) Compression\ndef dct_compression(img, k=50):\n    img = img.astype(np.float32)\n    dct_img = cv2.dct(img)  # Apply DCT\n    dct_img[np.abs(dct_img) < k] = 0  # Thresholding\n    compressed_img = cv2.idct(dct_img)  # Apply inverse DCT\n    display_images(img, compressed_img, \"DCT Compressed Image\")\n    return compressed_img\n\n# 2. Wavelet Transform Compression (JPEG 2000 equivalent)\ndef wavelet_compression(img, wavelet='haar', level=1, threshold=10):\n    coeffs = pywt.wavedec2(img, wavelet, level=level)\n    coeffs_thresholded = []\n    for c in coeffs:\n        if isinstance(c, tuple):  # For detail coefficients\n            coeffs_thresholded.append(tuple(pywt.threshold(arr, threshold, mode='soft') for arr in c))\n        else:  # For approximation coefficients\n            coeffs_thresholded.append(pywt.threshold(c, threshold, mode='soft'))\n    compressed_img = pywt.waverec2(coeffs_thresholded, wavelet)\n    display_images(img, compressed_img, \"Wavelet Compressed Image\")\n    return compressed_img\n\n# 3. Fractal Compression (simplified example with downsampling)\ndef fractal_compression(img, scale_factor=0.5):\n    small_img = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)\n    compressed_img = cv2.resize(small_img, (img.shape[1], img.shape[0]))  # Upscale back\n    display_images(img, compressed_img, \"Fractal Compressed Image (downsampled)\")\n    return compressed_img\n\n# 4. Run-Length Encoding (RLE) Compression\ndef rle_compression(img):\n    pixels = img.flatten()\n    rle = []\n    i = 0\n    while i < len(pixels):\n        count = 1\n        while i + 1 < len(pixels) and pixels[i] == pixels[i + 1]:\n            i += 1\n            count += 1\n        rle.append((pixels[i], count))\n        i += 1\n    # Decoding RLE for display (just a simple reconstruction)\n    decompressed = np.concatenate([np.full(count, val) for val, count in rle])\n    decompressed_img = decompressed.reshape(img.shape)\n    display_images(img, decompressed_img, \"RLE Compressed Image\")\n    return decompressed_img\n\n# 5. Predictive Coding Compression\ndef predictive_coding_compression(img):\n    img = img.astype(np.int16)  # To handle negative differences\n    prediction_error = img.copy()\n    for i in range(1, img.shape[0]):\n        for j in range(1, img.shape[1]):\n            prediction = (img[i-1, j] + img[i, j-1]) // 2\n            prediction_error[i, j] = img[i, j] - prediction\n    compressed_img = np.clip(prediction_error + img.mean(), 0, 255).astype(np.uint8)\n    display_images(img, compressed_img, \"Predictive Coded Image\")\n    return compressed_img\n\n# Run all compression methods\ndct_compressed = dct_compression(img)\nwavelet_compressed = wavelet_compression(img)\nfractal_compressed = fractal_compression(img)\nrle_compressed = rle_compression(img)\npredictive_coded = predictive_coding_compression(img)\n\n```\n\n```{.python}\nimport numpy as np\nimport cv2\nimport pywt\nimport matplotlib.pyplot as plt\n\ndef load_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n    return img\n\ndef calculate_metrics(original, compressed):\n    mse = np.mean((original - compressed) ** 2)\n    psnr = 10 * np.log10(255**2 / mse) if mse != 0 else float('inf')\n    \n    # Update SSIM to include data_range\n    from skimage.metrics import structural_similarity as ssim\n    ssim_value = ssim(original, compressed, data_range=original.max() - original.min())\n    \n    return mse, psnr, ssim_value\n\ndef svd_compression(img, k=50):\n    A = img.astype(np.float32)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    A_k = np.dot(U, np.dot(np.diag(S_k), Vt))\n\n    compressed = {\n        'U': U[:, :k],\n        'S': S_k[:k],\n        'Vt': Vt[:k, :]\n    }\n    \n    # Debug: Check sizes\n    compressed_size = compressed['U'].nbytes + compressed['S'].nbytes + compressed['Vt'].nbytes\n    print(f\"SVD Compressed Size: {compressed_size} bytes\")\n    return A_k\n\ndef dct_compression(img, threshold=10):\n    dct = cv2.dct(np.float32(img))\n    dct[dct < threshold] = 0  # Zero out small coefficients\n    idct = cv2.idct(dct)\n\n    # Debug: Check sizes\n    compressed_size = dct.nbytes + idct.nbytes\n    print(f\"DCT Compressed Size: {compressed_size} bytes\")\n    return idct\n\ndef wavelet_compression(img, threshold=0.1):\n    coeffs = pywt.wavedec2(img, 'haar', level=2)\n    coeffs_thresholded = [coeffs[0]] + [tuple(pywt.threshold(c, threshold, mode='soft') for c in detail) for detail in coeffs[1:]]\n    \n    # Reconstruct the image from the thresholded coefficients\n    img_reconstructed = pywt.waverec2(coeffs_thresholded, 'haar')\n    \n    # Calculate the compressed size correctly\n    compressed_size = sum(c.nbytes for c in coeffs_thresholded[1]) + coeffs_thresholded[0].nbytes + img_reconstructed.nbytes\n    print(f\"Wavelet Compressed Size: {compressed_size} bytes\")\n    \n    return img_reconstructed\n\ndef display_images(original, compressed, title1, title2):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(title1)\n    plt.imshow(original, cmap='gray')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title(title2)\n    plt.imshow(compressed, cmap='gray')\n    plt.axis('off')\n\n    plt.show()\n\ndef main():\n    file_path = r'D:\\SVD_project\\amrita_campus.jpg'  # Use a raw string for Windows paths\n    original_image = load_image(file_path)\n\n    # Get original image size in KB\n    original_size = original_image.nbytes / 1024  # Convert bytes to KB\n\n    # Perform compression\n    svd_compressed = svd_compression(original_image)\n    dct_compressed = dct_compression(original_image)\n    wavelet_compressed = wavelet_compression(original_image)\n\n    # Calculate metrics\n    svd_mse, svd_psnr, svd_ssim = calculate_metrics(original_image, svd_compressed)\n    dct_mse, dct_psnr, dct_ssim = calculate_metrics(original_image, dct_compressed)\n    wavelet_mse, wavelet_psnr, wavelet_ssim = calculate_metrics(original_image, wavelet_compressed)\n\n    # Calculate Compressed Sizes and additional metrics\n    svd_compressed_size = svd_compressed.nbytes / 1024  # Size in KB\n    dct_compressed_size = dct_compressed.nbytes / 1024  # Size in KB\n    wavelet_compressed_size = wavelet_compressed.nbytes / 1024  # Size in KB\n\n    svd_cr = original_size / svd_compressed_size\n    dct_cr = original_size / dct_compressed_size\n    wavelet_cr = original_size / wavelet_compressed_size\n\n    # NCC calculation\n    def normalized_cross_correlation(original, compressed):\n        return np.sum(original * compressed) / (np.linalg.norm(original) * np.linalg.norm(compressed))\n\n    svd_ncc = normalized_cross_correlation(original_image, svd_compressed)\n    dct_ncc = normalized_cross_correlation(original_image, dct_compressed)\n    wavelet_ncc = normalized_cross_correlation(original_image, wavelet_compressed)\n\n    # Size Reduction\n    svd_size_reduction = original_size - svd_compressed_size\n    dct_size_reduction = original_size - dct_compressed_size\n    wavelet_size_reduction = original_size - wavelet_compressed_size\n\n    # Print Comparison Table\n    print(f\"{'Method':<10} {'MSE':<20} {'PSNR (dB)':<15} {'SSIM':<15} {'CR':<10} {'NCC':<10} {'Compressed Size (KB)':<25} {'Size Reduction (KB)':<20}\")\n    print(f\"{'SVD':<10} {svd_mse:<20.4f} {svd_psnr:<15.4f} {svd_ssim:<15.4f} {svd_cr:<10.2f} {svd_ncc:<10.4f} {svd_compressed_size:<25.2f} {svd_size_reduction:<20.2f}\")\n    print(f\"{'DCT':<10} {dct_mse:<20.4f} {dct_psnr:<15.4f} {dct_ssim:<15.4f} {dct_cr:<10.2f} {dct_ncc:<10.4f} {dct_compressed_size:<25.2f} {dct_size_reduction:<20.2f}\")\n    print(f\"{'Wavelet':<10} {wavelet_mse:<20.4f} {wavelet_psnr:<15.4f} {wavelet_ssim:<15.4f} {wavelet_cr:<10.2f} {wavelet_ncc:<10.4f} {wavelet_compressed_size:<25.2f} {wavelet_size_reduction:<20.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n```{.python}\nimport numpy as np\nimport cv2\nimport pywt\nimport matplotlib.pyplot as plt\nfrom skimage.metrics import structural_similarity as ssim\n\ndef load_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n    return img\n\ndef calculate_metrics(original, compressed):\n    mse = np.mean((original - compressed) ** 2)\n    psnr = 10 * np.log10(255**2 / mse) if mse != 0 else float('inf')\n    ssim_value = ssim(original, compressed, data_range=original.max() - original.min())\n    return mse, psnr, ssim_value\n\ndef svd_compression(img, k=20):\n    A = img.astype(np.float32)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    A_k = np.dot(U[:, :k], np.dot(np.diag(S_k[:k]), Vt[:k, :]))\n\n    compressed = {\n        'U': U[:, :k],\n        'S': S_k[:k],\n        'Vt': Vt[:k, :]\n    }\n    \n    # Calculate the size of compressed data\n    compressed_size = compressed['U'].nbytes + compressed['S'].nbytes + compressed['Vt'].nbytes\n    print(f\"SVD Compressed Size: {compressed_size} bytes\")\n    return A_k, compressed_size\n\ndef dct_compression(img, threshold=10):\n    dct = cv2.dct(np.float32(img))\n    dct[dct < threshold] = 0  # Zero out small coefficients\n    \n    # Count non-zero coefficients\n    non_zero_coeffs = np.count_nonzero(dct)\n    compressed_size = dct.nbytes - (dct.size - non_zero_coeffs) * dct.dtype.itemsize  # Size excluding zeros\n\n    idct = cv2.idct(dct)  # Reconstruct the image (not needed for size calculation)\n\n    print(f\"DCT Compressed Size: {compressed_size} bytes\")\n    return idct, compressed_size\n\ndef wavelet_compression(image, wavelet='haar', threshold=0.2):\n    # Perform 2D wavelet decomposition\n    coeffs = pywt.wavedec2(image, wavelet)\n    \n    # Threshold the detail coefficients\n    coeffs_thresholded = list(coeffs)\n    for i in range(1, len(coeffs_thresholded)):\n        coeffs_thresholded[i] = tuple(pywt.threshold(c, threshold, mode='soft') for c in coeffs_thresholded[i])\n    \n    # Calculate compressed size\n    compressed_size = sum(np.prod(c.shape) * c.dtype.itemsize for detail in coeffs_thresholded[1:] for c in detail) + coeffs_thresholded[0].nbytes\n    \n    # Reconstruct the image\n    img_reconstructed = pywt.waverec2(coeffs_thresholded, wavelet)\n    \n    return img_reconstructed, compressed_size\n\ndef display_images(original, compressed, title1, title2):\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(title1)\n    plt.imshow(original, cmap='gray')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title(title2)\n    plt.imshow(compressed, cmap='gray')\n    plt.axis('off')\n\n    plt.show()\n\ndef main():\n    file_path = r'D:\\SVD_project\\amrita_campus.jpg'  # Use a raw string for Windows paths\n    original_image = load_image(file_path)\n\n    # Get original image size in bytes\n    original_size = original_image.nbytes  # Size in bytes\n\n    # Perform compression\n    svd_compressed, svd_compressed_size = svd_compression(original_image)\n    dct_compressed, dct_compressed_size = dct_compression(original_image)\n    wavelet_compressed, wavelet_compressed_size = wavelet_compression(original_image)\n\n    # Calculate metrics\n    svd_mse, svd_psnr, svd_ssim = calculate_metrics(original_image, svd_compressed)\n    dct_mse, dct_psnr, dct_ssim = calculate_metrics(original_image, dct_compressed)\n    wavelet_mse, wavelet_psnr, wavelet_ssim = calculate_metrics(original_image, wavelet_compressed)\n    \n    # Calculate Compressed Sizes and additional metrics\n    svd_cr = original_size / svd_compressed_size\n    dct_cr = original_size / dct_compressed_size\n    wavelet_cr = original_size / wavelet_compressed_size\n\n    # NCC calculation\n    def normalized_cross_correlation(original, compressed):\n        return np.sum(original * compressed) / (np.linalg.norm(original) * np.linalg.norm(compressed))\n\n    svd_ncc = normalized_cross_correlation(original_image, svd_compressed)\n    dct_ncc = normalized_cross_correlation(original_image, dct_compressed)\n    wavelet_ncc = normalized_cross_correlation(original_image, wavelet_compressed)\n\n    # Size Reduction\n    svd_size_reduction = original_size - svd_compressed_size\n    dct_size_reduction = original_size - dct_compressed_size\n    wavelet_size_reduction = original_size - wavelet_compressed_size\n\n    # Print Comparison Table\n    print(f\"{'Method':<10} {'MSE':<20} {'PSNR (dB)':<15} {'SSIM':<15} {'CR':<10} {'NCC':<10} {'Compressed Size (KB)':<25} {'Size Reduction (KB)':<20}\")\n    print(f\"{'SVD':<10} {svd_mse:<20.4f} {svd_psnr:<15.4f} {svd_ssim:<15.4f} {svd_cr:<10.2f} {svd_ncc:<10.4f} {svd_compressed_size/1024:<25.2f} {svd_size_reduction/1024:<20.2f}\")\n    print(f\"{'DCT':<10} {dct_mse:<20.4f} {dct_psnr:<15.4f} {dct_ssim:<15.4f} {dct_cr:<10.2f} {dct_ncc:<10.4f} {dct_compressed_size/1024:<25.2f} {dct_size_reduction/1024:<20.2f}\")\n    print(f\"{'Wavelet':<10} {wavelet_mse:<20.4f} {wavelet_psnr:<15.4f} {wavelet_ssim:<15.4f} {wavelet_cr:<10.2f} {wavelet_ncc:<10.4f} {wavelet_compressed_size/1024:<25.2f} {wavelet_size_reduction/1024:<20.2f}\")\n    display_images(original_image, svd_compressed,\"original\",\"compressed\")\nif __name__ == \"__main__\":\n    main()\n    \n```\n\n```{.python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Set number of components to visualize\nnum_components = 2\n\n# Function to normalize and visualize singular vectors\ndef visualize_singular_vectors(vectors, title, n_components, shape):\n    fig, axs = plt.subplots(1, n_components, figsize=(15, 5))\n    fig.suptitle(title, fontsize=16)\n    for i in range(n_components):\n        vector = vectors[:, i] if title == 'Column Space (U)' else vectors[i, :]\n        # Normalize vector for better visibility\n        normalized_vector = (vector - np.min(vector)) / (np.max(vector) - np.min(vector))\n        axs[i].imshow(normalized_vector.reshape(shape), cmap='gray', aspect='auto')\n        axs[i].axis('off')\n        axs[i].set_title(f'Component {i+1}')\n    plt.show()\n\n# Visualize Column Space (U matrix columns)\nvisualize_singular_vectors(U, \"Column Space (U)\", num_components, (image.shape[0], 1))\n\n# Visualize Row Space (V^T matrix rows)\nvisualize_singular_vectors(Vt, \"Row Space (V^T)\", num_components, (1, image.shape[1]))\n\n# Plot Singular Values\nplt.figure(figsize=(8, 6))\nplt.plot(np.log(1+S), 'o-', label=\"Singular Values\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Singular Value\")\nplt.title(\"Singular Values Plot\")\nplt.legend()\nplt.grid()\nplt.show()\n\n```\n\n### Ploting the signal and noise part of an image \n\n\n```{.python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\n\n# Load the image and convert it to grayscale\nimage = io.imread('.TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\n\n# Perform SVD\nU, S, VT = np.linalg.svd(image, full_matrices=False)\n\n# Number of components to keep\nk = 40  # Adjust this for more or fewer components\n\n# Reconstruct the signal part\nS_k = np.zeros_like(S)  # Create a zero array for singular values\nS_k[:k] = S[:k]  # Keep the largest k singular values\n\n# Reconstruct the signal image\nreconstructed_signal = U @ np.diag(S_k) @ VT\n\n# Extract noise\nnoise = image - reconstructed_signal\n# Convert images to uint8 for saving\nimage_uint8 = (image * 255).astype(np.uint8)\nreconstructed_signal_uint8 = (reconstructed_signal * 255).astype(np.uint8)\nnoise_uint8 = (noise * 255).astype(np.uint8)\n\n# Save each image as a PDF\nio.imsave('original_image.pdf', image_uint8)\nio.imsave('reconstructed_signal_k_{}.pdf'.format(k), reconstructed_signal_uint8)\nio.imsave('extracted_noise.pdf', noise_uint8)\n# Plotting\nplt.figure(figsize=(15, 10))\n\nplt.subplot(1, 3, 1)\nplt.title('Original Image')\nplt.imshow(image, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.title('Reconstructed Signal (k={})'.format(k))\nplt.imshow(reconstructed_signal, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.title('Extracted Noise')\nplt.imshow(noise, cmap='gray')\nplt.axis('off')\n\nplt.tight_layout()\n# Save the entire figure as a PDF\nplt.savefig('comparison_plot.pdf', bbox_inches='tight')\nplt.show()\n```\n\n\n### Compression quality with different values of k\n\n```{.python}\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\n\n# Initialize a list to store results\nresults = []\n\n# Define a range of k values\nk_values = [1, 5, 10, 20, 50, 100, 200, 400, 600, 800, 1000]\n\nfor k in k_values:\n    # Perform SVD\n    U, S, VT = np.linalg.svd(image, full_matrices=False)\n\n    # Reconstruct the signal part with k components\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]\n    reconstructed_signal = U @ np.diag(S_k) @ VT\n\n    # Compute PSNR and SSIM\n    current_psnr = psnr(image, reconstructed_signal)\n    \n    # Set data_range for SSIM\n    data_range = 1  # Use 255 if your image is in the range [0, 255]\n    current_ssim = ssim(image, reconstructed_signal, data_range=data_range)\n\n    # Append results\n    results.append({'k': k, 'PSNR': current_psnr, 'SSIM': current_ssim})\n# Create a DataFrame from the results\nresults_df = pd.DataFrame(results)\n\n# Display the DataFrame as a table\nprint(results_df)\n\n# Optionally, save the results to a CSV file\nresults_df.to_csv('psnr_ssim_variation.csv', index=False)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(results_df['k'], results_df['PSNR'], marker='o', label='PSNR')\nplt.plot(results_df['k'], results_df['SSIM'], marker='o', label='SSIM')\nplt.xscale('log')  # Log scale for better visualization\nplt.xlabel('Number of Components (k)')\nplt.ylabel('Value')\nplt.title('Variation of PSNR and SSIM with Different k Values')\nplt.legend()\nplt.grid()\nplt.savefig('psnr_ssim_variation_plot.pdf')\nplt.show()\n```\n\n### Creating images from the left singular and right singular matrices\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Normalize function for better visualization\ndef normalize_matrix(matrix):\n    return (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n\n# Save the original image\nplt.imshow(image, cmap='gray')\nplt.axis('off')\nplt.title('Original Image')\nplt.savefig('original_image.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n\n# Save the left singular matrix (U)\nplt.imshow(normalize_matrix(U), cmap='gray', aspect='auto')\nplt.axis('off')\nplt.title('Left Singular Matrix (U)')\nplt.savefig('left_singular_matrix_U.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n\n# Save the right singular matrix (V^T)\nplt.imshow(normalize_matrix(Vt), cmap='gray', aspect='auto')\nplt.axis('off')\nplt.title('Right Singular Matrix (V^T)')\nplt.savefig('right_singular_matrix_Vt.pdf', format='pdf', bbox_inches='tight')\nplt.close()\n```\n\n### Ploting the Dominant and sub dominant components\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# Number of components for partial reconstruction\nk = 50  # Choose k to retain enough detail without full reconstruction\n\n# Reconstructing left singular matrix U and right singular matrix V^T with k components\nU_reconstructed = U[:, :k] @ np.diag(S[:k])\nVt_reconstructed = np.diag(S[:k]) @ Vt[:k, :]\n\n# Set figure size based on the original image dimensions\nfigsize = (image.shape[1] / 100, image.shape[0] / 100)\n\n# Function to save reconstructed singular matrix images with the same dimensions as the original\ndef save_reconstructed_matrix(matrix, title, filename):\n    fig, ax = plt.subplots(figsize=figsize)\n    # Normalize for better visualization\n    #normalized_matrix = (matrix - np.min(matrix)) / (np.max(matrix) - np.min(matrix))\n    ax.imshow(matrix, cmap='gray', aspect='auto')\n    ax.axis('off')\n    plt.title(title)\n    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n\n# Save the original image in grayscale for reference\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\n\n# Save reconstructed left singular matrix U\nsave_reconstructed_matrix(U_reconstructed, \"Reconstructed Left Singular Matrix (U)\", \"left_singular_matrix_U_traces.pdf\")\n\n# Save reconstructed right singular matrix V^T\nsave_reconstructed_matrix(Vt_reconstructed, \"Reconstructed Right Singular Matrix (V^T)\", \"right_singular_matrix_Vt_traces.pdf\")\n```\n\n### Code for Denoising with SVD\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the original image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Check the original image statistics\nprint(\"Original Image - Min:\", np.min(image), \"Max:\", np.max(image))\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.1  # Set this value based on your requirements\n# Generate Gaussian noise\nnoise = np.random.normal(0, np.sqrt(noise_variance), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Check the noisy image statistics\nprint(\"Noisy Image - Min:\", np.min(noisy_image), \"Max:\", np.max(noisy_image))\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Filter Noise by Truncating Smaller Singular Values\n# Adjust `k` to retain more singular values, which should improve quality\nk = 50  # Adjust this value as needed\nS_denoised = np.zeros_like(S)\nS_denoised[:k] = S[:k]  # Keep the top `k` singular values\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\n\n# Check the denoised image statistics\nprint(\"Denoised Image - Min:\", np.min(denoised_image), \"Max:\", np.max(denoised_image))\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(\"Noisy Image\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(\"Denoised Image (SVD)\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Saving each image as a standalone PDF\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\nplt.imsave(\"noisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"denoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n## Denoising BSD400 Dataet using SVD\n\n```{python}\nimport numpy as np\nfrom skimage import io, color, metrics\nimport matplotlib.pyplot as plt\n\n# Load the original image from the BSD400 dataset and convert it to grayscale\nimage = io.imread('DenoiseImage.png')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Normalize the image to the range [0, 1]\nimage -= np.min(image)\nimage /= np.max(image)\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.001  # Use a smaller noise variance relative to the normalized image\n# Generate Gaussian noise scaled by the maximum value of the image\nnoise = np.random.normal(0, np.sqrt(noise_variance * np.max(image)), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Determine a threshold for singular values\nthreshold =0.618*np.mean(S)  # Example threshold: mean of singular values\nS_denoised = np.where(S > threshold, S, 0)  # Set small singular values to zero\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\ndata_range = 1.0  # If your images are in the range [0, 1]. Use 255 if they are in [0, 255].\n\n# Step 5: Calculate PSNR and SSIM to assess denoising performance\npsnr_noisy = metrics.peak_signal_noise_ratio(image, noisy_image, data_range=data_range)\nssim_noisy = metrics.structural_similarity(image, noisy_image, data_range=data_range)\n\npsnr_denoised = metrics.peak_signal_noise_ratio(image, denoised_image, data_range=data_range)\nssim_denoised = metrics.structural_similarity(image, denoised_image, data_range=data_range)\nprint(f\"PSNR (Noisy Image): {psnr_noisy:.2f}\")\nprint(f\"SSIM (Noisy Image): {ssim_noisy:.4f}\")\nprint(f\"PSNR (Denoised Image): {psnr_denoised:.2f}\")\nprint(f\"SSIM (Denoised Image): {ssim_denoised:.4f}\")\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}, SSIM: {ssim_noisy:.4f}\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(f\"Denoised Image (SVD)\\nPSNR: {psnr_denoised:.2f}, SSIM: {ssim_denoised:.4f}\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Saving each image as a standalone PDF\nplt.imsave(\"BSDoriginal_image.pdf\", image, cmap='gray')\nplt.imsave(\"BSDnoisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"BSDdenoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n\n### Assesing quality of SVD denoiser\n\n```{python}\nimport numpy as np\nfrom skimage import io, color, metrics\nimport matplotlib.pyplot as plt\n\n# Load the original image and convert it to grayscale\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Check the original image statistics\nprint(\"Original Image - Min:\", np.min(image), \"Max:\", np.max(image))\n\n# Step 1: Add Gaussian noise to the image\nnoise_variance = 0.1  # Set this value based on your requirements\n# Generate Gaussian noise\nnoise = np.random.normal(0, np.sqrt(noise_variance), image.shape)\n# Add noise to the original image\nnoisy_image = image + noise\n\n# Ensure the noisy image values are clipped to [0, 1]\nnoisy_image = np.clip(noisy_image, 0, 1)\n\n# Check the noisy image statistics\nprint(\"Noisy Image - Min:\", np.min(noisy_image), \"Max:\", np.max(noisy_image))\n\n# Step 2: Perform SVD on the noisy image\nU, S, Vt = np.linalg.svd(noisy_image, full_matrices=False)\n\n# Step 3: Filter Noise by Truncating Smaller Singular Values\n# Adjust `k` to retain more singular values, which should improve quality\nk = 50  # Adjust this value as needed\nS_denoised = np.zeros_like(S)\nS_denoised[:k] = S[:k]  # Keep the top `k` singular values\n\n# Step 4: Reconstruct the denoised image\ndenoised_image = U @ np.diag(S_denoised) @ Vt\n\n# Ensure the denoised image values are clipped to [0, 1]\ndenoised_image = np.clip(denoised_image, 0, 1)\n\n# Check the denoised image statistics\nprint(\"Denoised Image - Min:\", np.min(denoised_image), \"Max:\", np.max(denoised_image))\ndata_range = 1.0  # If your images are in the range [0, 1]. Use 255 if they are in [0, 255].\n\n# Step 5: Calculate PSNR and SSIM to assess denoising performance\npsnr_noisy = metrics.peak_signal_noise_ratio(image, noisy_image, data_range=data_range)\nssim_noisy = metrics.structural_similarity(image, noisy_image, data_range=data_range)\n\npsnr_denoised = metrics.peak_signal_noise_ratio(image, denoised_image, data_range=data_range)\nssim_denoised = metrics.structural_similarity(image, denoised_image, data_range=data_range)\nprint(f\"PSNR (Noisy Image): {psnr_noisy:.2f}\")\nprint(f\"SSIM (Noisy Image): {ssim_noisy:.4f}\")\nprint(f\"PSNR (Denoised Image): {psnr_denoised:.2f}\")\nprint(f\"SSIM (Denoised Image): {ssim_denoised:.4f}\")\n\n# Plotting and saving the results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Image\naxs[0].imshow(image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\n# Noisy Image\naxs[1].imshow(noisy_image, cmap='gray')\naxs[1].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}, SSIM: {ssim_noisy:.4f}\")\naxs[1].axis('off')\n\n# Denoised Image (SVD)\naxs[2].imshow(denoised_image, cmap='gray')\naxs[2].set_title(f\"Denoised Image (SVD)\\nPSNR: {psnr_denoised:.2f}, SSIM: {ssim_denoised:.4f}\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Optionally save each image as a standalone PDF\nplt.imsave(\"original_image.pdf\", image, cmap='gray')\nplt.imsave(\"noisy_image.pdf\", noisy_image, cmap='gray')\nplt.imsave(\"denoised_image.pdf\", denoised_image, cmap='gray')\n```\n\n### Ploting the correlation between regenerated images over k\n\n\n```{python}\nimport numpy as np\nfrom skimage import io, color\nimport matplotlib.pyplot as plt\n\n# Load the original grayscale image\nimage = io.imread('TestImage.jpg')\nif image.ndim == 3:\n    image = color.rgb2gray(image)\nimage = image.astype(float)\n\n# Perform SVD on the original image\nU, S, Vt = np.linalg.svd(image, full_matrices=False)\n\n# List of k-values to experiment with\nk_values = [10, 20, 50, 100, 200, 400, 600]\n\n# Flatten the original image to compare correlations\noriginal_flattened = image.flatten()\n\n# Array to store correlations with the original image for each k\ncorrelations = []\n\n# Reconstruct images using different numbers of singular values and compute correlations\nfor k in k_values:\n    S_k = np.zeros_like(S)\n    S_k[:k] = S[:k]  # Retain only the top k singular values\n    reconstructed_image = U @ np.diag(S_k) @ Vt\n    reconstructed_flattened = reconstructed_image.flatten()\n    \n    # Calculate correlation with the original image\n    corr = np.corrcoef(original_flattened, reconstructed_flattened)[0, 1]\n    correlations.append(corr)\n\n# Plotting k-values against their corresponding correlations\nplt.figure(figsize=(8, 5))\nplt.plot(k_values, correlations, marker='o', linestyle='-')\nplt.xlabel(\"Number of Singular Values (k)\")\nplt.ylabel(\"Correlation with Original Image\")\nplt.title(\"Correlation between Reconstructed Images and Original Image\")\nplt.grid()\nplt.show()\n```\n\n### Image Forensic with SVD\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io, color\n\ndef preprocess_image(image):\n    if image.ndim == 3:\n        if image.shape[2] == 4:  # Check for RGBA\n            image = image[:, :, :3]  # Take RGB only\n        gray_image = color.rgb2gray(image)\n    else:\n        gray_image = image  # Already grayscale\n    return gray_image.astype(float)\n\ndef embed_watermark(image, watermark, alpha=0.1):\n    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n    watermark_resized = np.resize(watermark, S.shape)\n    S_watermarked = S + alpha * watermark_resized\n    watermarked_image = np.dot(U, np.dot(np.diag(S_watermarked), Vt))\n    return np.clip(watermarked_image, 0, 1)\n\ndef extract_watermark(original_image, watermarked_image, alpha=0.1):\n    U1, S1, Vt1 = np.linalg.svd(original_image, full_matrices=False)\n    U2, S2, Vt2 = np.linalg.svd(watermarked_image, full_matrices=False)\n    extracted_watermark = (S2 - S1) / alpha\n    return extracted_watermark\n\n# Load images\nhost_image = io.imread('TESTIMAGE.png')\nhost_image = preprocess_image(host_image)\n\nwatermark_image = io.imread('amritha_TL.png')\nwatermark_image = preprocess_image(watermark_image)\n\n# Ensure images are in [0, 1] range\nhost_image = np.clip(host_image, 0, 1)\nwatermark_image = np.clip(watermark_image, 0, 1)\n\n# Embed watermark\nwatermarked_image = embed_watermark(host_image, watermark_image, alpha=0.1)\n\n# Extract watermark\nextracted_watermark = extract_watermark(host_image, watermarked_image, alpha=0.1)\n\n# Reshape the extracted watermark to the size of the original watermark image\nextracted_watermark = np.clip(extracted_watermark, 0, 1)  # Ensure valid pixel range\nextracted_watermark = np.resize(extracted_watermark, watermark_image.shape)  # Resize to match original watermark\n\n# Plotting results\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\naxs[0].imshow(host_image, cmap='gray')\naxs[0].set_title(\"Original Image\")\naxs[0].axis('off')\n\naxs[1].imshow(watermarked_image, cmap='gray')\naxs[1].set_title(\"Watermarked Image\")\naxs[1].axis('off')\n\naxs[2].imshow(extracted_watermark, cmap='gray')\naxs[2].set_title(\"Extracted Watermark\")\naxs[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n```\n\n```{python}\nimport numpy as np\nimport cv2\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\ndef read_image(filename):\n    # Read the image using OpenCV\n    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)  # Read with unchanged channels\n    if img is None:\n        raise ValueError(\"Image not found or unable to read.\")\n    \n    # Check if the image has an alpha channel\n    if img.shape[2] == 4:  # If there are 4 channels (RGBA)\n        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)  # Convert to BGR without alpha\n    \n    # Convert to grayscale if it's a 3-channel image\n    if len(img.shape) == 3 and img.shape[2] == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    return img\n\n# Load host image\nhost_image = read_image('TESTIMAGE.png')  # Replace with your actual path\nhost_image = cv2.resize(host_image, (256, 256))  # Resize to 256x256\nhost_image = host_image.astype(np.float64)  # Convert to double\n\n# Perform SVD\nUimg, Simg, Vimg = svd(host_image)\n\n# Read watermark\nwatermark_image = read_image('copyright1.png')  # Replace with your actual path\nwatermark_image = cv2.resize(watermark_image, (256, 256))  # Resize to 256x256\nalfa = 0.1  # Alpha value for watermark embedding\nwatermark_image = watermark_image.astype(np.float64)  # Convert to double\n\n# Ensure watermark is in the same shape as Simg\nif watermark_image.shape[0] > len(Simg):\n    watermark_image = watermark_image[:len(Simg), :]\n\n# Modify the singular values by adding the scaled watermark\nfor i in range(len(Simg)):\n    Simg[i] += alfa * watermark_image[i, 0]  # Update singular values\n\n# Reconstruct the watermarked image\nSimg_matrix = np.diag(Simg)  # Create a diagonal matrix from singular values\nwatermarked_image = np.dot(Uimg, np.dot(Simg_matrix, Vimg))\n\n# Normalize the watermarked image to the range [0, 255]\nwatermarked_image = np.clip(watermarked_image, 0, 255).astype(np.uint8)\n\n# Display the original host image\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(host_image, cmap='gray')\nplt.title('The Original Image')\nplt.axis('off')\n\n# Display the watermarked image\nplt.subplot(1, 2, 2)\nplt.imshow(watermarked_image, cmap='gray')\nplt.title('Watermarked Image')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n```\n\n## Extraction part\n\n```{python}\nimport numpy as np\nimport cv2\nfrom scipy.linalg import svd\nimport matplotlib.pyplot as plt\n\ndef read_image(filename):\n    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n    if img is None:\n        raise ValueError(f\"Image not found or unable to read: {filename}\")\n    \n    if len(img.shape) == 3 and img.shape[2] == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    return img\n\ndef embed_watermark(host_image, watermark_image, alfa=0.1):\n    Uimg, Simg, Vimg = svd(host_image)\n    \n    if watermark_image.shape[0] > len(Simg):\n        raise ValueError(\"Watermark size exceeds singular values length.\")\n\n    for i in range(len(Simg)):\n        Simg[i] += alfa * watermark_image[i, 0]\n\n    Simg_matrix = np.diag(Simg)\n    watermarked_image = np.dot(Uimg, np.dot(Simg_matrix, Vimg))\n    \n    return watermarked_image, Simg, Uimg, Vimg\n\ndef extract_watermark(watermarked_image, original_singular_values, alfa=0.1, watermark_shape=None):\n    U_Wimg, S_Wimg, V_Wimg = svd(watermarked_image)\n\n    # Calculate the extracted watermark\n    extracted_watermark = (S_Wimg - original_singular_values) / alfa\n\n    if watermark_shape is not None:\n        extracted_watermark = extracted_watermark.reshape(watermark_shape)\n\n    # Normalize to uint8 range\n    extracted_watermark = np.clip(extracted_watermark, 0, 255).astype(np.uint8)\n\n    return extracted_watermark\n\ndef calculate_psnr(original_image, watermarked_image):\n    mse = np.mean((original_image.astype(np.float64) - watermarked_image.astype(np.float64)) ** 2)\n    if mse == 0:\n        return 100\n    max_pixel_value = 255.0\n    psnr = 10 * np.log10((max_pixel_value ** 2) / mse)\n    return psnr\n\n# Load host image\nhost_image = read_image('TESTIMAGE.png')  # Replace with your actual path\nhost_image = cv2.resize(host_image, (256, 256)).astype(np.float64)\n\n# Load watermark image\nwatermark_image = read_image('copyright1.png')  # Replace with your actual path\nwatermark_image = cv2.resize(watermark_image, (256, 1)).astype(np.float64)\n\n# Embed watermark\nalfa = 0.1\nwatermarked_image, Simg, Uimg, Vimg = embed_watermark(host_image, watermark_image, alfa)\n\n# Extract watermark\nextracted_watermark = extract_watermark(watermarked_image, Simg, alfa, watermark_shape=(256, 1))\n\n# Calculate PSNR\npsnr_value = calculate_psnr(host_image, watermarked_image)\n\n# Display the images\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.imshow(host_image, cmap='gray')\nplt.title('The Original Image')\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(watermarked_image, cmap='gray')\nplt.title('Watermarked Image')\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(extracted_watermark, cmap='gray')\nplt.title('Extracted Watermark')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"PSNR between the original and watermarked image: {psnr_value:.2f} dB\")\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"workbook1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","formats":{"ieee-pdf":"default","ieee-html":"default"},"title":"SVD workbook","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}